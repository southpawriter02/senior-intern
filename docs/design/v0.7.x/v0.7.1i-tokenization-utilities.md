# Design Specification: AIntern v0.7.1i "Tokenization Utilities"

## Overview

**Version**: v0.7.1i
**Parent**: v0.7.1 Embedding Foundation
**Focus**: Provide tokenization utilities for accurate chunk sizing

### Purpose

Implement tokenization utilities for accurate token count estimation:
1. Create `SimpleTokenizer` for rule-based token counting
2. Create `HuggingFaceTokenizerService` for loading tokenizer.json vocabularies
3. Create `TokenizerFactory` for selecting appropriate tokenizer
4. Create `TokenEstimator` static utilities for quick estimates
5. Define `TokenEstimationMode` enum for content-type-aware estimation

### Dependencies

**From v0.7.1c (ONNX Embedding Pipeline)**:
- `ITokenizerService` interface
- `TokenizedText` model

**From v0.7.1e (Chunking Service Interface)**:
- `IChunkingService.EstimateTokenCount()` method (consumer)

---

## Architecture

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                   v0.7.1i Tokenization Architecture                           │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Tokenizer Selection Flow:                                                   │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │                                                                          │ │
│  │  TokenizerFactory.CreateTokenizer(modelPath)                            │ │
│  │               │                                                          │ │
│  │               ↓                                                          │ │
│  │  ┌─────────────────────────────────────────────────────────────────────┐│ │
│  │  │ modelPath provided?                                                  ││ │
│  │  │     │                                                                ││ │
│  │  │     ├── YES → Check for tokenizer.json                              ││ │
│  │  │     │         │                                                      ││ │
│  │  │     │         ├── EXISTS → HuggingFaceTokenizerService              ││ │
│  │  │     │         │            (accurate, model-specific)                ││ │
│  │  │     │         │                                                      ││ │
│  │  │     │         └── MISSING → SimpleTokenizer                         ││ │
│  │  │     │                        (rule-based fallback)                   ││ │
│  │  │     │                                                                ││ │
│  │  │     └── NO → SimpleTokenizer                                        ││ │
│  │  │               (always available)                                     ││ │
│  │  └─────────────────────────────────────────────────────────────────────┘│ │
│  │                                                                          │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ITokenizerService Implementations:                                          │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │                                                                          │ │
│  │  ┌─────────────────────────────┬─────────────────────────────────────┐  │ │
│  │  │ HuggingFaceTokenizerService │ SimpleTokenizer                     │  │ │
│  │  ├─────────────────────────────┼─────────────────────────────────────┤  │ │
│  │  │ ✓ Accurate tokenization     │ ✓ No model needed                   │  │ │
│  │  │ ✓ WordPiece subwords        │ ✓ Always available                  │  │ │
│  │  │ ✓ Special tokens            │ ✓ Fast regex-based                  │  │ │
│  │  │ ✓ Encode/Decode             │ ✓ Reasonable estimates              │  │ │
│  │  │ ✗ Requires tokenizer.json   │ ✗ Decode not supported              │  │ │
│  │  │ ✗ Slower startup            │ ✗ Less accurate                     │  │ │
│  │  └─────────────────────────────┴─────────────────────────────────────┘  │ │
│  │                                                                          │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  Quick Estimation (No Tokenizer):                                            │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │                                                                          │ │
│  │  TokenEstimator (static class)                                          │ │
│  │  ├── EstimateForCode(text)      → ~4 chars/token                       │ │
│  │  ├── EstimateForText(text)      → ~4.5 chars/token                     │ │
│  │  ├── EstimateAdaptive(text)     → Content-type aware                   │ │
│  │  ├── CharactersToTokens(count)  → Conversion utility                   │ │
│  │  └── TokensToCharacters(count)  → Reverse conversion                   │ │
│  │                                                                          │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
└──────────────────────────────────────────────────────────────────────────────┘
```

---

## ITokenizerService Interface (from v0.7.1c)

```csharp
/// <summary>
/// Service for text tokenization.
/// </summary>
public interface ITokenizerService
{
    /// <summary>
    /// Whether a tokenizer is currently loaded.
    /// </summary>
    bool IsLoaded { get; }

    /// <summary>
    /// Load a tokenizer from a JSON file.
    /// </summary>
    Task LoadTokenizerAsync(string path, CancellationToken ct = default);

    /// <summary>
    /// Tokenize text and return token IDs.
    /// </summary>
    TokenizedText Encode(string text, int maxLength);

    /// <summary>
    /// Decode token IDs back to text.
    /// </summary>
    string Decode(long[] tokenIds);

    /// <summary>
    /// Count tokens in text without full encoding.
    /// </summary>
    int CountTokens(string text);
}

/// <summary>
/// Result of text tokenization.
/// </summary>
public sealed class TokenizedText
{
    public long[] InputIds { get; init; } = Array.Empty<long>();
    public long[] AttentionMask { get; init; } = Array.Empty<long>();
    public long[]? TokenTypeIds { get; init; }
}
```

---

## File Specifications

### 1. SimpleTokenizer.cs

**Location**: `src/SeniorIntern.Services/Embedding/SimpleTokenizer.cs`

```csharp
namespace SeniorIntern.Services.Embedding;

using System;
using System.Collections.Generic;
using System.Linq;
using System.Text.RegularExpressions;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.Extensions.Logging;
using SeniorIntern.Core.Interfaces;

/// <summary>
/// Simple tokenizer for token count estimation when no model tokenizer is loaded.
/// </summary>
/// <remarks>
/// <para>
/// This tokenizer uses a rule-based approach similar to GPT-style tokenization,
/// identifying tokens via regex patterns. It is designed for estimation purposes
/// when a full model tokenizer (tokenizer.json) is not available.
/// </para>
/// <para>
/// Token patterns recognized:
/// <list type="bullet">
///   <item>Numbers (integers and decimals)</item>
///   <item>Identifiers (variables, function names)</item>
///   <item>String literals (single and double quoted)</item>
///   <item>Comments (single-line and multi-line)</item>
///   <item>Operators (+, -, *, /, =, etc.)</item>
///   <item>Punctuation ({, }, (, ), [, ], etc.)</item>
///   <item>Whitespace (treated as single tokens)</item>
/// </list>
/// </para>
/// <para>
/// Limitations:
/// <list type="bullet">
///   <item>Cannot decode token IDs back to text</item>
///   <item>Uses hash-based pseudo-token IDs (not vocabulary-based)</item>
///   <item>Approximately 10-20% variance from actual model tokenization</item>
/// </list>
/// </para>
/// </remarks>
public sealed class SimpleTokenizer : ITokenizerService
{
    private readonly ILogger<SimpleTokenizer> _logger;

    /// <summary>
    /// Regex pattern for tokenizing programming-related content.
    /// </summary>
    /// <remarks>
    /// This pattern matches common code constructs in order of precedence:
    /// numbers, identifiers, strings, comments, operators, punctuation, whitespace.
    /// </remarks>
    private static readonly Regex _tokenPattern = new(
        @"(?:
            \d+\.?\d*              |  # Numbers (integers and decimals)
            [a-zA-Z_]\w*          |  # Identifiers (variables, functions)
            ""(?:[^""\\]|\\.)*""  |  # Double-quoted strings
            '(?:[^'\\]|\\.)*'     |  # Single-quoted strings
            //.*$                  |  # Single-line comments
            /\*[\s\S]*?\*/        |  # Multi-line comments
            [+\-*/=<>!&|^~%]+     |  # Operators
            [{}()\[\];:,.]        |  # Punctuation
            \s+                      # Whitespace
        )",
        RegexOptions.Compiled | RegexOptions.IgnorePatternWhitespace | RegexOptions.Multiline);

    #region Properties

    /// <inheritdoc />
    /// <remarks>
    /// SimpleTokenizer is always considered "loaded" since it doesn't require
    /// external vocabulary files.
    /// </remarks>
    public bool IsLoaded => true;

    #endregion

    #region Constructor

    /// <summary>
    /// Initializes a new instance of the SimpleTokenizer.
    /// </summary>
    /// <param name="logger">Logger instance.</param>
    public SimpleTokenizer(ILogger<SimpleTokenizer> logger)
    {
        _logger = logger ?? throw new ArgumentNullException(nameof(logger));
    }

    #endregion

    #region ITokenizerService Implementation

    /// <inheritdoc />
    /// <remarks>
    /// SimpleTokenizer doesn't require loading. This method completes immediately.
    /// </remarks>
    public Task LoadTokenizerAsync(string path, CancellationToken ct = default)
    {
        _logger.LogDebug("SimpleTokenizer doesn't require loading");
        return Task.CompletedTask;
    }

    /// <inheritdoc />
    public TokenizedText Encode(string text, int maxLength)
    {
        if (string.IsNullOrEmpty(text))
        {
            return new TokenizedText
            {
                InputIds = Array.Empty<long>(),
                AttentionMask = Array.Empty<long>()
            };
        }

        var matches = _tokenPattern.Matches(text);
        var tokenIds = new List<long>();

        foreach (Match match in matches)
        {
            if (tokenIds.Count >= maxLength)
                break;

            // Use hash as pseudo-token ID
            // Note: These IDs are not vocabulary-based and cannot be decoded
            var tokenId = (long)match.Value.GetHashCode();
            tokenIds.Add(tokenId);
        }

        // Create attention mask (all 1s for real tokens)
        var attentionMask = Enumerable.Repeat(1L, tokenIds.Count).ToArray();

        return new TokenizedText
        {
            InputIds = tokenIds.ToArray(),
            AttentionMask = attentionMask
        };
    }

    /// <inheritdoc />
    /// <exception cref="NotSupportedException">Always thrown. SimpleTokenizer cannot decode.</exception>
    public string Decode(long[] tokenIds)
    {
        throw new NotSupportedException(
            "SimpleTokenizer does not support decoding. " +
            "Use HuggingFaceTokenizerService for full encode/decode support.");
    }

    /// <inheritdoc />
    public int CountTokens(string text)
    {
        if (string.IsNullOrEmpty(text))
            return 0;

        return _tokenPattern.Matches(text).Count;
    }

    #endregion
}
```

### 2. HuggingFaceTokenizerService.cs

**Location**: `src/SeniorIntern.Services/Embedding/HuggingFaceTokenizerService.cs`

```csharp
namespace SeniorIntern.Services.Embedding;

using System;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Text.Json;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.Extensions.Logging;
using SeniorIntern.Core.Interfaces;

/// <summary>
/// Tokenizer service for HuggingFace-style tokenizer.json files.
/// </summary>
/// <remarks>
/// <para>
/// Loads and parses HuggingFace tokenizer.json files, which contain:
/// <list type="bullet">
///   <item>Vocabulary mapping (token → ID)</item>
///   <item>Special tokens ([PAD], [CLS], [SEP], [UNK])</item>
///   <item>Tokenization rules for WordPiece subword splitting</item>
/// </list>
/// </para>
/// <para>
/// Supports BERT-style models with:
/// <list type="bullet">
///   <item>[CLS] token at start of sequence</item>
///   <item>[SEP] token at end of sequence</item>
///   <item>[PAD] token for padding</item>
///   <item>[UNK] token for unknown words</item>
///   <item>WordPiece subword tokenization (## prefix)</item>
/// </list>
/// </para>
/// </remarks>
public sealed class HuggingFaceTokenizerService : ITokenizerService
{
    private readonly ILogger<HuggingFaceTokenizerService> _logger;

    /// <summary>
    /// Vocabulary mapping: token string → token ID.
    /// </summary>
    private Dictionary<string, int>? _vocab;

    /// <summary>
    /// Reverse vocabulary mapping: token ID → token string.
    /// </summary>
    private Dictionary<int, string>? _reverseVocab;

    /// <summary>
    /// Special token IDs.
    /// </summary>
    private int _padTokenId;
    private int _clsTokenId;
    private int _sepTokenId;
    private int _unkTokenId;

    #region Properties

    /// <inheritdoc />
    public bool IsLoaded => _vocab is not null;

    #endregion

    #region Constructor

    /// <summary>
    /// Initializes a new instance of the HuggingFaceTokenizerService.
    /// </summary>
    /// <param name="logger">Logger instance.</param>
    public HuggingFaceTokenizerService(ILogger<HuggingFaceTokenizerService> logger)
    {
        _logger = logger ?? throw new ArgumentNullException(nameof(logger));

        // Default special token IDs (common for BERT-style models)
        // These are overridden when tokenizer.json is loaded
        _padTokenId = 0;
        _clsTokenId = 101;
        _sepTokenId = 102;
        _unkTokenId = 100;
    }

    #endregion

    #region ITokenizerService Implementation

    /// <inheritdoc />
    public async Task LoadTokenizerAsync(string path, CancellationToken ct = default)
    {
        ArgumentNullException.ThrowIfNull(path);

        if (!File.Exists(path))
            throw new FileNotFoundException($"Tokenizer file not found: {path}", path);

        _logger.LogInformation("Loading tokenizer from {Path}", path);

        var json = await File.ReadAllTextAsync(path, ct);
        var doc = JsonDocument.Parse(json);

        // Parse vocabulary from model section
        _vocab = new Dictionary<string, int>();
        _reverseVocab = new Dictionary<int, string>();

        if (doc.RootElement.TryGetProperty("model", out var model) &&
            model.TryGetProperty("vocab", out var vocab))
        {
            foreach (var item in vocab.EnumerateObject())
            {
                var id = item.Value.GetInt32();
                _vocab[item.Name] = id;
                _reverseVocab[id] = item.Name;
            }
        }

        // Parse special tokens from added_tokens section
        if (doc.RootElement.TryGetProperty("added_tokens", out var addedTokens))
        {
            foreach (var token in addedTokens.EnumerateArray())
            {
                var content = token.GetProperty("content").GetString();
                var id = token.GetProperty("id").GetInt32();

                switch (content)
                {
                    case "[PAD]": _padTokenId = id; break;
                    case "[CLS]": _clsTokenId = id; break;
                    case "[SEP]": _sepTokenId = id; break;
                    case "[UNK]": _unkTokenId = id; break;
                }
            }
        }

        _logger.LogInformation(
            "Loaded tokenizer with {Count} tokens (PAD={Pad}, CLS={Cls}, SEP={Sep}, UNK={Unk})",
            _vocab.Count, _padTokenId, _clsTokenId, _sepTokenId, _unkTokenId);
    }

    /// <inheritdoc />
    public TokenizedText Encode(string text, int maxLength)
    {
        if (_vocab is null)
            throw new InvalidOperationException("Tokenizer not loaded. Call LoadTokenizerAsync first.");

        if (string.IsNullOrEmpty(text))
        {
            return new TokenizedText
            {
                InputIds = new[] { (long)_clsTokenId, (long)_sepTokenId },
                AttentionMask = new[] { 1L, 1L }
            };
        }

        // Start with [CLS] token
        var tokens = new List<long> { _clsTokenId };

        // Split text into words and tokenize each
        var words = text.Split(' ', StringSplitOptions.RemoveEmptyEntries);
        foreach (var word in words)
        {
            if (tokens.Count >= maxLength - 1)
                break;

            var normalizedWord = word.ToLowerInvariant();

            // Try to find exact match in vocabulary
            if (_vocab.TryGetValue(normalizedWord, out var tokenId))
            {
                tokens.Add(tokenId);
            }
            else
            {
                // Fall back to WordPiece subword tokenization
                var subwords = TokenizeWordPiece(normalizedWord, maxLength - tokens.Count - 1);
                tokens.AddRange(subwords);
            }
        }

        // End with [SEP] token
        tokens.Add(_sepTokenId);

        // Create attention mask (all 1s for real tokens)
        var attentionMask = new long[tokens.Count];
        Array.Fill(attentionMask, 1L);

        return new TokenizedText
        {
            InputIds = tokens.ToArray(),
            AttentionMask = attentionMask
        };
    }

    /// <inheritdoc />
    public string Decode(long[] tokenIds)
    {
        if (_reverseVocab is null)
            throw new InvalidOperationException("Tokenizer not loaded. Call LoadTokenizerAsync first.");

        var tokens = new List<string>();

        foreach (var id in tokenIds)
        {
            if (_reverseVocab.TryGetValue((int)id, out var token))
            {
                // Skip special tokens in output
                if (token is "[CLS]" or "[SEP]" or "[PAD]")
                    continue;

                // Handle WordPiece continuation tokens
                if (token.StartsWith("##"))
                    token = token[2..];

                tokens.Add(token);
            }
        }

        return string.Join(" ", tokens);
    }

    /// <inheritdoc />
    public int CountTokens(string text)
    {
        if (_vocab is null)
        {
            // Rough estimate if tokenizer not loaded
            return text.Split(' ', StringSplitOptions.RemoveEmptyEntries).Length;
        }

        var encoded = Encode(text, int.MaxValue);
        return encoded.InputIds.Length;
    }

    #endregion

    #region WordPiece Tokenization

    /// <summary>
    /// Tokenize a word using WordPiece algorithm.
    /// </summary>
    /// <param name="word">Word to tokenize.</param>
    /// <param name="maxSubwords">Maximum number of subwords to produce.</param>
    /// <returns>Sequence of token IDs.</returns>
    private IEnumerable<long> TokenizeWordPiece(string word, int maxSubwords)
    {
        var subwords = new List<long>();
        var remaining = word;
        var isFirst = true;

        while (remaining.Length > 0 && subwords.Count < maxSubwords)
        {
            var found = false;

            // Try progressively shorter substrings
            for (int len = remaining.Length; len > 0; len--)
            {
                var subword = remaining[..len];

                // Add ## prefix for continuation tokens
                if (!isFirst)
                    subword = "##" + subword;

                if (_vocab!.TryGetValue(subword, out var tokenId))
                {
                    subwords.Add(tokenId);
                    remaining = remaining[len..];
                    found = true;
                    isFirst = false;
                    break;
                }
            }

            if (!found)
            {
                // Unknown character - use [UNK] token
                subwords.Add(_unkTokenId);
                remaining = remaining.Length > 1 ? remaining[1..] : "";
            }
        }

        return subwords;
    }

    #endregion
}
```

### 3. TokenizerFactory.cs

**Location**: `src/SeniorIntern.Services/Embedding/TokenizerFactory.cs`

```csharp
namespace SeniorIntern.Services.Embedding;

using System;
using System.IO;
using Microsoft.Extensions.DependencyInjection;
using Microsoft.Extensions.Logging;
using SeniorIntern.Core.Interfaces;

/// <summary>
/// Factory for creating tokenizer instances.
/// </summary>
/// <remarks>
/// <para>
/// Selects the appropriate tokenizer based on available resources:
/// <list type="number">
///   <item>If modelPath provided and tokenizer.json exists → HuggingFaceTokenizerService</item>
///   <item>Otherwise → SimpleTokenizer (rule-based fallback)</item>
/// </list>
/// </para>
/// <para>
/// Usage example:
/// <code>
/// var factory = serviceProvider.GetRequiredService&lt;TokenizerFactory&gt;();
/// var tokenizer = factory.CreateTokenizer("/models/bert/model.onnx");
/// var count = tokenizer.CountTokens("Hello world");
/// </code>
/// </para>
/// </remarks>
public sealed class TokenizerFactory
{
    private readonly IServiceProvider _serviceProvider;
    private readonly ILogger<TokenizerFactory> _logger;

    /// <summary>
    /// Initializes a new instance of the TokenizerFactory.
    /// </summary>
    /// <param name="serviceProvider">Service provider for dependency resolution.</param>
    /// <param name="logger">Logger instance.</param>
    public TokenizerFactory(
        IServiceProvider serviceProvider,
        ILogger<TokenizerFactory> logger)
    {
        _serviceProvider = serviceProvider ?? throw new ArgumentNullException(nameof(serviceProvider));
        _logger = logger ?? throw new ArgumentNullException(nameof(logger));
    }

    /// <summary>
    /// Create a tokenizer for the given model path.
    /// </summary>
    /// <param name="modelPath">
    /// Optional path to the model file. If provided, the factory will look for
    /// a tokenizer.json file in the same directory.
    /// </param>
    /// <returns>An appropriate ITokenizerService implementation.</returns>
    /// <remarks>
    /// The factory checks for tokenizer.json in the model's directory.
    /// Common locations for tokenizer.json:
    /// <list type="bullet">
    ///   <item>/path/to/model/tokenizer.json</item>
    ///   <item>/path/to/model/tokenizer_config.json</item>
    /// </list>
    /// </remarks>
    public ITokenizerService CreateTokenizer(string? modelPath = null)
    {
        // Try to find HuggingFace tokenizer.json if model path provided
        if (modelPath is not null)
        {
            var modelDir = Path.GetDirectoryName(modelPath);
            if (!string.IsNullOrEmpty(modelDir))
            {
                var tokenizerPath = Path.Combine(modelDir, "tokenizer.json");

                if (File.Exists(tokenizerPath))
                {
                    _logger.LogInformation(
                        "Creating HuggingFace tokenizer from {Path}",
                        tokenizerPath);

                    return _serviceProvider.GetRequiredService<HuggingFaceTokenizerService>();
                }

                _logger.LogDebug(
                    "No tokenizer.json found in {Dir}, falling back to SimpleTokenizer",
                    modelDir);
            }
        }

        // Fall back to simple tokenizer
        _logger.LogDebug("Using SimpleTokenizer for token estimation");
        return _serviceProvider.GetRequiredService<SimpleTokenizer>();
    }

    /// <summary>
    /// Create a tokenizer and automatically load it if a path is available.
    /// </summary>
    /// <param name="modelPath">Path to the model file.</param>
    /// <returns>A loaded ITokenizerService instance.</returns>
    public async Task<ITokenizerService> CreateAndLoadTokenizerAsync(
        string? modelPath = null,
        CancellationToken ct = default)
    {
        var tokenizer = CreateTokenizer(modelPath);

        if (modelPath is not null && tokenizer is HuggingFaceTokenizerService)
        {
            var tokenizerPath = Path.Combine(
                Path.GetDirectoryName(modelPath)!,
                "tokenizer.json");

            await tokenizer.LoadTokenizerAsync(tokenizerPath, ct);
        }

        return tokenizer;
    }
}
```

### 4. TokenEstimator.cs

**Location**: `src/SeniorIntern.Services/Embedding/TokenEstimator.cs`

```csharp
namespace SeniorIntern.Services.Embedding;

using System;

/// <summary>
/// Utility for estimating token counts without a full tokenizer.
/// </summary>
/// <remarks>
/// <para>
/// Provides quick token count estimates based on character ratios:
/// <list type="bullet">
///   <item>Code: ~4 characters per token (more symbols and short identifiers)</item>
///   <item>Text: ~4.5 characters per token (natural language, longer words)</item>
///   <item>Adaptive: Analyzes content to choose appropriate ratio</item>
/// </list>
/// </para>
/// <para>
/// These estimates are approximations. For accurate counts, use a proper tokenizer.
/// Variance from actual token counts is typically 10-20%.
/// </para>
/// </remarks>
public static class TokenEstimator
{
    #region Estimation Methods

    /// <summary>
    /// Estimate token count for code/technical text.
    /// </summary>
    /// <param name="text">Text to estimate.</param>
    /// <returns>Estimated token count.</returns>
    /// <remarks>
    /// Uses approximately 4 characters per token, which is typical for code
    /// due to frequent short identifiers, operators, and symbols.
    /// </remarks>
    public static int EstimateForCode(string text)
    {
        if (string.IsNullOrEmpty(text))
            return 0;

        return (int)Math.Ceiling(text.Length / 4.0);
    }

    /// <summary>
    /// Estimate token count for natural language text.
    /// </summary>
    /// <param name="text">Text to estimate.</param>
    /// <returns>Estimated token count.</returns>
    /// <remarks>
    /// Uses approximately 4.5 characters per token, which is typical for English
    /// text due to longer words and more regular structure.
    /// </remarks>
    public static int EstimateForText(string text)
    {
        if (string.IsNullOrEmpty(text))
            return 0;

        return (int)Math.Ceiling(text.Length / 4.5);
    }

    /// <summary>
    /// Estimate token count with more accuracy by analyzing content type.
    /// </summary>
    /// <param name="text">Text to estimate.</param>
    /// <returns>Estimated token count.</returns>
    /// <remarks>
    /// <para>
    /// Analyzes the distribution of character types to determine content type:
    /// <list type="bullet">
    ///   <item>&gt;15% symbols → likely code (3.5 chars/token)</item>
    ///   <item>&gt;20% digits → likely data/config (3.0 chars/token)</item>
    ///   <item>Otherwise → natural text (4.5 chars/token)</item>
    /// </list>
    /// </para>
    /// </remarks>
    public static int EstimateAdaptive(string text)
    {
        if (string.IsNullOrEmpty(text))
            return 0;

        // Count different character types
        int letters = 0, digits = 0, symbols = 0, spaces = 0;

        foreach (var c in text)
        {
            if (char.IsLetter(c))
                letters++;
            else if (char.IsDigit(c))
                digits++;
            else if (char.IsWhiteSpace(c))
                spaces++;
            else
                symbols++;
        }

        var total = text.Length;

        // Determine content type and select appropriate ratio
        double charsPerToken;

        if (symbols > total * 0.15)
        {
            // Lots of symbols = likely code
            charsPerToken = 3.5;
        }
        else if (digits > total * 0.2)
        {
            // Lots of numbers = likely data/config
            charsPerToken = 3.0;
        }
        else
        {
            // Mostly text
            charsPerToken = 4.5;
        }

        return (int)Math.Ceiling(total / charsPerToken);
    }

    #endregion

    #region Conversion Utilities

    /// <summary>
    /// Calculate how many tokens fit in a given character limit.
    /// </summary>
    /// <param name="characterCount">Number of characters.</param>
    /// <param name="mode">Content type mode.</param>
    /// <returns>Estimated token count.</returns>
    public static int CharactersToTokens(
        int characterCount,
        TokenEstimationMode mode = TokenEstimationMode.Code)
    {
        var charsPerToken = GetCharsPerToken(mode);
        return (int)Math.Ceiling(characterCount / charsPerToken);
    }

    /// <summary>
    /// Calculate character count for a target token count.
    /// </summary>
    /// <param name="tokenCount">Target number of tokens.</param>
    /// <param name="mode">Content type mode.</param>
    /// <returns>Estimated character count.</returns>
    public static int TokensToCharacters(
        int tokenCount,
        TokenEstimationMode mode = TokenEstimationMode.Code)
    {
        var charsPerToken = GetCharsPerToken(mode);
        return (int)(tokenCount * charsPerToken);
    }

    /// <summary>
    /// Get characters per token ratio for a mode.
    /// </summary>
    private static double GetCharsPerToken(TokenEstimationMode mode)
    {
        return mode switch
        {
            TokenEstimationMode.Code => 4.0,
            TokenEstimationMode.Text => 4.5,
            TokenEstimationMode.Mixed => 4.0,
            TokenEstimationMode.Data => 3.0,
            _ => 4.0
        };
    }

    #endregion
}

/// <summary>
/// Mode for token estimation.
/// </summary>
/// <remarks>
/// Different content types have different token densities.
/// Select the appropriate mode for more accurate estimates.
/// </remarks>
public enum TokenEstimationMode
{
    /// <summary>
    /// Code/technical content (~4 chars/token).
    /// Typical for source code with short identifiers and symbols.
    /// </summary>
    Code,

    /// <summary>
    /// Natural language text (~4.5 chars/token).
    /// Typical for English prose and documentation.
    /// </summary>
    Text,

    /// <summary>
    /// Mixed content (~4 chars/token).
    /// Use when content type is unknown or mixed.
    /// </summary>
    Mixed,

    /// <summary>
    /// Data/configuration content (~3 chars/token).
    /// Typical for JSON, numbers, and structured data.
    /// </summary>
    Data
}
```

---

## DI Registration Extension

```csharp
/// <summary>
/// Extension methods for registering tokenization services.
/// </summary>
public static class TokenizationServiceExtensions
{
    /// <summary>
    /// Add tokenization services to the service collection.
    /// </summary>
    public static IServiceCollection AddTokenizationServices(this IServiceCollection services)
    {
        // Register tokenizer implementations
        services.AddSingleton<SimpleTokenizer>();
        services.AddSingleton<HuggingFaceTokenizerService>();

        // Register factory
        services.AddSingleton<TokenizerFactory>();

        // Register ITokenizerService with SimpleTokenizer as default
        services.AddSingleton<ITokenizerService>(sp =>
            sp.GetRequiredService<SimpleTokenizer>());

        return services;
    }
}
```

---

## Token Estimation Ratios

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                     Token Estimation Ratios by Content Type                   │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Content Type     │ Chars/Token │ Typical Content                           │
│  ─────────────────│─────────────│───────────────────────────────────────── │
│  Code             │ 4.0         │ Source code, short identifiers           │
│  Text             │ 4.5         │ English prose, documentation             │
│  Data             │ 3.0         │ JSON, CSV, numbers                        │
│  Mixed            │ 4.0         │ Combination of above                      │
│                                                                              │
│  Adaptive Analysis Thresholds:                                               │
│  - symbols > 15% of total → Code mode (3.5 chars/token)                     │
│  - digits > 20% of total → Data mode (3.0 chars/token)                      │
│  - otherwise → Text mode (4.5 chars/token)                                  │
│                                                                              │
│  Example Estimates:                                                          │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ Content                           │ Chars │ Est. Tokens │ Mode        │ │
│  │ ─────────────────────────────────│───────│─────────────│───────────  │ │
│  │ "public void Main() { }"         │ 23    │ 6           │ Code        │ │
│  │ "Hello, world! This is a test."  │ 31    │ 7           │ Text        │ │
│  │ '{"name": "John", "age": 30}'    │ 28    │ 10          │ Data        │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
└──────────────────────────────────────────────────────────────────────────────┘
```

---

## Unit Test Plan

| Test | Description |
|------|-------------|
| `SimpleTokenizer_IsLoaded_AlwaysTrue` | Property verification |
| `SimpleTokenizer_CountTokens_EmptyString_ReturnsZero` | Empty input |
| `SimpleTokenizer_CountTokens_Code_ReturnsReasonableCount` | Code tokenization |
| `SimpleTokenizer_CountTokens_Text_ReturnsReasonableCount` | Text tokenization |
| `SimpleTokenizer_Encode_ReturnsTokenIds` | Encoding works |
| `SimpleTokenizer_Encode_RespectsMaxLength` | Truncation works |
| `SimpleTokenizer_Decode_ThrowsNotSupported` | Expected exception |
| `HuggingFace_LoadTokenizer_ParsesVocab` | Vocabulary parsing |
| `HuggingFace_LoadTokenizer_ParsesSpecialTokens` | Special token IDs |
| `HuggingFace_Encode_AddsClsAndSep` | BERT-style wrapping |
| `HuggingFace_Encode_HandlesWordPiece` | Subword tokenization |
| `HuggingFace_Decode_RemovesSpecialTokens` | Proper decoding |
| `HuggingFace_Decode_HandlesWordPieceContinuation` | ## token handling |
| `TokenizerFactory_NoPath_ReturnsSimple` | Fallback behavior |
| `TokenizerFactory_WithTokenizerJson_ReturnsHuggingFace` | Model detection |
| `TokenEstimator_ForCode_UsesCorrectRatio` | 4.0 chars/token |
| `TokenEstimator_ForText_UsesCorrectRatio` | 4.5 chars/token |
| `TokenEstimator_Adaptive_DetectsCode` | Symbol detection |
| `TokenEstimator_Adaptive_DetectsData` | Digit detection |
| `TokenEstimator_CharactersToTokens_Correct` | Conversion |
| `TokenEstimator_TokensToCharacters_Correct` | Reverse conversion |

---

## File Summary

| File | Location | Purpose | Lines |
|------|----------|---------|-------|
| `SimpleTokenizer.cs` | `Services/Embedding/` | Rule-based tokenizer | ~100 |
| `HuggingFaceTokenizerService.cs` | `Services/Embedding/` | Vocabulary tokenizer | ~200 |
| `TokenizerFactory.cs` | `Services/Embedding/` | Tokenizer selection | ~80 |
| `TokenEstimator.cs` | `Services/Embedding/` | Static utilities | ~140 |

---

## Acceptance Criteria

| ID | Criterion |
|----|-----------|
| AC-1 | SimpleTokenizer provides basic tokenization |
| AC-2 | SimpleTokenizer.IsLoaded always returns true |
| AC-3 | HuggingFaceTokenizerService loads tokenizer.json |
| AC-4 | HuggingFaceTokenizerService parses vocabulary |
| AC-5 | HuggingFaceTokenizerService handles WordPiece |
| AC-6 | TokenizerFactory selects appropriate tokenizer |
| AC-7 | TokenEstimator provides quick estimates |
| AC-8 | Adaptive estimation based on content type |

---

## Changelog Entry

```markdown
## v0.7.1i - Tokenization Utilities

### Added
- `SimpleTokenizer` class
  - Rule-based token counting via regex
  - IsLoaded always returns true
  - Patterns: numbers, identifiers, strings, comments, operators
  - Suitable for estimation when no model tokenizer available
  - Decode throws NotSupportedException (limitation)
- `HuggingFaceTokenizerService` class
  - Loads HuggingFace tokenizer.json files
  - Parses vocabulary and special tokens
  - BERT-style encoding with [CLS] and [SEP]
  - WordPiece subword tokenization (## prefix)
  - Full encode/decode support
- `TokenizerFactory` class
  - CreateTokenizer(modelPath) - selects appropriate implementation
  - CreateAndLoadTokenizerAsync() - creates and loads in one call
  - Falls back to SimpleTokenizer when no tokenizer.json
- `TokenEstimator` static class
  - EstimateForCode() - ~4 chars/token
  - EstimateForText() - ~4.5 chars/token
  - EstimateAdaptive() - content-type-aware estimation
  - CharactersToTokens() / TokensToCharacters() - conversion utilities
- `TokenEstimationMode` enum
  - Code, Text, Mixed, Data modes
  - Different char/token ratios per content type

### Token Estimation Ratios
| Mode | Chars/Token | Use Case |
|------|-------------|----------|
| Code | 4.0 | Source code |
| Text | 4.5 | Natural language |
| Data | 3.0 | JSON, numbers |
| Mixed | 4.0 | Unknown content |

### Integration
- Used by ChunkingService.EstimateTokenCount()
- TokenizerFactory registered in DI
- SimpleTokenizer as default ITokenizerService
```

---

## Timeline Estimate

| Version | Estimated Effort |
|---------|------------------|
| v0.7.1i | 0.5 day |
