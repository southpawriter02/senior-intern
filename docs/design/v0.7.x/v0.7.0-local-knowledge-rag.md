# Design Specification: AIntern v0.7.0 "Local Knowledge (RAG)"

## Executive Summary

This document provides a comprehensive design specification for v0.7.0, which introduces Retrieval-Augmented Generation (RAG) capabilities to The Senior Intern. This phase enables the assistant to index entire codebases into a local vector database, allowing it to answer questions like "Where is the database connection handled?" without requiring files to be manually opened or attached.

### v0.7.0 Scope (from ROADMAP.md)
- Integration of a local vector database (like **Qdrant** or **FAISS** via C# wrappers)
- "Index Project" feature: The Intern scans your entire codebase once, allowing it to answer questions like "Where is the database connection handled?" without the file being open

---

## Sub-Version Breakdown

| Version | Name | Focus |
|---------|------|-------|
| v0.7.1 | Embedding Foundation | Embedding model integration, text chunking, vector representation |
| v0.7.2 | Vector Storage | SQLite-vec integration, index management, CRUD operations |
| v0.7.3 | Indexing Pipeline | File scanning, incremental indexing, progress tracking |
| v0.7.4 | Retrieval & Context | Semantic search, context assembly, relevance scoring |
| v0.7.5 | UI & Integration | Index management UI, auto-indexing, settings, polish |

---

## Architecture Overview

### RAG System Architecture

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                              User Interface                                  │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐             │
│  │  Index Manager  │  │  Search Panel   │  │  Chat + RAG     │             │
│  │     Dialog      │  │   (optional)    │  │   Integration   │             │
│  └────────┬────────┘  └────────┬────────┘  └────────┬────────┘             │
└───────────┼────────────────────┼────────────────────┼───────────────────────┘
            │                    │                    │
            ▼                    ▼                    ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                           RAG Orchestrator                                   │
│  ┌──────────────────────────────────────────────────────────────────────┐  │
│  │                        IKnowledgeService                              │  │
│  │  ┌────────────────┐  ┌────────────────┐  ┌────────────────┐         │  │
│  │  │  Index Manager │  │    Retriever   │  │ Context Builder│         │  │
│  │  └────────────────┘  └────────────────┘  └────────────────┘         │  │
│  └──────────────────────────────────────────────────────────────────────┘  │
│                                    │                                        │
│                                    ▼                                        │
│  ┌──────────────────────────────────────────────────────────────────────┐  │
│  │                        Processing Pipeline                            │  │
│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐   │  │
│  │  │  File   │  │  Text   │  │ Chunker │  │Embedding│  │  Store  │   │  │
│  │  │ Scanner │→ │ Extract │→ │         │→ │  Model  │→ │         │   │  │
│  │  └─────────┘  └─────────┘  └─────────┘  └─────────┘  └─────────┘   │  │
│  └──────────────────────────────────────────────────────────────────────┘  │
│                                    │                                        │
│                                    ▼                                        │
│  ┌──────────────────────────────────────────────────────────────────────┐  │
│  │                         Storage Layer                                 │  │
│  │  ┌────────────────────────┐  ┌────────────────────────┐             │  │
│  │  │     SQLite-vec         │  │    File Metadata DB    │             │  │
│  │  │  (Vector Storage)      │  │    (SQLite + EF Core)  │             │  │
│  │  └────────────────────────┘  └────────────────────────┘             │  │
│  └──────────────────────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────────────────────┘
```

### RAG Query Flow

```
User Question: "Where is the database connection handled?"
     │
     ▼
┌─────────────────┐
│  Generate Query │ ← Convert question to embedding
│    Embedding    │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  Vector Search  │ ← Find top-k similar chunks
│  (SQLite-vec)   │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  Retrieve Full  │ ← Load complete code context
│    Contexts     │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  Rank & Filter  │ ← Apply relevance threshold, dedup
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│ Assemble Prompt │ ← Combine retrieved context with question
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  LLM Inference  │ ← Generate answer with context
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│    Response     │ ← "The database connection is handled in
│    to User      │    src/Data/DbContext.cs at line 42..."
└─────────────────┘
```

---

## v0.7.1: Embedding Foundation

### Objective
Establish the embedding infrastructure using a local embedding model, implement text chunking strategies optimized for code, and define the vector representation format.

### Embedding Model Selection

For local-first operation, we use **LLamaSharp's embedding capabilities** or a dedicated small embedding model (e.g., `nomic-embed-text`, `all-MiniLM-L6-v2` via ONNX).

#### IEmbeddingService Interface

```csharp
namespace SeniorIntern.Core.Interfaces;

/// <summary>
/// Service for generating text embeddings
/// </summary>
public interface IEmbeddingService : IAsyncDisposable
{
    /// <summary>
    /// Whether an embedding model is currently loaded
    /// </summary>
    bool IsModelLoaded { get; }

    /// <summary>
    /// Dimension of the embedding vectors
    /// </summary>
    int EmbeddingDimension { get; }

    /// <summary>
    /// Maximum tokens per embedding request
    /// </summary>
    int MaxTokens { get; }

    /// <summary>
    /// Load an embedding model
    /// </summary>
    Task LoadModelAsync(
        EmbeddingModelOptions options,
        IProgress<ModelLoadProgress>? progress = null,
        CancellationToken ct = default);

    /// <summary>
    /// Unload the current model
    /// </summary>
    Task UnloadModelAsync();

    /// <summary>
    /// Generate embedding for a single text
    /// </summary>
    Task<float[]> EmbedAsync(string text, CancellationToken ct = default);

    /// <summary>
    /// Generate embeddings for multiple texts (batched)
    /// </summary>
    Task<IReadOnlyList<float[]>> EmbedBatchAsync(
        IEnumerable<string> texts,
        IProgress<EmbeddingProgress>? progress = null,
        CancellationToken ct = default);

    /// <summary>
    /// Event when model state changes
    /// </summary>
    event EventHandler<EmbeddingModelStateChangedEventArgs>? ModelStateChanged;
}

public sealed class EmbeddingModelOptions
{
    /// <summary>
    /// Path to the embedding model (GGUF or ONNX)
    /// </summary>
    public string ModelPath { get; init; } = string.Empty;

    /// <summary>
    /// Model type for loading strategy
    /// </summary>
    public EmbeddingModelType ModelType { get; init; } = EmbeddingModelType.Gguf;

    /// <summary>
    /// Number of GPU layers (-1 for auto)
    /// </summary>
    public int GpuLayers { get; init; } = -1;

    /// <summary>
    /// Batch size for embedding generation
    /// </summary>
    public int BatchSize { get; init; } = 32;

    /// <summary>
    /// Whether to normalize embeddings to unit length
    /// </summary>
    public bool NormalizeEmbeddings { get; init; } = true;
}

public enum EmbeddingModelType
{
    Gguf,       // LLamaSharp embedding model
    Onnx,       // ONNX Runtime model
    SentenceTransformers  // Via ML.NET
}

public sealed class EmbeddingProgress
{
    public int ProcessedCount { get; init; }
    public int TotalCount { get; init; }
    public double PercentComplete => TotalCount > 0 ? (double)ProcessedCount / TotalCount * 100 : 0;
}
```

### Text Chunking

Code requires specialized chunking that respects language structure.

#### IChunkingService Interface

```csharp
namespace SeniorIntern.Core.Interfaces;

/// <summary>
/// Service for chunking text/code into embeddable segments
/// </summary>
public interface IChunkingService
{
    /// <summary>
    /// Chunk a document into segments
    /// </summary>
    IReadOnlyList<TextChunk> ChunkDocument(
        string content,
        string filePath,
        ChunkingOptions options);

    /// <summary>
    /// Get the appropriate chunking strategy for a file type
    /// </summary>
    IChunkingStrategy GetStrategy(string filePath);
}

public interface IChunkingStrategy
{
    /// <summary>
    /// Supported file extensions
    /// </summary>
    IReadOnlySet<string> SupportedExtensions { get; }

    /// <summary>
    /// Chunk content using this strategy
    /// </summary>
    IReadOnlyList<TextChunk> Chunk(
        string content,
        string filePath,
        ChunkingOptions options);
}

public sealed class ChunkingOptions
{
    /// <summary>
    /// Target chunk size in tokens (approximate)
    /// </summary>
    public int TargetChunkSize { get; init; } = 512;

    /// <summary>
    /// Overlap between chunks in tokens
    /// </summary>
    public int ChunkOverlap { get; init; } = 64;

    /// <summary>
    /// Minimum chunk size (skip smaller chunks)
    /// </summary>
    public int MinChunkSize { get; init; } = 50;

    /// <summary>
    /// Maximum chunk size (hard limit)
    /// </summary>
    public int MaxChunkSize { get; init; } = 1024;

    /// <summary>
    /// Whether to include file path in chunk metadata
    /// </summary>
    public bool IncludeFilePath { get; init; } = true;

    /// <summary>
    /// Whether to include line numbers
    /// </summary>
    public bool IncludeLineNumbers { get; init; } = true;
}
```

#### TextChunk Model

```csharp
namespace SeniorIntern.Core.Models;

/// <summary>
/// A chunk of text/code with metadata for embedding and retrieval
/// </summary>
public sealed class TextChunk
{
    /// <summary>
    /// Unique identifier for this chunk
    /// </summary>
    public Guid Id { get; init; } = Guid.NewGuid();

    /// <summary>
    /// The actual text content
    /// </summary>
    public string Content { get; init; } = string.Empty;

    /// <summary>
    /// Source file path (relative to workspace)
    /// </summary>
    public string FilePath { get; init; } = string.Empty;

    /// <summary>
    /// Starting line number (1-based)
    /// </summary>
    public int StartLine { get; init; }

    /// <summary>
    /// Ending line number (1-based)
    /// </summary>
    public int EndLine { get; init; }

    /// <summary>
    /// Character offset in file
    /// </summary>
    public int StartOffset { get; init; }

    /// <summary>
    /// End character offset
    /// </summary>
    public int EndOffset { get; init; }

    /// <summary>
    /// Chunk type for semantic understanding
    /// </summary>
    public ChunkType Type { get; init; } = ChunkType.Code;

    /// <summary>
    /// Programming language (if applicable)
    /// </summary>
    public string? Language { get; init; }

    /// <summary>
    /// Symbol name if this chunk represents a specific symbol
    /// </summary>
    public string? SymbolName { get; init; }

    /// <summary>
    /// Symbol type (class, method, function, etc.)
    /// </summary>
    public SymbolType? SymbolType { get; init; }

    /// <summary>
    /// Parent symbol (e.g., class containing a method)
    /// </summary>
    public string? ParentSymbol { get; init; }

    /// <summary>
    /// Approximate token count
    /// </summary>
    public int TokenCount { get; init; }

    /// <summary>
    /// Additional metadata
    /// </summary>
    public IReadOnlyDictionary<string, string>? Metadata { get; init; }
}

public enum ChunkType
{
    Code,
    Comment,
    Documentation,
    Markdown,
    PlainText,
    Config,
    Mixed
}

public enum SymbolType
{
    Namespace,
    Class,
    Struct,
    Interface,
    Enum,
    Method,
    Function,
    Property,
    Field,
    Constructor,
    Event,
    Delegate,
    Import,
    Other
}
```

### Chunking Strategies

#### Code-Aware Chunking Strategy

```csharp
namespace SeniorIntern.Services.Chunking;

/// <summary>
/// Chunking strategy that respects code structure (classes, methods, etc.)
/// </summary>
public sealed class CodeAwareChunkingStrategy : IChunkingStrategy
{
    private static readonly HashSet<string> _supportedExtensions = new(StringComparer.OrdinalIgnoreCase)
    {
        ".cs", ".fs", ".vb",           // .NET
        ".ts", ".tsx", ".js", ".jsx",  // JavaScript/TypeScript
        ".py",                          // Python
        ".java", ".kt",                 // JVM
        ".go",                          // Go
        ".rs",                          // Rust
        ".cpp", ".hpp", ".c", ".h",    // C/C++
        ".swift",                       // Swift
        ".rb",                          // Ruby
        ".php"                          // PHP
    };

    public IReadOnlySet<string> SupportedExtensions => _supportedExtensions;

    public IReadOnlyList<TextChunk> Chunk(
        string content,
        string filePath,
        ChunkingOptions options)
    {
        var extension = Path.GetExtension(filePath).ToLowerInvariant();
        var language = GetLanguageFromExtension(extension);

        // Try semantic chunking first (by symbols)
        var semanticChunks = TrySemanticChunk(content, filePath, language, options);
        if (semanticChunks.Count > 0)
            return semanticChunks;

        // Fall back to line-based chunking with overlap
        return LineBasedChunk(content, filePath, language, options);
    }

    private IReadOnlyList<TextChunk> TrySemanticChunk(
        string content,
        string filePath,
        string language,
        ChunkingOptions options)
    {
        var chunks = new List<TextChunk>();
        var symbols = ExtractSymbols(content, language);

        foreach (var symbol in symbols)
        {
            var symbolContent = content.Substring(symbol.StartOffset, symbol.Length);
            var tokenCount = EstimateTokenCount(symbolContent);

            if (tokenCount < options.MinChunkSize)
                continue;

            if (tokenCount <= options.MaxChunkSize)
            {
                // Symbol fits in one chunk
                chunks.Add(CreateChunk(symbolContent, filePath, symbol, language, tokenCount));
            }
            else
            {
                // Symbol too large, split it
                chunks.AddRange(SplitLargeSymbol(
                    symbolContent, filePath, symbol, language, options));
            }
        }

        return chunks;
    }

    private IReadOnlyList<TextChunk> LineBasedChunk(
        string content,
        string filePath,
        string language,
        ChunkingOptions options)
    {
        var chunks = new List<TextChunk>();
        var lines = content.Split('\n');
        var currentChunk = new StringBuilder();
        var startLine = 1;
        var currentTokenCount = 0;

        for (int i = 0; i < lines.Length; i++)
        {
            var line = lines[i];
            var lineTokens = EstimateTokenCount(line);

            if (currentTokenCount + lineTokens > options.TargetChunkSize && currentChunk.Length > 0)
            {
                // Emit current chunk
                chunks.Add(new TextChunk
                {
                    Content = currentChunk.ToString(),
                    FilePath = filePath,
                    StartLine = startLine,
                    EndLine = i,
                    Language = language,
                    Type = ChunkType.Code,
                    TokenCount = currentTokenCount
                });

                // Start new chunk with overlap
                var overlapLines = CalculateOverlapLines(lines, i, options.ChunkOverlap);
                currentChunk.Clear();
                foreach (var ol in overlapLines)
                    currentChunk.AppendLine(ol);

                startLine = i - overlapLines.Count + 1;
                currentTokenCount = EstimateTokenCount(currentChunk.ToString());
            }

            currentChunk.AppendLine(line);
            currentTokenCount += lineTokens;
        }

        // Don't forget the last chunk
        if (currentChunk.Length > 0 && currentTokenCount >= options.MinChunkSize)
        {
            chunks.Add(new TextChunk
            {
                Content = currentChunk.ToString(),
                FilePath = filePath,
                StartLine = startLine,
                EndLine = lines.Length,
                Language = language,
                Type = ChunkType.Code,
                TokenCount = currentTokenCount
            });
        }

        return chunks;
    }

    private static int EstimateTokenCount(string text)
    {
        // Rough estimate: ~4 characters per token for code
        return text.Length / 4;
    }

    private static string GetLanguageFromExtension(string extension) => extension switch
    {
        ".cs" => "csharp",
        ".fs" => "fsharp",
        ".ts" or ".tsx" => "typescript",
        ".js" or ".jsx" => "javascript",
        ".py" => "python",
        ".java" => "java",
        ".go" => "go",
        ".rs" => "rust",
        ".cpp" or ".hpp" or ".c" or ".h" => "cpp",
        ".swift" => "swift",
        ".rb" => "ruby",
        ".kt" => "kotlin",
        _ => "unknown"
    };
}
```

### Embedding Service Implementation

```csharp
namespace SeniorIntern.Services;

/// <summary>
/// Embedding service using LLamaSharp for GGUF embedding models
/// </summary>
public sealed class LlamaEmbeddingService : IEmbeddingService, IAsyncDisposable
{
    private readonly ILogger<LlamaEmbeddingService> _logger;
    private readonly SemaphoreSlim _loadLock = new(1, 1);
    private readonly SemaphoreSlim _embedLock = new(1, 1);

    private LLamaWeights? _model;
    private LLamaEmbedder? _embedder;
    private bool _isDisposed;

    public bool IsModelLoaded => _embedder != null;
    public int EmbeddingDimension { get; private set; }
    public int MaxTokens { get; private set; } = 512;

    public event EventHandler<EmbeddingModelStateChangedEventArgs>? ModelStateChanged;

    public async Task LoadModelAsync(
        EmbeddingModelOptions options,
        IProgress<ModelLoadProgress>? progress = null,
        CancellationToken ct = default)
    {
        await _loadLock.WaitAsync(ct);
        try
        {
            if (_embedder != null)
                await UnloadModelAsync();

            progress?.Report(new ModelLoadProgress
            {
                Stage = "Loading embedding model",
                PercentComplete = 10
            });

            var modelParams = new ModelParams(options.ModelPath)
            {
                ContextSize = (uint)MaxTokens,
                GpuLayerCount = options.GpuLayers == -1
                    ? DetectOptimalGpuLayers()
                    : options.GpuLayers,
                EmbeddingMode = true
            };

            _model = await LLamaWeights.LoadFromFileAsync(modelParams, ct);

            progress?.Report(new ModelLoadProgress
            {
                Stage = "Initializing embedder",
                PercentComplete = 80
            });

            _embedder = new LLamaEmbedder(_model, modelParams);
            EmbeddingDimension = _model.EmbeddingSize;

            progress?.Report(new ModelLoadProgress
            {
                Stage = "Ready",
                PercentComplete = 100
            });

            ModelStateChanged?.Invoke(this, new EmbeddingModelStateChangedEventArgs
            {
                IsLoaded = true,
                ModelPath = options.ModelPath,
                EmbeddingDimension = EmbeddingDimension
            });

            _logger.LogInformation(
                "Loaded embedding model: {Path}, Dimension: {Dim}",
                options.ModelPath, EmbeddingDimension);
        }
        finally
        {
            _loadLock.Release();
        }
    }

    public async Task<float[]> EmbedAsync(string text, CancellationToken ct = default)
    {
        if (_embedder == null)
            throw new InvalidOperationException("No embedding model loaded");

        await _embedLock.WaitAsync(ct);
        try
        {
            var embedding = await Task.Run(() => _embedder.GetEmbeddings(text), ct);
            return embedding.First();
        }
        finally
        {
            _embedLock.Release();
        }
    }

    public async Task<IReadOnlyList<float[]>> EmbedBatchAsync(
        IEnumerable<string> texts,
        IProgress<EmbeddingProgress>? progress = null,
        CancellationToken ct = default)
    {
        if (_embedder == null)
            throw new InvalidOperationException("No embedding model loaded");

        var textList = texts.ToList();
        var results = new List<float[]>(textList.Count);

        await _embedLock.WaitAsync(ct);
        try
        {
            for (int i = 0; i < textList.Count; i++)
            {
                ct.ThrowIfCancellationRequested();

                var embedding = await Task.Run(
                    () => _embedder.GetEmbeddings(textList[i]), ct);
                results.Add(embedding.First());

                progress?.Report(new EmbeddingProgress
                {
                    ProcessedCount = i + 1,
                    TotalCount = textList.Count
                });
            }
        }
        finally
        {
            _embedLock.Release();
        }

        return results;
    }

    public async Task UnloadModelAsync()
    {
        await _loadLock.WaitAsync();
        try
        {
            _embedder?.Dispose();
            _embedder = null;
            _model?.Dispose();
            _model = null;

            // Force GC to release memory
            GC.Collect();
            GC.WaitForPendingFinalizers();

            ModelStateChanged?.Invoke(this, new EmbeddingModelStateChangedEventArgs
            {
                IsLoaded = false
            });
        }
        finally
        {
            _loadLock.Release();
        }
    }

    private static int DetectOptimalGpuLayers()
    {
        // For embedding models, typically load all layers to GPU if available
        if (OperatingSystem.IsMacOS())
            return 999; // Metal backend
        // TODO: CUDA detection for Windows/Linux
        return 0;
    }

    public async ValueTask DisposeAsync()
    {
        if (_isDisposed) return;
        _isDisposed = true;

        await UnloadModelAsync();
        _loadLock.Dispose();
        _embedLock.Dispose();
    }
}
```

### v0.7.1 Files to Create

| File | Purpose |
|------|---------|
| `Core/Interfaces/IEmbeddingService.cs` | Embedding service interface |
| `Core/Interfaces/IChunkingService.cs` | Chunking service interface |
| `Core/Models/TextChunk.cs` | Chunk data model |
| `Core/Models/EmbeddingModels.cs` | Embedding-related DTOs |
| `Services/LlamaEmbeddingService.cs` | LLamaSharp embedding implementation |
| `Services/ChunkingService.cs` | Chunking orchestrator |
| `Services/Chunking/CodeAwareChunkingStrategy.cs` | Code-aware chunking |
| `Services/Chunking/MarkdownChunkingStrategy.cs` | Markdown chunking |
| `Services/Chunking/PlainTextChunkingStrategy.cs` | Plain text chunking |

---

## v0.7.2: Vector Storage

### Objective
Integrate SQLite-vec for local vector storage, implement index management, and provide CRUD operations for embeddings.

### Why SQLite-vec?

- **Local-first**: No external server required
- **Single-file**: Fits with existing SQLite infrastructure
- **Cross-platform**: Works on Windows, macOS, Linux
- **Efficient**: Uses optimized vector similarity search (IVF, HNSW)
- **Integrated**: Can join vector searches with metadata queries

### Vector Database Schema

```sql
-- Vector index metadata
CREATE TABLE vector_indexes (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL,
    workspace_path TEXT NOT NULL,
    created_at TEXT NOT NULL,
    updated_at TEXT NOT NULL,
    embedding_model TEXT NOT NULL,
    embedding_dimension INTEGER NOT NULL,
    chunk_count INTEGER DEFAULT 0,
    file_count INTEGER DEFAULT 0,
    status TEXT DEFAULT 'active',
    settings_json TEXT
);

-- Indexed files tracking
CREATE TABLE indexed_files (
    id TEXT PRIMARY KEY,
    index_id TEXT NOT NULL REFERENCES vector_indexes(id) ON DELETE CASCADE,
    file_path TEXT NOT NULL,
    file_hash TEXT NOT NULL,
    file_size INTEGER NOT NULL,
    last_modified TEXT NOT NULL,
    indexed_at TEXT NOT NULL,
    chunk_count INTEGER DEFAULT 0,
    language TEXT,
    UNIQUE(index_id, file_path)
);

-- Vector chunks with embeddings (using sqlite-vec)
CREATE VIRTUAL TABLE vector_chunks USING vec0(
    id TEXT PRIMARY KEY,
    embedding FLOAT[{dimension}]  -- dimension set at creation time
);

-- Chunk metadata (separate table for efficient filtering)
CREATE TABLE chunk_metadata (
    id TEXT PRIMARY KEY,
    index_id TEXT NOT NULL REFERENCES vector_indexes(id) ON DELETE CASCADE,
    file_id TEXT NOT NULL REFERENCES indexed_files(id) ON DELETE CASCADE,
    content TEXT NOT NULL,
    start_line INTEGER NOT NULL,
    end_line INTEGER NOT NULL,
    start_offset INTEGER NOT NULL,
    end_offset INTEGER NOT NULL,
    chunk_type TEXT NOT NULL,
    language TEXT,
    symbol_name TEXT,
    symbol_type TEXT,
    parent_symbol TEXT,
    token_count INTEGER,
    metadata_json TEXT
);

-- Indexes for efficient queries
CREATE INDEX idx_chunk_metadata_index ON chunk_metadata(index_id);
CREATE INDEX idx_chunk_metadata_file ON chunk_metadata(file_id);
CREATE INDEX idx_chunk_metadata_symbol ON chunk_metadata(symbol_name);
CREATE INDEX idx_indexed_files_index ON indexed_files(index_id);
CREATE INDEX idx_indexed_files_hash ON indexed_files(file_hash);
```

### IVectorStore Interface

```csharp
namespace SeniorIntern.Core.Interfaces;

/// <summary>
/// Vector storage and retrieval service
/// </summary>
public interface IVectorStore : IAsyncDisposable
{
    /// <summary>
    /// Create a new vector index for a workspace
    /// </summary>
    Task<VectorIndex> CreateIndexAsync(
        string workspacePath,
        string name,
        int embeddingDimension,
        CancellationToken ct = default);

    /// <summary>
    /// Get an existing index by ID
    /// </summary>
    Task<VectorIndex?> GetIndexAsync(string indexId, CancellationToken ct = default);

    /// <summary>
    /// Get index for a workspace (if exists)
    /// </summary>
    Task<VectorIndex?> GetIndexForWorkspaceAsync(
        string workspacePath,
        CancellationToken ct = default);

    /// <summary>
    /// List all indexes
    /// </summary>
    Task<IReadOnlyList<VectorIndex>> ListIndexesAsync(CancellationToken ct = default);

    /// <summary>
    /// Delete an index and all its data
    /// </summary>
    Task DeleteIndexAsync(string indexId, CancellationToken ct = default);

    /// <summary>
    /// Add chunks with embeddings to an index
    /// </summary>
    Task AddChunksAsync(
        string indexId,
        IEnumerable<(TextChunk Chunk, float[] Embedding)> chunks,
        CancellationToken ct = default);

    /// <summary>
    /// Remove chunks for a file (for re-indexing)
    /// </summary>
    Task RemoveChunksForFileAsync(
        string indexId,
        string filePath,
        CancellationToken ct = default);

    /// <summary>
    /// Search for similar chunks
    /// </summary>
    Task<IReadOnlyList<ChunkSearchResult>> SearchAsync(
        string indexId,
        float[] queryEmbedding,
        VectorSearchOptions options,
        CancellationToken ct = default);

    /// <summary>
    /// Get file indexing status
    /// </summary>
    Task<IndexedFile?> GetIndexedFileAsync(
        string indexId,
        string filePath,
        CancellationToken ct = default);

    /// <summary>
    /// Update file indexing status
    /// </summary>
    Task UpsertIndexedFileAsync(
        string indexId,
        IndexedFile file,
        CancellationToken ct = default);

    /// <summary>
    /// Get index statistics
    /// </summary>
    Task<IndexStatistics> GetStatisticsAsync(
        string indexId,
        CancellationToken ct = default);
}
```

### Vector Search Models

```csharp
namespace SeniorIntern.Core.Models;

public sealed class VectorIndex
{
    public string Id { get; init; } = string.Empty;
    public string Name { get; init; } = string.Empty;
    public string WorkspacePath { get; init; } = string.Empty;
    public DateTime CreatedAt { get; init; }
    public DateTime UpdatedAt { get; set; }
    public string EmbeddingModel { get; init; } = string.Empty;
    public int EmbeddingDimension { get; init; }
    public int ChunkCount { get; set; }
    public int FileCount { get; set; }
    public IndexStatus Status { get; set; } = IndexStatus.Active;
    public VectorIndexSettings Settings { get; init; } = new();
}

public enum IndexStatus
{
    Active,
    Indexing,
    Paused,
    Error,
    Deleted
}

public sealed class VectorIndexSettings
{
    public ChunkingOptions ChunkingOptions { get; init; } = new();
    public IReadOnlyList<string> IncludePatterns { get; init; } = new[] { "**/*" };
    public IReadOnlyList<string> ExcludePatterns { get; init; } = new[]
    {
        "**/node_modules/**",
        "**/bin/**",
        "**/obj/**",
        "**/.git/**",
        "**/dist/**",
        "**/*.min.js",
        "**/*.min.css"
    };
    public int MaxFileSizeKb { get; init; } = 1024; // 1MB
    public bool IndexHiddenFiles { get; init; } = false;
}

public sealed class IndexedFile
{
    public string Id { get; init; } = Guid.NewGuid().ToString();
    public string IndexId { get; init; } = string.Empty;
    public string FilePath { get; init; } = string.Empty;
    public string FileHash { get; init; } = string.Empty;
    public long FileSize { get; init; }
    public DateTime LastModified { get; init; }
    public DateTime IndexedAt { get; init; }
    public int ChunkCount { get; set; }
    public string? Language { get; init; }
}

public sealed class VectorSearchOptions
{
    /// <summary>
    /// Maximum number of results
    /// </summary>
    public int TopK { get; init; } = 10;

    /// <summary>
    /// Minimum similarity score (0-1)
    /// </summary>
    public float MinScore { get; init; } = 0.5f;

    /// <summary>
    /// Filter by file paths (glob patterns)
    /// </summary>
    public IReadOnlyList<string>? FilePatterns { get; init; }

    /// <summary>
    /// Filter by languages
    /// </summary>
    public IReadOnlyList<string>? Languages { get; init; }

    /// <summary>
    /// Filter by chunk types
    /// </summary>
    public IReadOnlyList<ChunkType>? ChunkTypes { get; init; }

    /// <summary>
    /// Filter by symbol types
    /// </summary>
    public IReadOnlyList<SymbolType>? SymbolTypes { get; init; }

    /// <summary>
    /// Whether to include the full chunk content
    /// </summary>
    public bool IncludeContent { get; init; } = true;

    /// <summary>
    /// Whether to deduplicate overlapping chunks
    /// </summary>
    public bool Deduplicate { get; init; } = true;
}

public sealed class ChunkSearchResult
{
    public string ChunkId { get; init; } = string.Empty;
    public float Score { get; init; }
    public TextChunk Chunk { get; init; } = null!;
    public string? ExpandedContext { get; init; }
}

public sealed class IndexStatistics
{
    public string IndexId { get; init; } = string.Empty;
    public int TotalFiles { get; init; }
    public int TotalChunks { get; init; }
    public long TotalSizeBytes { get; init; }
    public Dictionary<string, int> FilesByLanguage { get; init; } = new();
    public Dictionary<ChunkType, int> ChunksByType { get; init; } = new();
    public DateTime? LastIndexedAt { get; init; }
    public TimeSpan? AverageIndexTime { get; init; }
}
```

### SQLite-vec Store Implementation

```csharp
namespace SeniorIntern.Services;

/// <summary>
/// Vector store implementation using SQLite-vec
/// </summary>
public sealed class SqliteVectorStore : IVectorStore, IAsyncDisposable
{
    private readonly string _dbPath;
    private readonly ILogger<SqliteVectorStore> _logger;
    private SqliteConnection? _connection;
    private bool _isDisposed;

    public SqliteVectorStore(
        IOptions<VectorStoreOptions> options,
        ILogger<SqliteVectorStore> logger)
    {
        _dbPath = options.Value.DatabasePath
            ?? Path.Combine(
                Environment.GetFolderPath(Environment.SpecialFolder.ApplicationData),
                "SeniorIntern", "vectors.db");
        _logger = logger;
    }

    public async Task InitializeAsync(CancellationToken ct = default)
    {
        Directory.CreateDirectory(Path.GetDirectoryName(_dbPath)!);

        _connection = new SqliteConnection($"Data Source={_dbPath}");
        await _connection.OpenAsync(ct);

        // Load sqlite-vec extension
        await _connection.ExecuteAsync("SELECT load_extension('vec0')");

        // Create schema
        await CreateSchemaAsync(ct);
    }

    public async Task<IReadOnlyList<ChunkSearchResult>> SearchAsync(
        string indexId,
        float[] queryEmbedding,
        VectorSearchOptions options,
        CancellationToken ct = default)
    {
        // Convert embedding to blob format for sqlite-vec
        var embeddingBlob = EmbeddingToBlob(queryEmbedding);

        // Build the query with filters
        var sql = new StringBuilder();
        sql.AppendLine(@"
            SELECT
                vc.id,
                vc.distance,
                cm.content,
                cm.file_id,
                cm.start_line,
                cm.end_line,
                cm.start_offset,
                cm.end_offset,
                cm.chunk_type,
                cm.language,
                cm.symbol_name,
                cm.symbol_type,
                cm.parent_symbol,
                cm.token_count,
                cm.metadata_json,
                if.file_path
            FROM vector_chunks vc
            JOIN chunk_metadata cm ON vc.id = cm.id
            JOIN indexed_files if ON cm.file_id = if.id
            WHERE cm.index_id = @indexId
              AND vc.embedding MATCH @embedding
              AND k = @topK");

        var parameters = new DynamicParameters();
        parameters.Add("indexId", indexId);
        parameters.Add("embedding", embeddingBlob);
        parameters.Add("topK", options.TopK * 2); // Over-fetch for filtering

        // Add filters
        if (options.Languages?.Count > 0)
        {
            sql.AppendLine("AND cm.language IN @languages");
            parameters.Add("languages", options.Languages);
        }

        if (options.ChunkTypes?.Count > 0)
        {
            sql.AppendLine("AND cm.chunk_type IN @chunkTypes");
            parameters.Add("chunkTypes", options.ChunkTypes.Select(t => t.ToString()));
        }

        sql.AppendLine("ORDER BY vc.distance ASC");
        sql.AppendLine($"LIMIT {options.TopK}");

        var rows = await _connection!.QueryAsync(sql.ToString(), parameters);

        var results = new List<ChunkSearchResult>();
        foreach (var row in rows)
        {
            // Convert distance to similarity score (1 - normalized_distance)
            var score = 1.0f - (float)row.distance;
            if (score < options.MinScore)
                continue;

            results.Add(new ChunkSearchResult
            {
                ChunkId = row.id,
                Score = score,
                Chunk = new TextChunk
                {
                    Id = Guid.Parse(row.id),
                    Content = options.IncludeContent ? row.content : string.Empty,
                    FilePath = row.file_path,
                    StartLine = row.start_line,
                    EndLine = row.end_line,
                    StartOffset = row.start_offset,
                    EndOffset = row.end_offset,
                    Type = Enum.Parse<ChunkType>(row.chunk_type),
                    Language = row.language,
                    SymbolName = row.symbol_name,
                    SymbolType = row.symbol_type != null
                        ? Enum.Parse<SymbolType>(row.symbol_type)
                        : null,
                    ParentSymbol = row.parent_symbol,
                    TokenCount = row.token_count
                }
            });
        }

        if (options.Deduplicate)
            results = DeduplicateResults(results);

        return results.Take(options.TopK).ToList();
    }

    public async Task AddChunksAsync(
        string indexId,
        IEnumerable<(TextChunk Chunk, float[] Embedding)> chunks,
        CancellationToken ct = default)
    {
        await using var transaction = await _connection!.BeginTransactionAsync(ct);
        try
        {
            foreach (var (chunk, embedding) in chunks)
            {
                var chunkId = chunk.Id.ToString();

                // Insert into vector table
                var embeddingBlob = EmbeddingToBlob(embedding);
                await _connection.ExecuteAsync(
                    "INSERT INTO vector_chunks (id, embedding) VALUES (@id, @embedding)",
                    new { id = chunkId, embedding = embeddingBlob });

                // Insert metadata
                await _connection.ExecuteAsync(@"
                    INSERT INTO chunk_metadata (
                        id, index_id, file_id, content, start_line, end_line,
                        start_offset, end_offset, chunk_type, language,
                        symbol_name, symbol_type, parent_symbol, token_count, metadata_json
                    ) VALUES (
                        @id, @indexId, @fileId, @content, @startLine, @endLine,
                        @startOffset, @endOffset, @chunkType, @language,
                        @symbolName, @symbolType, @parentSymbol, @tokenCount, @metadata
                    )",
                    new
                    {
                        id = chunkId,
                        indexId,
                        fileId = chunk.Metadata?["fileId"],
                        content = chunk.Content,
                        startLine = chunk.StartLine,
                        endLine = chunk.EndLine,
                        startOffset = chunk.StartOffset,
                        endOffset = chunk.EndOffset,
                        chunkType = chunk.Type.ToString(),
                        language = chunk.Language,
                        symbolName = chunk.SymbolName,
                        symbolType = chunk.SymbolType?.ToString(),
                        parentSymbol = chunk.ParentSymbol,
                        tokenCount = chunk.TokenCount,
                        metadata = chunk.Metadata != null
                            ? JsonSerializer.Serialize(chunk.Metadata)
                            : null
                    });
            }

            await transaction.CommitAsync(ct);
        }
        catch
        {
            await transaction.RollbackAsync(ct);
            throw;
        }
    }

    private static byte[] EmbeddingToBlob(float[] embedding)
    {
        var bytes = new byte[embedding.Length * sizeof(float)];
        Buffer.BlockCopy(embedding, 0, bytes, 0, bytes.Length);
        return bytes;
    }

    private static List<ChunkSearchResult> DeduplicateResults(List<ChunkSearchResult> results)
    {
        // Remove chunks that overlap significantly with higher-scored chunks
        var deduplicated = new List<ChunkSearchResult>();

        foreach (var result in results.OrderByDescending(r => r.Score))
        {
            var overlaps = deduplicated.Any(existing =>
                existing.Chunk.FilePath == result.Chunk.FilePath &&
                RangesOverlap(
                    existing.Chunk.StartLine, existing.Chunk.EndLine,
                    result.Chunk.StartLine, result.Chunk.EndLine,
                    overlapThreshold: 0.5f));

            if (!overlaps)
                deduplicated.Add(result);
        }

        return deduplicated;
    }

    private static bool RangesOverlap(
        int start1, int end1, int start2, int end2, float overlapThreshold)
    {
        var overlapStart = Math.Max(start1, start2);
        var overlapEnd = Math.Min(end1, end2);
        var overlap = Math.Max(0, overlapEnd - overlapStart);
        var smaller = Math.Min(end1 - start1, end2 - start2);
        return smaller > 0 && (float)overlap / smaller > overlapThreshold;
    }

    public async ValueTask DisposeAsync()
    {
        if (_isDisposed) return;
        _isDisposed = true;

        if (_connection != null)
        {
            await _connection.CloseAsync();
            await _connection.DisposeAsync();
        }
    }
}
```

### v0.7.2 Files to Create

| File | Purpose |
|------|---------|
| `Core/Interfaces/IVectorStore.cs` | Vector store interface |
| `Core/Models/VectorIndex.cs` | Index and search models |
| `Services/SqliteVectorStore.cs` | SQLite-vec implementation |
| `Core/Options/VectorStoreOptions.cs` | Configuration options |

### NuGet Packages for v0.7.2

| Package | Version | Purpose |
|---------|---------|---------|
| `sqlite-vec` | 0.1.x | Vector extension for SQLite |
| `Microsoft.Data.Sqlite` | 8.0.x | SQLite connectivity |
| `Dapper` | 2.1.x | Micro-ORM for complex queries |

---

## v0.7.3: Indexing Pipeline

### Objective
Implement the file scanning and indexing pipeline with incremental updates, background processing, and progress tracking.

### IIndexingService Interface

```csharp
namespace SeniorIntern.Core.Interfaces;

/// <summary>
/// Service for indexing workspace files
/// </summary>
public interface IIndexingService
{
    /// <summary>
    /// Current indexing status
    /// </summary>
    IndexingStatus Status { get; }

    /// <summary>
    /// Index an entire workspace
    /// </summary>
    Task<IndexingResult> IndexWorkspaceAsync(
        string workspacePath,
        IndexingOptions options,
        IProgress<IndexingProgress>? progress = null,
        CancellationToken ct = default);

    /// <summary>
    /// Index specific files
    /// </summary>
    Task<IndexingResult> IndexFilesAsync(
        string indexId,
        IEnumerable<string> filePaths,
        IProgress<IndexingProgress>? progress = null,
        CancellationToken ct = default);

    /// <summary>
    /// Update index incrementally (changed files only)
    /// </summary>
    Task<IndexingResult> UpdateIndexAsync(
        string indexId,
        IProgress<IndexingProgress>? progress = null,
        CancellationToken ct = default);

    /// <summary>
    /// Remove files from index
    /// </summary>
    Task RemoveFilesAsync(
        string indexId,
        IEnumerable<string> filePaths,
        CancellationToken ct = default);

    /// <summary>
    /// Cancel ongoing indexing
    /// </summary>
    void CancelIndexing();

    /// <summary>
    /// Event for indexing state changes
    /// </summary>
    event EventHandler<IndexingStateChangedEventArgs>? StateChanged;

    /// <summary>
    /// Event for individual file indexed
    /// </summary>
    event EventHandler<FileIndexedEventArgs>? FileIndexed;
}
```

### Indexing Models

```csharp
namespace SeniorIntern.Core.Models;

public sealed class IndexingOptions
{
    /// <summary>
    /// Name for the index
    /// </summary>
    public string IndexName { get; init; } = "Default";

    /// <summary>
    /// Embedding model to use
    /// </summary>
    public string EmbeddingModelPath { get; init; } = string.Empty;

    /// <summary>
    /// Chunking configuration
    /// </summary>
    public ChunkingOptions ChunkingOptions { get; init; } = new();

    /// <summary>
    /// File patterns to include
    /// </summary>
    public IReadOnlyList<string> IncludePatterns { get; init; } = new[]
    {
        "**/*.cs", "**/*.fs", "**/*.vb",
        "**/*.ts", "**/*.tsx", "**/*.js", "**/*.jsx",
        "**/*.py", "**/*.java", "**/*.kt",
        "**/*.go", "**/*.rs", "**/*.cpp", "**/*.c", "**/*.h",
        "**/*.swift", "**/*.rb", "**/*.php",
        "**/*.md", "**/*.txt", "**/*.json", "**/*.yaml", "**/*.yml",
        "**/*.xml", "**/*.html", "**/*.css", "**/*.scss"
    };

    /// <summary>
    /// File patterns to exclude
    /// </summary>
    public IReadOnlyList<string> ExcludePatterns { get; init; } = new[]
    {
        "**/node_modules/**", "**/bin/**", "**/obj/**",
        "**/.git/**", "**/dist/**", "**/build/**",
        "**/*.min.js", "**/*.min.css", "**/*.map",
        "**/packages/**", "**/vendor/**",
        "**/.vs/**", "**/.idea/**", "**/.vscode/**"
    };

    /// <summary>
    /// Maximum file size to index (KB)
    /// </summary>
    public int MaxFileSizeKb { get; init; } = 1024;

    /// <summary>
    /// Number of parallel embedding tasks
    /// </summary>
    public int ParallelEmbeddings { get; init; } = 4;

    /// <summary>
    /// Whether to force re-index all files
    /// </summary>
    public bool ForceReindex { get; init; } = false;

    /// <summary>
    /// Whether to respect .gitignore
    /// </summary>
    public bool RespectGitignore { get; init; } = true;
}

public sealed class IndexingProgress
{
    public IndexingPhase Phase { get; init; }
    public int TotalFiles { get; init; }
    public int ProcessedFiles { get; init; }
    public int TotalChunks { get; init; }
    public int ProcessedChunks { get; init; }
    public string? CurrentFile { get; init; }
    public string? Message { get; init; }
    public double PercentComplete { get; init; }
    public TimeSpan Elapsed { get; init; }
    public TimeSpan? EstimatedRemaining { get; init; }
}

public enum IndexingPhase
{
    Scanning,
    Analyzing,
    Chunking,
    Embedding,
    Storing,
    Finalizing,
    Complete,
    Cancelled,
    Error
}

public sealed class IndexingResult
{
    public bool Success { get; init; }
    public string IndexId { get; init; } = string.Empty;
    public int FilesIndexed { get; init; }
    public int FilesSkipped { get; init; }
    public int FilesErrored { get; init; }
    public int ChunksCreated { get; init; }
    public TimeSpan Duration { get; init; }
    public IReadOnlyList<IndexingError> Errors { get; init; } = Array.Empty<IndexingError>();
    public string? ErrorMessage { get; init; }
}

public sealed class IndexingError
{
    public string FilePath { get; init; } = string.Empty;
    public string Error { get; init; } = string.Empty;
    public IndexingErrorType Type { get; init; }
}

public enum IndexingErrorType
{
    FileRead,
    Parsing,
    Chunking,
    Embedding,
    Storage,
    Unknown
}

public enum IndexingStatus
{
    Idle,
    Scanning,
    Indexing,
    Updating,
    Paused,
    Cancelling
}
```

### Indexing Service Implementation

```csharp
namespace SeniorIntern.Services;

public sealed class IndexingService : IIndexingService
{
    private readonly IEmbeddingService _embeddingService;
    private readonly IChunkingService _chunkingService;
    private readonly IVectorStore _vectorStore;
    private readonly ILogger<IndexingService> _logger;

    private CancellationTokenSource? _indexingCts;
    private IndexingStatus _status = IndexingStatus.Idle;

    public IndexingStatus Status => _status;

    public event EventHandler<IndexingStateChangedEventArgs>? StateChanged;
    public event EventHandler<FileIndexedEventArgs>? FileIndexed;

    public async Task<IndexingResult> IndexWorkspaceAsync(
        string workspacePath,
        IndexingOptions options,
        IProgress<IndexingProgress>? progress = null,
        CancellationToken ct = default)
    {
        if (_status != IndexingStatus.Idle)
            throw new InvalidOperationException($"Indexing already in progress: {_status}");

        _indexingCts = CancellationTokenSource.CreateLinkedTokenSource(ct);
        var stopwatch = Stopwatch.StartNew();
        var errors = new List<IndexingError>();

        try
        {
            SetStatus(IndexingStatus.Scanning);

            // Phase 1: Scan files
            progress?.Report(new IndexingProgress
            {
                Phase = IndexingPhase.Scanning,
                Message = "Scanning workspace for files..."
            });

            var files = await ScanFilesAsync(workspacePath, options, _indexingCts.Token);
            _logger.LogInformation("Found {Count} files to index", files.Count);

            // Phase 2: Create or get index
            var index = await _vectorStore.GetIndexForWorkspaceAsync(workspacePath, _indexingCts.Token)
                ?? await _vectorStore.CreateIndexAsync(
                    workspacePath,
                    options.IndexName,
                    _embeddingService.EmbeddingDimension,
                    _indexingCts.Token);

            // Phase 3: Filter unchanged files (unless force reindex)
            var filesToIndex = files;
            if (!options.ForceReindex)
            {
                filesToIndex = await FilterChangedFilesAsync(
                    index.Id, files, _indexingCts.Token);
                _logger.LogInformation(
                    "After filtering unchanged: {Count} files to index",
                    filesToIndex.Count);
            }

            SetStatus(IndexingStatus.Indexing);

            // Phase 4: Process files
            var processedFiles = 0;
            var totalChunks = 0;

            var semaphore = new SemaphoreSlim(options.ParallelEmbeddings);
            var tasks = new List<Task>();

            foreach (var file in filesToIndex)
            {
                await semaphore.WaitAsync(_indexingCts.Token);

                tasks.Add(Task.Run(async () =>
                {
                    try
                    {
                        var chunks = await ProcessFileAsync(
                            index.Id, file, options, _indexingCts.Token);

                        Interlocked.Add(ref totalChunks, chunks);
                        var processed = Interlocked.Increment(ref processedFiles);

                        progress?.Report(new IndexingProgress
                        {
                            Phase = IndexingPhase.Embedding,
                            TotalFiles = filesToIndex.Count,
                            ProcessedFiles = processed,
                            CurrentFile = Path.GetRelativePath(workspacePath, file),
                            TotalChunks = totalChunks,
                            PercentComplete = (double)processed / filesToIndex.Count * 100,
                            Elapsed = stopwatch.Elapsed
                        });

                        FileIndexed?.Invoke(this, new FileIndexedEventArgs
                        {
                            FilePath = file,
                            ChunkCount = chunks,
                            Success = true
                        });
                    }
                    catch (Exception ex)
                    {
                        _logger.LogWarning(ex, "Error indexing file: {File}", file);
                        lock (errors)
                        {
                            errors.Add(new IndexingError
                            {
                                FilePath = file,
                                Error = ex.Message,
                                Type = ClassifyError(ex)
                            });
                        }

                        FileIndexed?.Invoke(this, new FileIndexedEventArgs
                        {
                            FilePath = file,
                            Success = false,
                            Error = ex.Message
                        });
                    }
                    finally
                    {
                        semaphore.Release();
                    }
                }, _indexingCts.Token));
            }

            await Task.WhenAll(tasks);

            // Phase 5: Finalize
            progress?.Report(new IndexingProgress
            {
                Phase = IndexingPhase.Finalizing,
                Message = "Finalizing index...",
                PercentComplete = 99
            });

            // Update index statistics
            index.ChunkCount = totalChunks;
            index.FileCount = processedFiles;
            index.UpdatedAt = DateTime.UtcNow;

            stopwatch.Stop();

            progress?.Report(new IndexingProgress
            {
                Phase = IndexingPhase.Complete,
                TotalFiles = filesToIndex.Count,
                ProcessedFiles = processedFiles,
                TotalChunks = totalChunks,
                ProcessedChunks = totalChunks,
                PercentComplete = 100,
                Elapsed = stopwatch.Elapsed
            });

            return new IndexingResult
            {
                Success = true,
                IndexId = index.Id,
                FilesIndexed = processedFiles,
                FilesSkipped = files.Count - filesToIndex.Count,
                FilesErrored = errors.Count,
                ChunksCreated = totalChunks,
                Duration = stopwatch.Elapsed,
                Errors = errors
            };
        }
        catch (OperationCanceledException)
        {
            return new IndexingResult
            {
                Success = false,
                ErrorMessage = "Indexing was cancelled",
                Duration = stopwatch.Elapsed,
                Errors = errors
            };
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Indexing failed");
            return new IndexingResult
            {
                Success = false,
                ErrorMessage = ex.Message,
                Duration = stopwatch.Elapsed,
                Errors = errors
            };
        }
        finally
        {
            SetStatus(IndexingStatus.Idle);
            _indexingCts?.Dispose();
            _indexingCts = null;
        }
    }

    private async Task<int> ProcessFileAsync(
        string indexId,
        string filePath,
        IndexingOptions options,
        CancellationToken ct)
    {
        // Read file
        var content = await File.ReadAllTextAsync(filePath, ct);
        var fileInfo = new FileInfo(filePath);
        var fileHash = ComputeHash(content);

        // Chunk the file
        var chunks = _chunkingService.ChunkDocument(
            content, filePath, options.ChunkingOptions);

        if (chunks.Count == 0)
            return 0;

        // Generate embeddings
        var embeddings = await _embeddingService.EmbedBatchAsync(
            chunks.Select(c => c.Content), null, ct);

        // Store in vector database
        var chunksWithEmbeddings = chunks.Zip(embeddings, (c, e) =>
        {
            // Add file reference to chunk metadata
            var metadata = new Dictionary<string, string>(c.Metadata ?? new Dictionary<string, string>())
            {
                ["fileId"] = Guid.NewGuid().ToString()
            };
            return (c with { Metadata = metadata }, e);
        });

        await _vectorStore.AddChunksAsync(indexId, chunksWithEmbeddings, ct);

        // Update file tracking
        await _vectorStore.UpsertIndexedFileAsync(indexId, new IndexedFile
        {
            IndexId = indexId,
            FilePath = filePath,
            FileHash = fileHash,
            FileSize = fileInfo.Length,
            LastModified = fileInfo.LastWriteTimeUtc,
            IndexedAt = DateTime.UtcNow,
            ChunkCount = chunks.Count,
            Language = chunks.FirstOrDefault()?.Language
        }, ct);

        return chunks.Count;
    }

    private async Task<List<string>> ScanFilesAsync(
        string workspacePath,
        IndexingOptions options,
        CancellationToken ct)
    {
        var files = new List<string>();
        var gitignoreMatcher = options.RespectGitignore
            ? await LoadGitignoreAsync(workspacePath, ct)
            : null;

        var matcher = new Matcher();
        foreach (var pattern in options.IncludePatterns)
            matcher.AddInclude(pattern);
        foreach (var pattern in options.ExcludePatterns)
            matcher.AddExclude(pattern);

        var result = matcher.Execute(
            new DirectoryInfoWrapper(new DirectoryInfo(workspacePath)));

        foreach (var file in result.Files)
        {
            var fullPath = Path.Combine(workspacePath, file.Path);
            var fileInfo = new FileInfo(fullPath);

            // Skip large files
            if (fileInfo.Length > options.MaxFileSizeKb * 1024)
                continue;

            // Skip gitignored files
            if (gitignoreMatcher?.IsIgnored(file.Path) == true)
                continue;

            files.Add(fullPath);
        }

        return files;
    }

    private async Task<List<string>> FilterChangedFilesAsync(
        string indexId,
        List<string> files,
        CancellationToken ct)
    {
        var changedFiles = new List<string>();

        foreach (var file in files)
        {
            var indexed = await _vectorStore.GetIndexedFileAsync(indexId, file, ct);
            if (indexed == null)
            {
                changedFiles.Add(file);
                continue;
            }

            var fileInfo = new FileInfo(file);
            if (fileInfo.LastWriteTimeUtc > indexed.IndexedAt)
            {
                // File modified, remove old chunks and re-index
                await _vectorStore.RemoveChunksForFileAsync(indexId, file, ct);
                changedFiles.Add(file);
            }
        }

        return changedFiles;
    }

    private void SetStatus(IndexingStatus status)
    {
        _status = status;
        StateChanged?.Invoke(this, new IndexingStateChangedEventArgs { Status = status });
    }

    public void CancelIndexing()
    {
        if (_status != IndexingStatus.Idle)
        {
            SetStatus(IndexingStatus.Cancelling);
            _indexingCts?.Cancel();
        }
    }

    private static string ComputeHash(string content)
    {
        var bytes = Encoding.UTF8.GetBytes(content);
        var hash = SHA256.HashData(bytes);
        return Convert.ToHexString(hash);
    }

    private static IndexingErrorType ClassifyError(Exception ex) => ex switch
    {
        IOException => IndexingErrorType.FileRead,
        JsonException => IndexingErrorType.Parsing,
        _ => IndexingErrorType.Unknown
    };
}
```

### v0.7.3 Files to Create

| File | Purpose |
|------|---------|
| `Core/Interfaces/IIndexingService.cs` | Indexing service interface |
| `Core/Models/IndexingModels.cs` | Indexing progress, result, options |
| `Services/IndexingService.cs` | Indexing pipeline implementation |
| `Services/GitignoreMatcher.cs` | .gitignore pattern matching |

---

## v0.7.4: Retrieval & Context

### Objective
Implement semantic search, context assembly for LLM prompts, relevance scoring, and integration with the chat system.

### IKnowledgeService Interface

```csharp
namespace SeniorIntern.Core.Interfaces;

/// <summary>
/// High-level knowledge retrieval service for RAG
/// </summary>
public interface IKnowledgeService
{
    /// <summary>
    /// Query the knowledge base with a natural language question
    /// </summary>
    Task<KnowledgeQueryResult> QueryAsync(
        string question,
        KnowledgeQueryOptions options,
        CancellationToken ct = default);

    /// <summary>
    /// Build context for an LLM prompt from retrieved knowledge
    /// </summary>
    Task<string> BuildContextAsync(
        string question,
        ContextBuildOptions options,
        CancellationToken ct = default);

    /// <summary>
    /// Get relevant files for a query (without full content)
    /// </summary>
    Task<IReadOnlyList<RelevantFile>> FindRelevantFilesAsync(
        string query,
        int maxFiles = 10,
        CancellationToken ct = default);

    /// <summary>
    /// Check if knowledge base is available for workspace
    /// </summary>
    Task<bool> IsIndexedAsync(string workspacePath, CancellationToken ct = default);

    /// <summary>
    /// Get index health/statistics
    /// </summary>
    Task<KnowledgeIndexHealth> GetIndexHealthAsync(
        string workspacePath,
        CancellationToken ct = default);
}
```

### Knowledge Query Models

```csharp
namespace SeniorIntern.Core.Models;

public sealed class KnowledgeQueryOptions
{
    /// <summary>
    /// Workspace to search in
    /// </summary>
    public string WorkspacePath { get; init; } = string.Empty;

    /// <summary>
    /// Maximum results to return
    /// </summary>
    public int MaxResults { get; init; } = 10;

    /// <summary>
    /// Minimum relevance score (0-1)
    /// </summary>
    public float MinRelevance { get; init; } = 0.5f;

    /// <summary>
    /// Whether to expand context around matches
    /// </summary>
    public bool ExpandContext { get; init; } = true;

    /// <summary>
    /// Lines of context to include around matches
    /// </summary>
    public int ContextLines { get; init; } = 10;

    /// <summary>
    /// File filters
    /// </summary>
    public IReadOnlyList<string>? FilePatterns { get; init; }

    /// <summary>
    /// Language filters
    /// </summary>
    public IReadOnlyList<string>? Languages { get; init; }

    /// <summary>
    /// Whether to include code snippets
    /// </summary>
    public bool IncludeSnippets { get; init; } = true;

    /// <summary>
    /// Reranking strategy
    /// </summary>
    public RerankingStrategy Reranking { get; init; } = RerankingStrategy.None;
}

public enum RerankingStrategy
{
    /// <summary>
    /// Use embedding similarity only
    /// </summary>
    None,

    /// <summary>
    /// Apply keyword matching bonus
    /// </summary>
    KeywordBoost,

    /// <summary>
    /// Use reciprocal rank fusion
    /// </summary>
    RRF,

    /// <summary>
    /// Use a cross-encoder reranker (future)
    /// </summary>
    CrossEncoder
}

public sealed class KnowledgeQueryResult
{
    /// <summary>
    /// Retrieved code/text chunks
    /// </summary>
    public IReadOnlyList<KnowledgeChunk> Chunks { get; init; } = Array.Empty<KnowledgeChunk>();

    /// <summary>
    /// Files containing relevant information
    /// </summary>
    public IReadOnlyList<string> RelevantFiles { get; init; } = Array.Empty<string>();

    /// <summary>
    /// Query processing time
    /// </summary>
    public TimeSpan QueryTime { get; init; }

    /// <summary>
    /// Total chunks searched
    /// </summary>
    public int TotalChunksSearched { get; init; }
}

public sealed class KnowledgeChunk
{
    /// <summary>
    /// The code/text content
    /// </summary>
    public string Content { get; init; } = string.Empty;

    /// <summary>
    /// File path (relative to workspace)
    /// </summary>
    public string FilePath { get; init; } = string.Empty;

    /// <summary>
    /// Starting line number
    /// </summary>
    public int StartLine { get; init; }

    /// <summary>
    /// Ending line number
    /// </summary>
    public int EndLine { get; init; }

    /// <summary>
    /// Relevance score (0-1)
    /// </summary>
    public float Relevance { get; init; }

    /// <summary>
    /// Programming language
    /// </summary>
    public string? Language { get; init; }

    /// <summary>
    /// Symbol name if applicable
    /// </summary>
    public string? SymbolName { get; init; }

    /// <summary>
    /// Symbol type if applicable
    /// </summary>
    public SymbolType? SymbolType { get; init; }

    /// <summary>
    /// Expanded context around the match
    /// </summary>
    public string? ExpandedContext { get; init; }

    /// <summary>
    /// Highlighted keywords in content
    /// </summary>
    public IReadOnlyList<(int Start, int Length)>? Highlights { get; init; }
}

public sealed class ContextBuildOptions
{
    public string WorkspacePath { get; init; } = string.Empty;
    public int MaxTokens { get; init; } = 4000;
    public int MaxChunks { get; init; } = 10;
    public float MinRelevance { get; init; } = 0.5f;
    public bool IncludeFileHeaders { get; init; } = true;
    public bool IncludeLineNumbers { get; init; } = true;
    public ContextFormat Format { get; init; } = ContextFormat.Markdown;
}

public enum ContextFormat
{
    Markdown,
    Plain,
    Xml
}

public sealed class RelevantFile
{
    public string FilePath { get; init; } = string.Empty;
    public float Relevance { get; init; }
    public int MatchCount { get; init; }
    public string? Summary { get; init; }
}

public sealed class KnowledgeIndexHealth
{
    public bool IsIndexed { get; init; }
    public string? IndexId { get; init; }
    public int TotalFiles { get; init; }
    public int TotalChunks { get; init; }
    public DateTime? LastUpdated { get; init; }
    public int StaleFiles { get; init; }
    public bool NeedsReindex { get; init; }
    public string? EmbeddingModel { get; init; }
}
```

### Knowledge Service Implementation

```csharp
namespace SeniorIntern.Services;

public sealed class KnowledgeService : IKnowledgeService
{
    private readonly IVectorStore _vectorStore;
    private readonly IEmbeddingService _embeddingService;
    private readonly ILogger<KnowledgeService> _logger;

    public async Task<KnowledgeQueryResult> QueryAsync(
        string question,
        KnowledgeQueryOptions options,
        CancellationToken ct = default)
    {
        var stopwatch = Stopwatch.StartNew();

        // Get index for workspace
        var index = await _vectorStore.GetIndexForWorkspaceAsync(
            options.WorkspacePath, ct);

        if (index == null)
        {
            return new KnowledgeQueryResult
            {
                Chunks = Array.Empty<KnowledgeChunk>(),
                RelevantFiles = Array.Empty<string>(),
                QueryTime = stopwatch.Elapsed
            };
        }

        // Generate query embedding
        var queryEmbedding = await _embeddingService.EmbedAsync(question, ct);

        // Search vector store
        var searchResults = await _vectorStore.SearchAsync(
            index.Id,
            queryEmbedding,
            new VectorSearchOptions
            {
                TopK = options.MaxResults * 2, // Over-fetch for reranking
                MinScore = options.MinRelevance,
                FilePatterns = options.FilePatterns,
                Languages = options.Languages,
                IncludeContent = true,
                Deduplicate = true
            },
            ct);

        // Apply reranking if requested
        var rankedResults = options.Reranking switch
        {
            RerankingStrategy.KeywordBoost => ApplyKeywordBoost(searchResults, question),
            RerankingStrategy.RRF => ApplyRRF(searchResults, question),
            _ => searchResults
        };

        // Take top results
        var topResults = rankedResults
            .OrderByDescending(r => r.Score)
            .Take(options.MaxResults)
            .ToList();

        // Expand context if requested
        var chunks = new List<KnowledgeChunk>();
        foreach (var result in topResults)
        {
            var chunk = new KnowledgeChunk
            {
                Content = result.Chunk.Content,
                FilePath = result.Chunk.FilePath,
                StartLine = result.Chunk.StartLine,
                EndLine = result.Chunk.EndLine,
                Relevance = result.Score,
                Language = result.Chunk.Language,
                SymbolName = result.Chunk.SymbolName,
                SymbolType = result.Chunk.SymbolType
            };

            if (options.ExpandContext)
            {
                chunk = chunk with
                {
                    ExpandedContext = await ExpandContextAsync(
                        options.WorkspacePath,
                        result.Chunk,
                        options.ContextLines,
                        ct)
                };
            }

            chunks.Add(chunk);
        }

        // Get unique files
        var relevantFiles = chunks
            .Select(c => c.FilePath)
            .Distinct()
            .ToList();

        stopwatch.Stop();

        return new KnowledgeQueryResult
        {
            Chunks = chunks,
            RelevantFiles = relevantFiles,
            QueryTime = stopwatch.Elapsed,
            TotalChunksSearched = index.ChunkCount
        };
    }

    public async Task<string> BuildContextAsync(
        string question,
        ContextBuildOptions options,
        CancellationToken ct = default)
    {
        var result = await QueryAsync(question, new KnowledgeQueryOptions
        {
            WorkspacePath = options.WorkspacePath,
            MaxResults = options.MaxChunks,
            MinRelevance = options.MinRelevance,
            ExpandContext = true
        }, ct);

        if (result.Chunks.Count == 0)
            return string.Empty;

        var sb = new StringBuilder();
        var currentTokens = 0;
        var tokenEstimator = 4; // ~4 chars per token

        foreach (var chunk in result.Chunks)
        {
            var chunkText = FormatChunk(chunk, options);
            var estimatedTokens = chunkText.Length / tokenEstimator;

            if (currentTokens + estimatedTokens > options.MaxTokens)
                break;

            sb.AppendLine(chunkText);
            sb.AppendLine();
            currentTokens += estimatedTokens;
        }

        return sb.ToString().TrimEnd();
    }

    private string FormatChunk(KnowledgeChunk chunk, ContextBuildOptions options)
    {
        return options.Format switch
        {
            ContextFormat.Markdown => FormatMarkdown(chunk, options),
            ContextFormat.Xml => FormatXml(chunk, options),
            _ => FormatPlain(chunk, options)
        };
    }

    private string FormatMarkdown(KnowledgeChunk chunk, ContextBuildOptions options)
    {
        var sb = new StringBuilder();

        if (options.IncludeFileHeaders)
        {
            sb.AppendLine($"### {chunk.FilePath}");
            if (options.IncludeLineNumbers)
                sb.AppendLine($"Lines {chunk.StartLine}-{chunk.EndLine}");
            sb.AppendLine();
        }

        var lang = chunk.Language ?? "text";
        sb.AppendLine($"```{lang}");
        sb.AppendLine(chunk.ExpandedContext ?? chunk.Content);
        sb.AppendLine("```");

        return sb.ToString();
    }

    private string FormatXml(KnowledgeChunk chunk, ContextBuildOptions options)
    {
        return $@"<code-context file=""{chunk.FilePath}"" lines=""{chunk.StartLine}-{chunk.EndLine}"" language=""{chunk.Language}"">
{chunk.ExpandedContext ?? chunk.Content}
</code-context>";
    }

    private string FormatPlain(KnowledgeChunk chunk, ContextBuildOptions options)
    {
        var sb = new StringBuilder();
        sb.AppendLine($"File: {chunk.FilePath} (lines {chunk.StartLine}-{chunk.EndLine})");
        sb.AppendLine(chunk.ExpandedContext ?? chunk.Content);
        return sb.ToString();
    }

    private async Task<string> ExpandContextAsync(
        string workspacePath,
        TextChunk chunk,
        int contextLines,
        CancellationToken ct)
    {
        try
        {
            var fullPath = Path.Combine(workspacePath, chunk.FilePath);
            if (!File.Exists(fullPath))
                return chunk.Content;

            var lines = await File.ReadAllLinesAsync(fullPath, ct);

            var startLine = Math.Max(0, chunk.StartLine - contextLines - 1);
            var endLine = Math.Min(lines.Length, chunk.EndLine + contextLines);

            return string.Join('\n', lines.Skip(startLine).Take(endLine - startLine));
        }
        catch
        {
            return chunk.Content;
        }
    }

    private IReadOnlyList<ChunkSearchResult> ApplyKeywordBoost(
        IReadOnlyList<ChunkSearchResult> results,
        string query)
    {
        var keywords = ExtractKeywords(query);

        return results
            .Select(r =>
            {
                var keywordScore = keywords.Count(k =>
                    r.Chunk.Content.Contains(k, StringComparison.OrdinalIgnoreCase));
                var boost = keywordScore * 0.05f; // 5% boost per keyword match
                return r with { Score = Math.Min(1.0f, r.Score + boost) };
            })
            .OrderByDescending(r => r.Score)
            .ToList();
    }

    private IReadOnlyList<ChunkSearchResult> ApplyRRF(
        IReadOnlyList<ChunkSearchResult> results,
        string query)
    {
        const float k = 60f; // RRF constant

        // Semantic ranking
        var semanticRanks = results
            .Select((r, i) => (Result: r, Rank: i + 1))
            .ToDictionary(x => x.Result.ChunkId, x => x.Rank);

        // Keyword ranking
        var keywords = ExtractKeywords(query);
        var keywordScores = results
            .Select(r => (
                Result: r,
                Score: keywords.Sum(k =>
                    r.Chunk.Content.Split(k, StringComparison.OrdinalIgnoreCase).Length - 1)
            ))
            .OrderByDescending(x => x.Score)
            .Select((x, i) => (x.Result, Rank: i + 1))
            .ToDictionary(x => x.Result.ChunkId, x => x.Rank);

        // Combine with RRF
        return results
            .Select(r =>
            {
                var semanticRank = semanticRanks[r.ChunkId];
                var keywordRank = keywordScores.GetValueOrDefault(r.ChunkId, results.Count);
                var rrfScore = (1f / (k + semanticRank)) + (1f / (k + keywordRank));
                return r with { Score = rrfScore };
            })
            .OrderByDescending(r => r.Score)
            .ToList();
    }

    private static List<string> ExtractKeywords(string query)
    {
        // Simple keyword extraction - split on spaces, filter short/common words
        var stopWords = new HashSet<string>(StringComparer.OrdinalIgnoreCase)
        {
            "a", "an", "the", "is", "are", "was", "were", "be", "been",
            "being", "have", "has", "had", "do", "does", "did", "will",
            "would", "could", "should", "may", "might", "must", "shall",
            "can", "need", "dare", "ought", "used", "to", "of", "in",
            "for", "on", "with", "at", "by", "from", "as", "into",
            "through", "during", "before", "after", "above", "below",
            "between", "under", "again", "further", "then", "once",
            "where", "how", "what", "which", "who", "whom", "this",
            "that", "these", "those", "am", "or", "and", "but", "if"
        };

        return query
            .Split(' ', StringSplitOptions.RemoveEmptyEntries)
            .Where(w => w.Length > 2 && !stopWords.Contains(w))
            .Select(w => w.Trim('?', '.', ',', '!', '"', '\''))
            .Where(w => !string.IsNullOrEmpty(w))
            .ToList();
    }
}
```

### Chat Integration

```csharp
namespace SeniorIntern.Services;

// Extension to existing ChatService/LlmService for RAG integration
public sealed class RagAwareChatService
{
    private readonly ILlmService _llmService;
    private readonly IKnowledgeService _knowledgeService;
    private readonly IConversationService _conversationService;

    /// <summary>
    /// Generate a response with RAG context
    /// </summary>
    public async IAsyncEnumerable<string> GenerateWithRagAsync(
        Guid conversationId,
        string userMessage,
        RagChatOptions options,
        [EnumeratorCancellation] CancellationToken ct = default)
    {
        var conversation = await _conversationService.GetConversationAsync(conversationId, ct);
        if (conversation == null)
            throw new InvalidOperationException("Conversation not found");

        // Check if RAG should be used
        string? ragContext = null;
        if (options.EnableRag && !string.IsNullOrEmpty(options.WorkspacePath))
        {
            var isIndexed = await _knowledgeService.IsIndexedAsync(options.WorkspacePath, ct);
            if (isIndexed)
            {
                ragContext = await _knowledgeService.BuildContextAsync(
                    userMessage,
                    new ContextBuildOptions
                    {
                        WorkspacePath = options.WorkspacePath,
                        MaxTokens = options.MaxRagTokens,
                        MaxChunks = options.MaxRagChunks,
                        MinRelevance = options.MinRelevance,
                        Format = ContextFormat.Markdown
                    },
                    ct);
            }
        }

        // Build messages with RAG context
        var messages = BuildMessagesWithRag(
            conversation.Messages,
            userMessage,
            ragContext,
            options);

        // Generate response
        await foreach (var token in _llmService.GenerateStreamingAsync(
            messages, options.InferenceOptions, ct))
        {
            yield return token;
        }
    }

    private IEnumerable<ChatMessage> BuildMessagesWithRag(
        IEnumerable<ChatMessage> history,
        string userMessage,
        string? ragContext,
        RagChatOptions options)
    {
        // System message with RAG instructions
        var systemPrompt = options.SystemPrompt ?? DefaultSystemPrompt;

        if (!string.IsNullOrEmpty(ragContext))
        {
            systemPrompt += "\n\n" + RagInstructions + "\n\n" +
                "## Relevant Code Context\n\n" + ragContext;
        }

        yield return new ChatMessage
        {
            Role = MessageRole.System,
            Content = systemPrompt
        };

        // History
        foreach (var msg in history.TakeLast(options.MaxHistoryMessages))
        {
            yield return msg;
        }

        // Current user message
        yield return new ChatMessage
        {
            Role = MessageRole.User,
            Content = userMessage
        };
    }

    private const string RagInstructions = @"
You have been provided with relevant code context from the user's codebase.
Use this context to provide accurate, specific answers about their code.
When referencing code, mention the file path and line numbers.
If the context doesn't contain relevant information, say so and provide general guidance.";

    private const string DefaultSystemPrompt = @"
You are The Senior Intern, a helpful coding assistant with knowledge of the user's codebase.
Provide clear, concise answers with code examples when appropriate.";
}

public sealed class RagChatOptions
{
    public bool EnableRag { get; init; } = true;
    public string? WorkspacePath { get; init; }
    public int MaxRagTokens { get; init; } = 4000;
    public int MaxRagChunks { get; init; } = 10;
    public float MinRelevance { get; init; } = 0.5f;
    public int MaxHistoryMessages { get; init; } = 20;
    public string? SystemPrompt { get; init; }
    public InferenceOptions InferenceOptions { get; init; } = new();
}
```

### v0.7.4 Files to Create

| File | Purpose |
|------|---------|
| `Core/Interfaces/IKnowledgeService.cs` | Knowledge service interface |
| `Core/Models/KnowledgeModels.cs` | Query, result, context models |
| `Services/KnowledgeService.cs` | Knowledge retrieval implementation |
| `Services/RagAwareChatService.cs` | Chat integration with RAG |

---

## v0.7.5: UI & Integration

### Objective
Create the index management UI, implement auto-indexing, add settings, and polish the user experience.

### Index Manager Dialog

```
┌──────────────────────────────────────────────────────────────────┐
│  Knowledge Index Manager                                    [X]  │
├──────────────────────────────────────────────────────────────────┤
│                                                                   │
│  Workspace: ~/Projects/MyApp                                      │
│  ┌─────────────────────────────────────────────────────────────┐ │
│  │ Index Status: ● Indexed                                      │ │
│  │ Last Updated: 2 hours ago                                    │ │
│  │ Files: 1,234  |  Chunks: 15,678  |  Size: 45 MB             │ │
│  └─────────────────────────────────────────────────────────────┘ │
│                                                                   │
│  ┌─ Embedding Model ─────────────────────────────────────────┐  │
│  │ [nomic-embed-text-v1.5.Q8_0.gguf           ] [Browse...]  │  │
│  │ Dimension: 768  |  Max Tokens: 512                         │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                   │
│  ┌─ Index Options ───────────────────────────────────────────┐  │
│  │ ☑ Auto-index on file changes                              │  │
│  │ ☑ Respect .gitignore                                      │  │
│  │ ☐ Index hidden files                                      │  │
│  │                                                            │  │
│  │ Chunk Size:    [512 ▼] tokens                             │  │
│  │ Chunk Overlap: [64  ▼] tokens                             │  │
│  │ Max File Size: [1024▼] KB                                 │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                   │
│  ┌─ File Filters ────────────────────────────────────────────┐  │
│  │ Include: *.cs, *.ts, *.js, *.py, *.md, *.json             │  │
│  │ Exclude: node_modules/**, bin/**, obj/**                  │  │
│  │ [Edit Patterns...]                                         │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                   │
│  ┌─ Actions ─────────────────────────────────────────────────┐  │
│  │  [Reindex All]  [Update Changed]  [Clear Index]           │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                   │
│  ─────────────────────────────────────────────────────────────   │
│  [Close]                                              [Save]     │
└──────────────────────────────────────────────────────────────────┘
```

### Indexing Progress Dialog

```
┌──────────────────────────────────────────────────────────────────┐
│  Indexing Project                                           [X]  │
├──────────────────────────────────────────────────────────────────┤
│                                                                   │
│  Phase: Embedding chunks...                                       │
│                                                                   │
│  ████████████████████░░░░░░░░░░░░░  65%                          │
│                                                                   │
│  Files:  823 / 1,267                                             │
│  Chunks: 10,456 / ~16,000                                        │
│                                                                   │
│  Current: src/Services/UserService.cs                            │
│                                                                   │
│  Elapsed: 2m 34s  |  Remaining: ~1m 20s                          │
│                                                                   │
│  ┌─ Log ─────────────────────────────────────────────────────┐  │
│  │ ✓ Scanned 1,267 files                                     │  │
│  │ ✓ Filtered to 1,234 indexable files                       │  │
│  │ ⚡ Processing chunks...                                    │  │
│  │   └ src/Services/AuthService.cs (45 chunks)               │  │
│  │   └ src/Services/UserService.cs (processing...)           │  │
│  └───────────────────────────────────────────────────────────┘  │
│                                                                   │
│  [Run in Background]                               [Cancel]      │
└──────────────────────────────────────────────────────────────────┘
```

### IndexManagerViewModel

```csharp
namespace SeniorIntern.Desktop.ViewModels;

public partial class IndexManagerViewModel : ViewModelBase
{
    private readonly IKnowledgeService _knowledgeService;
    private readonly IIndexingService _indexingService;
    private readonly IEmbeddingService _embeddingService;
    private readonly IVectorStore _vectorStore;
    private readonly ISettingsService _settingsService;

    [ObservableProperty]
    private string _workspacePath = string.Empty;

    [ObservableProperty]
    private bool _isIndexed;

    [ObservableProperty]
    private DateTime? _lastUpdated;

    [ObservableProperty]
    private int _fileCount;

    [ObservableProperty]
    private int _chunkCount;

    [ObservableProperty]
    private string _indexSize = "0 MB";

    [ObservableProperty]
    private string? _embeddingModelPath;

    [ObservableProperty]
    private int _embeddingDimension;

    [ObservableProperty]
    private bool _autoIndexEnabled = true;

    [ObservableProperty]
    private bool _respectGitignore = true;

    [ObservableProperty]
    private bool _indexHiddenFiles;

    [ObservableProperty]
    private int _chunkSize = 512;

    [ObservableProperty]
    private int _chunkOverlap = 64;

    [ObservableProperty]
    private int _maxFileSizeKb = 1024;

    [ObservableProperty]
    private string _includePatterns = "*.cs, *.ts, *.js, *.py, *.md";

    [ObservableProperty]
    private string _excludePatterns = "node_modules/**, bin/**, obj/**";

    [ObservableProperty]
    private bool _isIndexing;

    [ObservableProperty]
    private IndexingProgress? _indexingProgress;

    [RelayCommand]
    private async Task LoadIndexStatusAsync()
    {
        var health = await _knowledgeService.GetIndexHealthAsync(WorkspacePath);

        IsIndexed = health.IsIndexed;
        LastUpdated = health.LastUpdated;
        FileCount = health.TotalFiles;
        ChunkCount = health.TotalChunks;
        EmbeddingModelPath = health.EmbeddingModel;

        if (health.IsIndexed && health.IndexId != null)
        {
            var stats = await _vectorStore.GetStatisticsAsync(health.IndexId);
            IndexSize = FormatSize(stats.TotalSizeBytes);
        }
    }

    [RelayCommand]
    private async Task BrowseEmbeddingModelAsync()
    {
        var dialog = new OpenFileDialog
        {
            Title = "Select Embedding Model",
            Filters = new List<FileDialogFilter>
            {
                new() { Name = "GGUF Models", Extensions = { "gguf" } },
                new() { Name = "ONNX Models", Extensions = { "onnx" } }
            }
        };

        var result = await dialog.ShowAsync(GetWindow());
        if (result?.Length > 0)
        {
            EmbeddingModelPath = result[0];
            await LoadEmbeddingModelAsync();
        }
    }

    private async Task LoadEmbeddingModelAsync()
    {
        if (string.IsNullOrEmpty(EmbeddingModelPath))
            return;

        try
        {
            await _embeddingService.LoadModelAsync(new EmbeddingModelOptions
            {
                ModelPath = EmbeddingModelPath,
                ModelType = EmbeddingModelPath.EndsWith(".onnx")
                    ? EmbeddingModelType.Onnx
                    : EmbeddingModelType.Gguf
            });

            EmbeddingDimension = _embeddingService.EmbeddingDimension;
        }
        catch (Exception ex)
        {
            await ShowErrorAsync("Failed to load embedding model", ex.Message);
        }
    }

    [RelayCommand]
    private async Task ReindexAllAsync()
    {
        if (!_embeddingService.IsModelLoaded)
        {
            await ShowErrorAsync("No embedding model", "Please select an embedding model first.");
            return;
        }

        IsIndexing = true;
        var progress = new Progress<IndexingProgress>(p =>
        {
            Dispatcher.UIThread.InvokeAsync(() => IndexingProgress = p);
        });

        try
        {
            var result = await _indexingService.IndexWorkspaceAsync(
                WorkspacePath,
                BuildIndexingOptions(forceReindex: true),
                progress);

            if (result.Success)
            {
                await LoadIndexStatusAsync();
                await ShowSuccessAsync(
                    "Indexing complete",
                    $"Indexed {result.FilesIndexed} files with {result.ChunksCreated} chunks.");
            }
            else
            {
                await ShowErrorAsync("Indexing failed", result.ErrorMessage ?? "Unknown error");
            }
        }
        finally
        {
            IsIndexing = false;
            IndexingProgress = null;
        }
    }

    [RelayCommand]
    private async Task UpdateChangedAsync()
    {
        if (!_embeddingService.IsModelLoaded)
        {
            await ShowErrorAsync("No embedding model", "Please select an embedding model first.");
            return;
        }

        var index = await _vectorStore.GetIndexForWorkspaceAsync(WorkspacePath);
        if (index == null)
        {
            await ReindexAllAsync();
            return;
        }

        IsIndexing = true;
        var progress = new Progress<IndexingProgress>(p =>
        {
            Dispatcher.UIThread.InvokeAsync(() => IndexingProgress = p);
        });

        try
        {
            var result = await _indexingService.UpdateIndexAsync(index.Id, progress);

            if (result.Success)
            {
                await LoadIndexStatusAsync();
                if (result.FilesIndexed > 0)
                {
                    await ShowSuccessAsync(
                        "Update complete",
                        $"Updated {result.FilesIndexed} changed files.");
                }
                else
                {
                    await ShowSuccessAsync("Up to date", "No files needed updating.");
                }
            }
        }
        finally
        {
            IsIndexing = false;
            IndexingProgress = null;
        }
    }

    [RelayCommand]
    private async Task ClearIndexAsync()
    {
        var confirmed = await ShowConfirmAsync(
            "Clear Index",
            "Are you sure you want to clear the index? You will need to re-index the workspace.");

        if (!confirmed)
            return;

        var index = await _vectorStore.GetIndexForWorkspaceAsync(WorkspacePath);
        if (index != null)
        {
            await _vectorStore.DeleteIndexAsync(index.Id);
            await LoadIndexStatusAsync();
        }
    }

    [RelayCommand]
    private void CancelIndexing()
    {
        _indexingService.CancelIndexing();
    }

    [RelayCommand]
    private async Task SaveSettingsAsync()
    {
        var settings = await _settingsService.GetSettingsAsync();
        settings.Rag = new RagSettings
        {
            EmbeddingModelPath = EmbeddingModelPath,
            AutoIndexEnabled = AutoIndexEnabled,
            RespectGitignore = RespectGitignore,
            IndexHiddenFiles = IndexHiddenFiles,
            ChunkSize = ChunkSize,
            ChunkOverlap = ChunkOverlap,
            MaxFileSizeKb = MaxFileSizeKb,
            IncludePatterns = ParsePatterns(IncludePatterns),
            ExcludePatterns = ParsePatterns(ExcludePatterns)
        };
        await _settingsService.SaveSettingsAsync(settings);
    }

    private IndexingOptions BuildIndexingOptions(bool forceReindex = false)
    {
        return new IndexingOptions
        {
            IndexName = Path.GetFileName(WorkspacePath),
            EmbeddingModelPath = EmbeddingModelPath!,
            ChunkingOptions = new ChunkingOptions
            {
                TargetChunkSize = ChunkSize,
                ChunkOverlap = ChunkOverlap
            },
            IncludePatterns = ParsePatterns(IncludePatterns),
            ExcludePatterns = ParsePatterns(ExcludePatterns),
            MaxFileSizeKb = MaxFileSizeKb,
            RespectGitignore = RespectGitignore,
            ForceReindex = forceReindex
        };
    }

    private static List<string> ParsePatterns(string patterns)
    {
        return patterns
            .Split(',', StringSplitOptions.RemoveEmptyEntries)
            .Select(p => p.Trim())
            .Where(p => !string.IsNullOrEmpty(p))
            .ToList();
    }

    private static string FormatSize(long bytes)
    {
        return bytes switch
        {
            < 1024 => $"{bytes} B",
            < 1024 * 1024 => $"{bytes / 1024.0:F1} KB",
            < 1024 * 1024 * 1024 => $"{bytes / (1024.0 * 1024):F1} MB",
            _ => $"{bytes / (1024.0 * 1024 * 1024):F2} GB"
        };
    }
}
```

### File Watcher for Auto-Indexing

```csharp
namespace SeniorIntern.Services;

/// <summary>
/// Watches workspace for file changes and triggers incremental indexing
/// </summary>
public sealed class IndexFileWatcher : IDisposable
{
    private readonly IIndexingService _indexingService;
    private readonly IVectorStore _vectorStore;
    private readonly ILogger<IndexFileWatcher> _logger;

    private FileSystemWatcher? _watcher;
    private string? _workspacePath;
    private string? _indexId;
    private readonly Channel<FileChangeEvent> _changeChannel;
    private readonly CancellationTokenSource _cts = new();
    private Task? _processingTask;

    // Debounce settings
    private readonly TimeSpan _debounceTime = TimeSpan.FromSeconds(2);
    private readonly Dictionary<string, DateTime> _pendingChanges = new();

    public bool IsWatching => _watcher != null;

    public event EventHandler<FileIndexedEventArgs>? FileReindexed;

    public IndexFileWatcher(
        IIndexingService indexingService,
        IVectorStore vectorStore,
        ILogger<IndexFileWatcher> logger)
    {
        _indexingService = indexingService;
        _vectorStore = vectorStore;
        _logger = logger;
        _changeChannel = Channel.CreateUnbounded<FileChangeEvent>();
    }

    public async Task StartWatchingAsync(string workspacePath, CancellationToken ct = default)
    {
        if (_watcher != null)
            StopWatching();

        var index = await _vectorStore.GetIndexForWorkspaceAsync(workspacePath, ct);
        if (index == null)
        {
            _logger.LogWarning("No index found for workspace: {Path}", workspacePath);
            return;
        }

        _workspacePath = workspacePath;
        _indexId = index.Id;

        _watcher = new FileSystemWatcher(workspacePath)
        {
            IncludeSubdirectories = true,
            NotifyFilter = NotifyFilters.LastWrite | NotifyFilters.FileName | NotifyFilters.DirectoryName
        };

        _watcher.Changed += OnFileChanged;
        _watcher.Created += OnFileChanged;
        _watcher.Deleted += OnFileDeleted;
        _watcher.Renamed += OnFileRenamed;

        _watcher.EnableRaisingEvents = true;

        // Start processing task
        _processingTask = ProcessChangesAsync(_cts.Token);

        _logger.LogInformation("Started watching workspace: {Path}", workspacePath);
    }

    public void StopWatching()
    {
        _watcher?.Dispose();
        _watcher = null;
        _cts.Cancel();
        _processingTask?.Wait(TimeSpan.FromSeconds(5));
    }

    private void OnFileChanged(object sender, FileSystemEventArgs e)
    {
        if (ShouldIgnoreFile(e.FullPath))
            return;

        _changeChannel.Writer.TryWrite(new FileChangeEvent
        {
            FilePath = e.FullPath,
            ChangeType = FileChangeType.Modified
        });
    }

    private void OnFileDeleted(object sender, FileSystemEventArgs e)
    {
        if (ShouldIgnoreFile(e.FullPath))
            return;

        _changeChannel.Writer.TryWrite(new FileChangeEvent
        {
            FilePath = e.FullPath,
            ChangeType = FileChangeType.Deleted
        });
    }

    private void OnFileRenamed(object sender, RenamedEventArgs e)
    {
        // Treat as delete old + create new
        _changeChannel.Writer.TryWrite(new FileChangeEvent
        {
            FilePath = e.OldFullPath,
            ChangeType = FileChangeType.Deleted
        });
        _changeChannel.Writer.TryWrite(new FileChangeEvent
        {
            FilePath = e.FullPath,
            ChangeType = FileChangeType.Modified
        });
    }

    private async Task ProcessChangesAsync(CancellationToken ct)
    {
        while (!ct.IsCancellationRequested)
        {
            try
            {
                await foreach (var change in _changeChannel.Reader.ReadAllAsync(ct))
                {
                    // Debounce: track pending changes
                    lock (_pendingChanges)
                    {
                        _pendingChanges[change.FilePath] = DateTime.UtcNow;
                    }

                    // Process after debounce period
                    _ = Task.Run(async () =>
                    {
                        await Task.Delay(_debounceTime, ct);
                        await ProcessPendingChangesAsync(ct);
                    }, ct);
                }
            }
            catch (OperationCanceledException)
            {
                break;
            }
            catch (Exception ex)
            {
                _logger.LogError(ex, "Error processing file changes");
            }
        }
    }

    private async Task ProcessPendingChangesAsync(CancellationToken ct)
    {
        List<(string Path, FileChangeType Type)> toProcess;

        lock (_pendingChanges)
        {
            var cutoff = DateTime.UtcNow - _debounceTime;
            toProcess = _pendingChanges
                .Where(kv => kv.Value <= cutoff)
                .Select(kv => (kv.Key, FileChangeType.Modified))
                .ToList();

            foreach (var path in toProcess.Select(p => p.Path))
                _pendingChanges.Remove(path);
        }

        if (toProcess.Count == 0 || _indexId == null)
            return;

        foreach (var (path, changeType) in toProcess)
        {
            try
            {
                if (changeType == FileChangeType.Deleted || !File.Exists(path))
                {
                    await _vectorStore.RemoveChunksForFileAsync(_indexId, path, ct);
                    _logger.LogDebug("Removed chunks for deleted file: {Path}", path);
                }
                else
                {
                    // Re-index the file
                    await _indexingService.IndexFilesAsync(
                        _indexId,
                        new[] { path },
                        null,
                        ct);
                    _logger.LogDebug("Re-indexed file: {Path}", path);
                }

                FileReindexed?.Invoke(this, new FileIndexedEventArgs
                {
                    FilePath = path,
                    Success = true
                });
            }
            catch (Exception ex)
            {
                _logger.LogWarning(ex, "Failed to process change for: {Path}", path);
            }
        }
    }

    private bool ShouldIgnoreFile(string path)
    {
        // Quick checks for common ignored paths
        var relativePath = Path.GetRelativePath(_workspacePath!, path);

        return relativePath.StartsWith(".git") ||
               relativePath.Contains("node_modules") ||
               relativePath.Contains("bin" + Path.DirectorySeparatorChar) ||
               relativePath.Contains("obj" + Path.DirectorySeparatorChar) ||
               path.EndsWith(".tmp") ||
               path.EndsWith(".lock");
    }

    public void Dispose()
    {
        StopWatching();
        _cts.Dispose();
    }

    private sealed record FileChangeEvent
    {
        public string FilePath { get; init; } = string.Empty;
        public FileChangeType ChangeType { get; init; }
    }

    private enum FileChangeType
    {
        Modified,
        Deleted
    }
}
```

### RAG Settings Model

```csharp
namespace SeniorIntern.Core.Models;

public sealed class RagSettings
{
    /// <summary>
    /// Path to the embedding model
    /// </summary>
    public string? EmbeddingModelPath { get; set; }

    /// <summary>
    /// Whether to auto-index on file changes
    /// </summary>
    public bool AutoIndexEnabled { get; set; } = true;

    /// <summary>
    /// Whether to respect .gitignore
    /// </summary>
    public bool RespectGitignore { get; set; } = true;

    /// <summary>
    /// Whether to index hidden files
    /// </summary>
    public bool IndexHiddenFiles { get; set; } = false;

    /// <summary>
    /// Target chunk size in tokens
    /// </summary>
    public int ChunkSize { get; set; } = 512;

    /// <summary>
    /// Overlap between chunks in tokens
    /// </summary>
    public int ChunkOverlap { get; set; } = 64;

    /// <summary>
    /// Maximum file size to index (KB)
    /// </summary>
    public int MaxFileSizeKb { get; set; } = 1024;

    /// <summary>
    /// File patterns to include
    /// </summary>
    public IReadOnlyList<string> IncludePatterns { get; set; } = new[]
    {
        "**/*.cs", "**/*.ts", "**/*.js", "**/*.py", "**/*.md"
    };

    /// <summary>
    /// File patterns to exclude
    /// </summary>
    public IReadOnlyList<string> ExcludePatterns { get; set; } = new[]
    {
        "**/node_modules/**", "**/bin/**", "**/obj/**"
    };

    /// <summary>
    /// Enable RAG in chat by default
    /// </summary>
    public bool EnableRagInChat { get; set; } = true;

    /// <summary>
    /// Maximum tokens for RAG context
    /// </summary>
    public int MaxRagContextTokens { get; set; } = 4000;

    /// <summary>
    /// Minimum relevance score for including chunks
    /// </summary>
    public float MinRelevanceScore { get; set; } = 0.5f;
}
```

### v0.7.5 Files to Create

| File | Purpose |
|------|---------|
| `Desktop/Views/IndexManagerDialog.axaml` | Index manager UI |
| `Desktop/Views/IndexManagerDialog.axaml.cs` | Code-behind |
| `Desktop/ViewModels/IndexManagerViewModel.cs` | Index manager logic |
| `Desktop/Views/IndexingProgressDialog.axaml` | Progress dialog |
| `Desktop/Views/IndexingProgressDialog.axaml.cs` | Code-behind |
| `Services/IndexFileWatcher.cs` | Auto-indexing watcher |
| `Core/Models/RagSettings.cs` | RAG settings model |
| `Desktop/Controls/RagStatusIndicator.axaml` | Status bar indicator |

---

## NuGet Packages Summary

| Package | Version | Purpose |
|---------|---------|---------|
| `sqlite-vec` | 0.1.x | Vector extension for SQLite |
| `Microsoft.Data.Sqlite` | 8.0.x | SQLite connectivity |
| `Dapper` | 2.1.x | Micro-ORM for complex queries |
| `Microsoft.Extensions.FileSystemGlobbing` | 8.0.x | Glob pattern matching |
| `LLamaSharp` | 0.25.x | Embeddings via GGUF models |

---

## Testing Strategy

### Unit Tests

```csharp
namespace SeniorIntern.Services.Tests;

public class ChunkingServiceTests
{
    [Fact]
    public void Chunk_CSharpFile_PreservesMethodBoundaries()
    {
        var service = new ChunkingService();
        var content = @"
public class UserService
{
    public User GetUser(int id)
    {
        return _repository.Find(id);
    }

    public void UpdateUser(User user)
    {
        _repository.Update(user);
    }
}";

        var chunks = service.ChunkDocument(content, "UserService.cs", new ChunkingOptions
        {
            TargetChunkSize = 100
        });

        // Should have separate chunks for each method
        Assert.True(chunks.Any(c => c.SymbolName == "GetUser"));
        Assert.True(chunks.Any(c => c.SymbolName == "UpdateUser"));
    }

    [Fact]
    public void Chunk_LargeMethod_SplitsWithOverlap()
    {
        // Test that large symbols are split correctly
    }
}

public class KnowledgeServiceTests
{
    [Fact]
    public async Task Query_ReturnsRelevantChunks()
    {
        // Setup mock vector store with known chunks
        var mockStore = new Mock<IVectorStore>();
        // ...

        var service = new KnowledgeService(mockStore.Object, ...);

        var result = await service.QueryAsync(
            "Where is authentication handled?",
            new KnowledgeQueryOptions { WorkspacePath = "/test" });

        Assert.NotEmpty(result.Chunks);
        Assert.All(result.Chunks, c => Assert.True(c.Relevance >= 0.5f));
    }
}
```

### Integration Tests

```csharp
public class IndexingIntegrationTests : IAsyncLifetime
{
    private readonly string _testWorkspace;
    private readonly SqliteVectorStore _vectorStore;
    private readonly IndexingService _indexingService;

    [Fact]
    public async Task IndexWorkspace_IndexesAllFiles()
    {
        // Create test files
        await File.WriteAllTextAsync(
            Path.Combine(_testWorkspace, "Test.cs"),
            "public class Test { }");

        var result = await _indexingService.IndexWorkspaceAsync(
            _testWorkspace,
            new IndexingOptions { ... });

        Assert.True(result.Success);
        Assert.Equal(1, result.FilesIndexed);
        Assert.True(result.ChunksCreated > 0);
    }

    [Fact]
    public async Task IncrementalIndex_OnlyReindexesChangedFiles()
    {
        // First index
        await _indexingService.IndexWorkspaceAsync(_testWorkspace, ...);

        // Modify one file
        await File.WriteAllTextAsync(
            Path.Combine(_testWorkspace, "Test.cs"),
            "public class Test { public void Method() { } }");

        // Update
        var result = await _indexingService.UpdateIndexAsync(...);

        Assert.Equal(1, result.FilesIndexed);
    }
}
```

---

## Files to Create Summary (31 total)

**Core Project (10):**
- `Core/Interfaces/IEmbeddingService.cs`
- `Core/Interfaces/IChunkingService.cs`
- `Core/Interfaces/IVectorStore.cs`
- `Core/Interfaces/IIndexingService.cs`
- `Core/Interfaces/IKnowledgeService.cs`
- `Core/Models/TextChunk.cs`
- `Core/Models/EmbeddingModels.cs`
- `Core/Models/VectorIndex.cs`
- `Core/Models/IndexingModels.cs`
- `Core/Models/KnowledgeModels.cs`
- `Core/Models/RagSettings.cs`
- `Core/Options/VectorStoreOptions.cs`

**Services Project (10):**
- `Services/LlamaEmbeddingService.cs`
- `Services/ChunkingService.cs`
- `Services/Chunking/CodeAwareChunkingStrategy.cs`
- `Services/Chunking/MarkdownChunkingStrategy.cs`
- `Services/Chunking/PlainTextChunkingStrategy.cs`
- `Services/SqliteVectorStore.cs`
- `Services/IndexingService.cs`
- `Services/GitignoreMatcher.cs`
- `Services/KnowledgeService.cs`
- `Services/RagAwareChatService.cs`
- `Services/IndexFileWatcher.cs`

**Desktop Project (8):**
- `Desktop/Views/IndexManagerDialog.axaml`
- `Desktop/Views/IndexManagerDialog.axaml.cs`
- `Desktop/ViewModels/IndexManagerViewModel.cs`
- `Desktop/Views/IndexingProgressDialog.axaml`
- `Desktop/Views/IndexingProgressDialog.axaml.cs`
- `Desktop/Controls/RagStatusIndicator.axaml`
- `Desktop/Controls/RagStatusIndicator.axaml.cs`

**Tests (3):**
- `Tests/SeniorIntern.Services.Tests/ChunkingServiceTests.cs`
- `Tests/SeniorIntern.Services.Tests/KnowledgeServiceTests.cs`
- `Tests/SeniorIntern.Services.Tests/IndexingIntegrationTests.cs`

---

## Acceptance Criteria

### v0.7.1
- [ ] Embedding model can be loaded (GGUF format)
- [ ] Text can be embedded into vectors
- [ ] Code files are chunked respecting language structure
- [ ] Chunks include accurate line numbers and symbol information

### v0.7.2
- [ ] SQLite-vec extension loads successfully
- [ ] Vector indexes can be created and deleted
- [ ] Chunks with embeddings can be stored and retrieved
- [ ] Similarity search returns relevant results

### v0.7.3
- [ ] Workspace can be fully indexed
- [ ] Incremental indexing only processes changed files
- [ ] Progress is reported accurately
- [ ] Indexing can be cancelled

### v0.7.4
- [ ] Questions return relevant code chunks
- [ ] Context is properly formatted for LLM consumption
- [ ] Chat integration uses RAG context when available
- [ ] Relevance filtering works correctly

### v0.7.5
- [ ] Index Manager UI allows configuration
- [ ] Auto-indexing responds to file changes
- [ ] Settings are persisted
- [ ] Status bar shows RAG availability

---

## Risk Assessment

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| sqlite-vec compatibility issues | Medium | High | Test on all platforms early, have FAISS fallback ready |
| Embedding model memory usage | Medium | Medium | Support model unloading, monitor memory, configurable batch sizes |
| Slow indexing for large repos | High | Medium | Background processing, incremental updates, progress UI |
| Poor retrieval quality | Medium | High | Multiple chunking strategies, reranking, user feedback mechanism |
| File watcher reliability | Low | Medium | Debouncing, manual refresh option, periodic full scans |
