# Design Specification: AIntern v0.7.1 "Embedding Foundation"

## Overview

This document provides a comprehensive design specification for v0.7.1 of The Senior Intern project. This version establishes the embedding infrastructure using local embedding models, implements text chunking strategies optimized for code, and defines the vector representation format that powers the RAG system.

### Objectives
- Implement embedding model loading and inference via LLamaSharp
- Add ONNX Runtime support for sentence transformer models
- Create code-aware chunking strategies that respect language structure
- Define text chunk models with comprehensive metadata
- Build tokenization utilities for accurate chunk sizing
- Establish the foundation for semantic code search

### Prerequisites
- v0.6.5 (Agent Loop & Polish) completed
- LLamaSharp infrastructure from v0.4.x available
- Basic understanding of embedding models and vector representations

---

## Sub-version Breakdown

| Version | Focus | Files to Create | Files to Modify |
|---------|-------|-----------------|-----------------|
| v0.7.1a | Embedding Service Interface | 4 | 0 |
| v0.7.1b | LLamaSharp Embedding Implementation | 3 | 1 |
| v0.7.1c | ONNX Embedding Implementation | 4 | 1 |
| v0.7.1d | Text Chunk Models | 3 | 0 |
| v0.7.1e | Chunking Service Interface | 3 | 0 |
| v0.7.1f | Code-Aware Chunking Strategy | 4 | 0 |
| v0.7.1g | Language-Specific Chunkers | 5 | 0 |
| v0.7.1h | Markdown & Plain Text Chunking | 3 | 0 |
| v0.7.1i | Tokenization Utilities | 3 | 0 |
| v0.7.1j | Unit Testing & Integration | 8 | 0 |

---

## v0.7.1a: Embedding Service Interface

### Objective
Define the core interfaces and models for embedding generation, supporting multiple backend implementations.

### File: `src/SeniorIntern.Core/Interfaces/IEmbeddingService.cs`

```csharp
using System;
using System.Collections.Generic;
using System.Threading;
using System.Threading.Tasks;

namespace SeniorIntern.Core.Interfaces;

/// <summary>
/// Service for generating text embeddings using local models.
/// Supports both GGUF models via LLamaSharp and ONNX models.
/// </summary>
public interface IEmbeddingService : IAsyncDisposable
{
    /// <summary>
    /// Whether an embedding model is currently loaded and ready.
    /// </summary>
    bool IsModelLoaded { get; }

    /// <summary>
    /// Dimension of the embedding vectors produced by the current model.
    /// Returns 0 if no model is loaded.
    /// </summary>
    int EmbeddingDimension { get; }

    /// <summary>
    /// Maximum number of tokens the model can process per embedding request.
    /// Text exceeding this limit will be truncated.
    /// </summary>
    int MaxTokens { get; }

    /// <summary>
    /// Name/identifier of the currently loaded model.
    /// </summary>
    string? CurrentModelName { get; }

    /// <summary>
    /// Load an embedding model from disk.
    /// </summary>
    /// <param name="options">Model loading configuration.</param>
    /// <param name="progress">Optional progress reporter.</param>
    /// <param name="ct">Cancellation token.</param>
    Task LoadModelAsync(
        EmbeddingModelOptions options,
        IProgress<ModelLoadProgress>? progress = null,
        CancellationToken ct = default);

    /// <summary>
    /// Unload the currently loaded model and free resources.
    /// </summary>
    Task UnloadModelAsync();

    /// <summary>
    /// Generate an embedding vector for a single text input.
    /// </summary>
    /// <param name="text">The text to embed.</param>
    /// <param name="ct">Cancellation token.</param>
    /// <returns>The embedding vector as a float array.</returns>
    Task<float[]> EmbedAsync(string text, CancellationToken ct = default);

    /// <summary>
    /// Generate embedding vectors for multiple text inputs efficiently.
    /// Uses batching for improved throughput.
    /// </summary>
    /// <param name="texts">Collection of texts to embed.</param>
    /// <param name="progress">Optional progress reporter.</param>
    /// <param name="ct">Cancellation token.</param>
    /// <returns>List of embedding vectors in the same order as input.</returns>
    Task<IReadOnlyList<float[]>> EmbedBatchAsync(
        IEnumerable<string> texts,
        IProgress<EmbeddingProgress>? progress = null,
        CancellationToken ct = default);

    /// <summary>
    /// Calculate cosine similarity between two embedding vectors.
    /// </summary>
    /// <param name="embedding1">First embedding vector.</param>
    /// <param name="embedding2">Second embedding vector.</param>
    /// <returns>Similarity score between -1 and 1.</returns>
    float CosineSimilarity(ReadOnlySpan<float> embedding1, ReadOnlySpan<float> embedding2);

    /// <summary>
    /// Normalize an embedding vector to unit length.
    /// </summary>
    /// <param name="embedding">The embedding to normalize (modified in place).</param>
    void NormalizeEmbedding(Span<float> embedding);

    /// <summary>
    /// Event raised when model loading state changes.
    /// </summary>
    event EventHandler<EmbeddingModelStateChangedEventArgs>? ModelStateChanged;
}
```

### File: `src/SeniorIntern.Core/Models/EmbeddingModels.cs`

```csharp
using System;

namespace SeniorIntern.Core.Models;

/// <summary>
/// Configuration options for loading an embedding model.
/// </summary>
public sealed class EmbeddingModelOptions
{
    /// <summary>
    /// Path to the embedding model file (GGUF or ONNX).
    /// </summary>
    public string ModelPath { get; init; } = string.Empty;

    /// <summary>
    /// Display name for the model.
    /// </summary>
    public string ModelName { get; init; } = string.Empty;

    /// <summary>
    /// Type of model for selecting the appropriate loader.
    /// </summary>
    public EmbeddingModelType ModelType { get; init; } = EmbeddingModelType.Auto;

    /// <summary>
    /// Number of GPU layers to offload (-1 for auto-detect, 0 for CPU only).
    /// </summary>
    public int GpuLayers { get; init; } = -1;

    /// <summary>
    /// Context size (max tokens) for the model.
    /// </summary>
    public int ContextSize { get; init; } = 512;

    /// <summary>
    /// Batch size for embedding generation.
    /// Larger batches are faster but use more memory.
    /// </summary>
    public int BatchSize { get; init; } = 32;

    /// <summary>
    /// Number of threads for CPU inference.
    /// </summary>
    public int Threads { get; init; } = 0; // 0 = auto

    /// <summary>
    /// Whether to normalize embeddings to unit length after generation.
    /// Recommended for cosine similarity comparisons.
    /// </summary>
    public bool NormalizeEmbeddings { get; init; } = true;

    /// <summary>
    /// Pooling strategy for combining token embeddings.
    /// </summary>
    public EmbeddingPoolingType PoolingType { get; init; } = EmbeddingPoolingType.Mean;

    /// <summary>
    /// Optional prefix to prepend to all texts before embedding.
    /// Some models require specific prefixes (e.g., "query: " or "passage: ").
    /// </summary>
    public string? TextPrefix { get; init; }

    /// <summary>
    /// Whether to use memory mapping for model loading.
    /// Faster startup but may affect inference speed.
    /// </summary>
    public bool UseMemoryMapping { get; init; } = true;
}

/// <summary>
/// Types of embedding models supported.
/// </summary>
public enum EmbeddingModelType
{
    /// <summary>
    /// Auto-detect based on file extension.
    /// </summary>
    Auto,

    /// <summary>
    /// GGUF format models loaded via LLamaSharp.
    /// </summary>
    Gguf,

    /// <summary>
    /// ONNX format models loaded via ONNX Runtime.
    /// </summary>
    Onnx,

    /// <summary>
    /// Sentence Transformers models via ML.NET or custom loader.
    /// </summary>
    SentenceTransformers
}

/// <summary>
/// Pooling strategies for combining token embeddings into a single vector.
/// </summary>
public enum EmbeddingPoolingType
{
    /// <summary>
    /// Mean of all token embeddings (most common).
    /// </summary>
    Mean,

    /// <summary>
    /// Use the [CLS] token embedding.
    /// </summary>
    Cls,

    /// <summary>
    /// Use the last token embedding.
    /// </summary>
    Last,

    /// <summary>
    /// Max pooling across token dimension.
    /// </summary>
    Max
}

/// <summary>
/// Progress information during model loading.
/// </summary>
public sealed class ModelLoadProgress
{
    /// <summary>
    /// Current loading stage description.
    /// </summary>
    public string Stage { get; init; } = string.Empty;

    /// <summary>
    /// Percentage complete (0-100).
    /// </summary>
    public double PercentComplete { get; init; }

    /// <summary>
    /// Bytes loaded so far (for large models).
    /// </summary>
    public long? BytesLoaded { get; init; }

    /// <summary>
    /// Total bytes to load.
    /// </summary>
    public long? TotalBytes { get; init; }
}

/// <summary>
/// Progress information during batch embedding.
/// </summary>
public sealed class EmbeddingProgress
{
    /// <summary>
    /// Number of texts processed so far.
    /// </summary>
    public int ProcessedCount { get; init; }

    /// <summary>
    /// Total number of texts to process.
    /// </summary>
    public int TotalCount { get; init; }

    /// <summary>
    /// Percentage complete.
    /// </summary>
    public double PercentComplete => TotalCount > 0
        ? (double)ProcessedCount / TotalCount * 100
        : 0;

    /// <summary>
    /// Current processing rate (texts per second).
    /// </summary>
    public double? TextsPerSecond { get; init; }

    /// <summary>
    /// Estimated time remaining.
    /// </summary>
    public TimeSpan? EstimatedRemaining { get; init; }
}

/// <summary>
/// Event args for model state changes.
/// </summary>
public sealed class EmbeddingModelStateChangedEventArgs : EventArgs
{
    /// <summary>
    /// Whether a model is currently loaded.
    /// </summary>
    public bool IsLoaded { get; init; }

    /// <summary>
    /// Path to the loaded model (null if unloaded).
    /// </summary>
    public string? ModelPath { get; init; }

    /// <summary>
    /// Name of the loaded model.
    /// </summary>
    public string? ModelName { get; init; }

    /// <summary>
    /// Embedding dimension of the loaded model.
    /// </summary>
    public int EmbeddingDimension { get; init; }

    /// <summary>
    /// Error message if loading failed.
    /// </summary>
    public string? Error { get; init; }
}
```

### File: `src/SeniorIntern.Core/Models/EmbeddingModelInfo.cs`

```csharp
using System;

namespace SeniorIntern.Core.Models;

/// <summary>
/// Information about an available embedding model.
/// </summary>
public sealed class EmbeddingModelInfo
{
    /// <summary>
    /// Unique identifier for the model.
    /// </summary>
    public string Id { get; init; } = string.Empty;

    /// <summary>
    /// Display name.
    /// </summary>
    public string Name { get; init; } = string.Empty;

    /// <summary>
    /// Model description.
    /// </summary>
    public string Description { get; init; } = string.Empty;

    /// <summary>
    /// Path to the model file.
    /// </summary>
    public string Path { get; init; } = string.Empty;

    /// <summary>
    /// Model type (GGUF, ONNX, etc.).
    /// </summary>
    public EmbeddingModelType Type { get; init; }

    /// <summary>
    /// Embedding dimension.
    /// </summary>
    public int Dimension { get; init; }

    /// <summary>
    /// Maximum context length in tokens.
    /// </summary>
    public int MaxTokens { get; init; }

    /// <summary>
    /// File size in bytes.
    /// </summary>
    public long SizeBytes { get; init; }

    /// <summary>
    /// Whether this is the default/recommended model.
    /// </summary>
    public bool IsDefault { get; init; }

    /// <summary>
    /// Whether the model supports GPU acceleration.
    /// </summary>
    public bool SupportsGpu { get; init; }

    /// <summary>
    /// Recommended use cases.
    /// </summary>
    public string[] UseCases { get; init; } = Array.Empty<string>();

    /// <summary>
    /// Optional text prefix required by the model.
    /// </summary>
    public string? TextPrefix { get; init; }

    /// <summary>
    /// Human-readable size string.
    /// </summary>
    public string SizeDisplay => SizeBytes switch
    {
        < 1024 => $"{SizeBytes} B",
        < 1024 * 1024 => $"{SizeBytes / 1024.0:F1} KB",
        < 1024 * 1024 * 1024 => $"{SizeBytes / (1024.0 * 1024):F1} MB",
        _ => $"{SizeBytes / (1024.0 * 1024 * 1024):F2} GB"
    };
}

/// <summary>
/// Predefined embedding model configurations.
/// </summary>
public static class EmbeddingModelPresets
{
    /// <summary>
    /// Nomic Embed Text v1.5 - Excellent for code and general text.
    /// </summary>
    public static EmbeddingModelInfo NomicEmbedText => new()
    {
        Id = "nomic-embed-text-v1.5",
        Name = "Nomic Embed Text v1.5",
        Description = "High-quality embedding model optimized for code and technical content",
        Type = EmbeddingModelType.Gguf,
        Dimension = 768,
        MaxTokens = 8192,
        IsDefault = true,
        SupportsGpu = true,
        UseCases = new[] { "Code Search", "Documentation", "General Text" },
        TextPrefix = "search_document: "
    };

    /// <summary>
    /// All-MiniLM-L6-v2 - Fast and lightweight.
    /// </summary>
    public static EmbeddingModelInfo AllMiniLmL6V2 => new()
    {
        Id = "all-minilm-l6-v2",
        Name = "All-MiniLM-L6-v2",
        Description = "Lightweight model for fast embedding generation",
        Type = EmbeddingModelType.Onnx,
        Dimension = 384,
        MaxTokens = 512,
        IsDefault = false,
        SupportsGpu = true,
        UseCases = new[] { "Fast Search", "Low Memory" }
    };

    /// <summary>
    /// BGE-Small-EN - Good balance of quality and speed.
    /// </summary>
    public static EmbeddingModelInfo BgeSmallEn => new()
    {
        Id = "bge-small-en-v1.5",
        Name = "BGE Small EN v1.5",
        Description = "Balanced model from BAAI with good code understanding",
        Type = EmbeddingModelType.Onnx,
        Dimension = 384,
        MaxTokens = 512,
        IsDefault = false,
        SupportsGpu = true,
        UseCases = new[] { "Code Search", "Semantic Search" }
    };

    /// <summary>
    /// CodeBERT - Specialized for code.
    /// </summary>
    public static EmbeddingModelInfo CodeBert => new()
    {
        Id = "codebert-base",
        Name = "CodeBERT Base",
        Description = "Microsoft's model trained specifically on code",
        Type = EmbeddingModelType.Onnx,
        Dimension = 768,
        MaxTokens = 512,
        IsDefault = false,
        SupportsGpu = true,
        UseCases = new[] { "Code Search", "Code Understanding" }
    };
}
```

### File: `src/SeniorIntern.Core/Interfaces/IEmbeddingModelRegistry.cs`

```csharp
using System.Collections.Generic;
using System.Threading;
using System.Threading.Tasks;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Core.Interfaces;

/// <summary>
/// Registry for managing available embedding models.
/// </summary>
public interface IEmbeddingModelRegistry
{
    /// <summary>
    /// Get all available embedding models.
    /// </summary>
    Task<IReadOnlyList<EmbeddingModelInfo>> GetAvailableModelsAsync(
        CancellationToken ct = default);

    /// <summary>
    /// Get a specific model by ID.
    /// </summary>
    Task<EmbeddingModelInfo?> GetModelAsync(
        string modelId,
        CancellationToken ct = default);

    /// <summary>
    /// Get the default/recommended embedding model.
    /// </summary>
    Task<EmbeddingModelInfo?> GetDefaultModelAsync(
        CancellationToken ct = default);

    /// <summary>
    /// Check if a model file exists at the expected path.
    /// </summary>
    Task<bool> IsModelDownloadedAsync(
        string modelId,
        CancellationToken ct = default);

    /// <summary>
    /// Register a custom embedding model.
    /// </summary>
    Task RegisterModelAsync(
        EmbeddingModelInfo model,
        CancellationToken ct = default);

    /// <summary>
    /// Unregister a custom model.
    /// </summary>
    Task UnregisterModelAsync(
        string modelId,
        CancellationToken ct = default);

    /// <summary>
    /// Scan a directory for embedding model files.
    /// </summary>
    Task<IReadOnlyList<EmbeddingModelInfo>> ScanDirectoryAsync(
        string directory,
        CancellationToken ct = default);
}
```

### Acceptance Criteria (v0.7.1a)
- [ ] `IEmbeddingService` interface defined with all methods
- [ ] `EmbeddingModelOptions` supports all configuration options
- [ ] `EmbeddingModelType` enum covers GGUF, ONNX, and SentenceTransformers
- [ ] `EmbeddingPoolingType` includes Mean, CLS, Last, Max
- [ ] Progress reporting models defined
- [ ] Model registry interface for managing available models
- [ ] Preset model configurations defined
- [ ] Documentation comments complete

---

## v0.7.1b: LLamaSharp Embedding Implementation

### Objective
Implement embedding generation using LLamaSharp for GGUF-format embedding models.

### File: `src/SeniorIntern.Services/Embedding/LlamaEmbeddingService.cs`

```csharp
using System;
using System.Collections.Generic;
using System.Diagnostics;
using System.Linq;
using System.Numerics;
using System.Threading;
using System.Threading.Tasks;
using LLama;
using LLama.Common;
using LLama.Native;
using Microsoft.Extensions.Logging;
using SeniorIntern.Core.Interfaces;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Services.Embedding;

/// <summary>
/// Embedding service implementation using LLamaSharp for GGUF models.
/// </summary>
public sealed class LlamaEmbeddingService : IEmbeddingService
{
    private readonly ILogger<LlamaEmbeddingService> _logger;
    private readonly SemaphoreSlim _loadLock = new(1, 1);
    private readonly SemaphoreSlim _embedLock = new(1, 1);

    private LLamaWeights? _model;
    private LLamaEmbedder? _embedder;
    private EmbeddingModelOptions? _currentOptions;
    private bool _isDisposed;

    public bool IsModelLoaded => _embedder is not null;
    public int EmbeddingDimension { get; private set; }
    public int MaxTokens { get; private set; } = 512;
    public string? CurrentModelName { get; private set; }

    public event EventHandler<EmbeddingModelStateChangedEventArgs>? ModelStateChanged;

    public LlamaEmbeddingService(ILogger<LlamaEmbeddingService> logger)
    {
        _logger = logger;
    }

    public async Task LoadModelAsync(
        EmbeddingModelOptions options,
        IProgress<ModelLoadProgress>? progress = null,
        CancellationToken ct = default)
    {
        ObjectDisposedException.ThrowIf(_isDisposed, this);

        if (options.ModelType != EmbeddingModelType.Gguf &&
            options.ModelType != EmbeddingModelType.Auto)
        {
            throw new ArgumentException(
                $"LlamaEmbeddingService only supports GGUF models, not {options.ModelType}",
                nameof(options));
        }

        await _loadLock.WaitAsync(ct);
        try
        {
            // Unload existing model if any
            if (_embedder is not null)
            {
                _logger.LogInformation("Unloading existing embedding model");
                await UnloadModelInternalAsync();
            }

            _logger.LogInformation("Loading GGUF embedding model: {Path}", options.ModelPath);

            progress?.Report(new ModelLoadProgress
            {
                Stage = "Initializing model parameters",
                PercentComplete = 5
            });

            // Configure model parameters
            var modelParams = new ModelParams(options.ModelPath)
            {
                ContextSize = (uint)options.ContextSize,
                GpuLayerCount = options.GpuLayers == -1
                    ? DetectOptimalGpuLayers()
                    : options.GpuLayers,
                Threads = options.Threads == 0
                    ? (uint)Environment.ProcessorCount
                    : (uint)options.Threads,
                BatchSize = (uint)options.BatchSize,
                EmbeddingMode = true,
                UseMemorymap = options.UseMemoryMapping
            };

            progress?.Report(new ModelLoadProgress
            {
                Stage = "Loading model weights",
                PercentComplete = 20
            });

            // Load model weights
            _model = await Task.Run(
                () => LLamaWeights.LoadFromFile(modelParams),
                ct);

            progress?.Report(new ModelLoadProgress
            {
                Stage = "Creating embedder",
                PercentComplete = 80
            });

            // Create embedder
            _embedder = new LLamaEmbedder(_model, modelParams);

            // Store configuration
            _currentOptions = options;
            EmbeddingDimension = _model.EmbeddingSize;
            MaxTokens = options.ContextSize;
            CurrentModelName = options.ModelName;

            progress?.Report(new ModelLoadProgress
            {
                Stage = "Model ready",
                PercentComplete = 100
            });

            _logger.LogInformation(
                "Loaded embedding model: {Name}, Dimension: {Dim}, GPU Layers: {Gpu}",
                options.ModelName,
                EmbeddingDimension,
                modelParams.GpuLayerCount);

            ModelStateChanged?.Invoke(this, new EmbeddingModelStateChangedEventArgs
            {
                IsLoaded = true,
                ModelPath = options.ModelPath,
                ModelName = options.ModelName,
                EmbeddingDimension = EmbeddingDimension
            });
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Failed to load embedding model: {Path}", options.ModelPath);

            ModelStateChanged?.Invoke(this, new EmbeddingModelStateChangedEventArgs
            {
                IsLoaded = false,
                Error = ex.Message
            });

            throw;
        }
        finally
        {
            _loadLock.Release();
        }
    }

    public async Task UnloadModelAsync()
    {
        await _loadLock.WaitAsync();
        try
        {
            await UnloadModelInternalAsync();
        }
        finally
        {
            _loadLock.Release();
        }
    }

    private Task UnloadModelInternalAsync()
    {
        _embedder?.Dispose();
        _embedder = null;

        _model?.Dispose();
        _model = null;

        _currentOptions = null;
        EmbeddingDimension = 0;
        MaxTokens = 512;
        CurrentModelName = null;

        // Force garbage collection to release GPU memory
        GC.Collect();
        GC.WaitForPendingFinalizers();

        ModelStateChanged?.Invoke(this, new EmbeddingModelStateChangedEventArgs
        {
            IsLoaded = false
        });

        _logger.LogInformation("Embedding model unloaded");

        return Task.CompletedTask;
    }

    public async Task<float[]> EmbedAsync(string text, CancellationToken ct = default)
    {
        ObjectDisposedException.ThrowIf(_isDisposed, this);

        if (_embedder is null)
            throw new InvalidOperationException("No embedding model is loaded");

        if (string.IsNullOrEmpty(text))
            return new float[EmbeddingDimension];

        // Apply text prefix if configured
        var processedText = _currentOptions?.TextPrefix is not null
            ? _currentOptions.TextPrefix + text
            : text;

        await _embedLock.WaitAsync(ct);
        try
        {
            var embeddings = await Task.Run(
                () => _embedder.GetEmbeddings(processedText),
                ct);

            var embedding = embeddings.First();

            // Normalize if configured
            if (_currentOptions?.NormalizeEmbeddings == true)
                NormalizeEmbedding(embedding);

            return embedding;
        }
        finally
        {
            _embedLock.Release();
        }
    }

    public async Task<IReadOnlyList<float[]>> EmbedBatchAsync(
        IEnumerable<string> texts,
        IProgress<EmbeddingProgress>? progress = null,
        CancellationToken ct = default)
    {
        ObjectDisposedException.ThrowIf(_isDisposed, this);

        if (_embedder is null)
            throw new InvalidOperationException("No embedding model is loaded");

        var textList = texts.ToList();
        if (textList.Count == 0)
            return Array.Empty<float[]>();

        var results = new List<float[]>(textList.Count);
        var stopwatch = Stopwatch.StartNew();

        await _embedLock.WaitAsync(ct);
        try
        {
            var batchSize = _currentOptions?.BatchSize ?? 32;

            for (int i = 0; i < textList.Count; i++)
            {
                ct.ThrowIfCancellationRequested();

                var text = textList[i];

                // Apply text prefix if configured
                if (_currentOptions?.TextPrefix is not null)
                    text = _currentOptions.TextPrefix + text;

                // Handle empty text
                if (string.IsNullOrEmpty(text))
                {
                    results.Add(new float[EmbeddingDimension]);
                    continue;
                }

                var embeddings = await Task.Run(
                    () => _embedder.GetEmbeddings(text),
                    ct);

                var embedding = embeddings.First();

                // Normalize if configured
                if (_currentOptions?.NormalizeEmbeddings == true)
                    NormalizeEmbedding(embedding);

                results.Add(embedding);

                // Report progress
                if (progress is not null && (i + 1) % 10 == 0 || i == textList.Count - 1)
                {
                    var elapsed = stopwatch.Elapsed;
                    var rate = (i + 1) / elapsed.TotalSeconds;
                    var remaining = TimeSpan.FromSeconds((textList.Count - i - 1) / rate);

                    progress.Report(new EmbeddingProgress
                    {
                        ProcessedCount = i + 1,
                        TotalCount = textList.Count,
                        TextsPerSecond = rate,
                        EstimatedRemaining = remaining
                    });
                }
            }
        }
        finally
        {
            _embedLock.Release();
        }

        _logger.LogDebug(
            "Generated {Count} embeddings in {Elapsed:F2}s ({Rate:F1}/s)",
            results.Count,
            stopwatch.Elapsed.TotalSeconds,
            results.Count / stopwatch.Elapsed.TotalSeconds);

        return results;
    }

    public float CosineSimilarity(ReadOnlySpan<float> embedding1, ReadOnlySpan<float> embedding2)
    {
        if (embedding1.Length != embedding2.Length)
            throw new ArgumentException("Embeddings must have the same dimension");

        if (embedding1.Length == 0)
            return 0f;

        // Use SIMD for efficient computation
        float dot = 0f, norm1 = 0f, norm2 = 0f;

        // Try to use hardware acceleration
        if (Vector.IsHardwareAccelerated && embedding1.Length >= Vector<float>.Count)
        {
            var vectorSize = Vector<float>.Count;
            var i = 0;

            var dotVec = Vector<float>.Zero;
            var norm1Vec = Vector<float>.Zero;
            var norm2Vec = Vector<float>.Zero;

            for (; i <= embedding1.Length - vectorSize; i += vectorSize)
            {
                var v1 = new Vector<float>(embedding1.Slice(i, vectorSize));
                var v2 = new Vector<float>(embedding2.Slice(i, vectorSize));

                dotVec += v1 * v2;
                norm1Vec += v1 * v1;
                norm2Vec += v2 * v2;
            }

            // Sum vector elements
            for (int j = 0; j < vectorSize; j++)
            {
                dot += dotVec[j];
                norm1 += norm1Vec[j];
                norm2 += norm2Vec[j];
            }

            // Handle remaining elements
            for (; i < embedding1.Length; i++)
            {
                dot += embedding1[i] * embedding2[i];
                norm1 += embedding1[i] * embedding1[i];
                norm2 += embedding2[i] * embedding2[i];
            }
        }
        else
        {
            // Scalar fallback
            for (int i = 0; i < embedding1.Length; i++)
            {
                dot += embedding1[i] * embedding2[i];
                norm1 += embedding1[i] * embedding1[i];
                norm2 += embedding2[i] * embedding2[i];
            }
        }

        var magnitude = MathF.Sqrt(norm1) * MathF.Sqrt(norm2);
        return magnitude > 0 ? dot / magnitude : 0f;
    }

    public void NormalizeEmbedding(Span<float> embedding)
    {
        if (embedding.Length == 0)
            return;

        float sumSquares = 0f;

        // Use SIMD if available
        if (Vector.IsHardwareAccelerated && embedding.Length >= Vector<float>.Count)
        {
            var vectorSize = Vector<float>.Count;
            var sumVec = Vector<float>.Zero;
            var i = 0;

            for (; i <= embedding.Length - vectorSize; i += vectorSize)
            {
                var v = new Vector<float>(embedding.Slice(i, vectorSize));
                sumVec += v * v;
            }

            for (int j = 0; j < vectorSize; j++)
                sumSquares += sumVec[j];

            for (; i < embedding.Length; i++)
                sumSquares += embedding[i] * embedding[i];
        }
        else
        {
            foreach (var val in embedding)
                sumSquares += val * val;
        }

        var magnitude = MathF.Sqrt(sumSquares);
        if (magnitude <= 0)
            return;

        // Normalize in place
        for (int i = 0; i < embedding.Length; i++)
            embedding[i] /= magnitude;
    }

    private static int DetectOptimalGpuLayers()
    {
        // For embedding models, typically load all layers to GPU if available
        if (OperatingSystem.IsMacOS())
        {
            // Metal backend - use all layers
            return 999;
        }

        // Check for CUDA availability
        try
        {
            var cudaAvailable = NativeLibraryConfig.All.WithCuda;
            if (cudaAvailable)
                return 999;
        }
        catch
        {
            // CUDA not available
        }

        // CPU only
        return 0;
    }

    public async ValueTask DisposeAsync()
    {
        if (_isDisposed)
            return;

        _isDisposed = true;

        await UnloadModelAsync();

        _loadLock.Dispose();
        _embedLock.Dispose();
    }
}
```

### File: `src/SeniorIntern.Services/Embedding/EmbeddingServiceFactory.cs`

```csharp
using System;
using System.IO;
using Microsoft.Extensions.DependencyInjection;
using Microsoft.Extensions.Logging;
using SeniorIntern.Core.Interfaces;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Services.Embedding;

/// <summary>
/// Factory for creating appropriate embedding service instances based on model type.
/// </summary>
public sealed class EmbeddingServiceFactory : IEmbeddingServiceFactory
{
    private readonly IServiceProvider _serviceProvider;
    private readonly ILogger<EmbeddingServiceFactory> _logger;

    public EmbeddingServiceFactory(
        IServiceProvider serviceProvider,
        ILogger<EmbeddingServiceFactory> logger)
    {
        _serviceProvider = serviceProvider;
        _logger = logger;
    }

    /// <summary>
    /// Create an embedding service for the specified model options.
    /// </summary>
    public IEmbeddingService CreateService(EmbeddingModelOptions options)
    {
        var modelType = options.ModelType;

        // Auto-detect model type from file extension
        if (modelType == EmbeddingModelType.Auto)
        {
            modelType = DetectModelType(options.ModelPath);
            _logger.LogDebug(
                "Auto-detected model type {Type} for {Path}",
                modelType, options.ModelPath);
        }

        return modelType switch
        {
            EmbeddingModelType.Gguf => _serviceProvider
                .GetRequiredService<LlamaEmbeddingService>(),

            EmbeddingModelType.Onnx => _serviceProvider
                .GetRequiredService<OnnxEmbeddingService>(),

            EmbeddingModelType.SentenceTransformers => throw new NotSupportedException(
                "SentenceTransformers models are not yet supported"),

            _ => throw new ArgumentException($"Unknown model type: {modelType}")
        };
    }

    /// <summary>
    /// Detect model type from file extension.
    /// </summary>
    public static EmbeddingModelType DetectModelType(string modelPath)
    {
        var extension = Path.GetExtension(modelPath).ToLowerInvariant();

        return extension switch
        {
            ".gguf" => EmbeddingModelType.Gguf,
            ".onnx" => EmbeddingModelType.Onnx,
            ".bin" => EmbeddingModelType.SentenceTransformers,
            _ => throw new ArgumentException(
                $"Cannot determine model type from extension: {extension}")
        };
    }
}

/// <summary>
/// Interface for embedding service factory.
/// </summary>
public interface IEmbeddingServiceFactory
{
    IEmbeddingService CreateService(EmbeddingModelOptions options);
}
```

### File: `src/SeniorIntern.Services/Embedding/EmbeddingServiceExtensions.cs`

```csharp
using Microsoft.Extensions.DependencyInjection;
using SeniorIntern.Core.Interfaces;

namespace SeniorIntern.Services.Embedding;

/// <summary>
/// Extension methods for registering embedding services.
/// </summary>
public static class EmbeddingServiceExtensions
{
    /// <summary>
    /// Add embedding services to the service collection.
    /// </summary>
    public static IServiceCollection AddEmbeddingServices(this IServiceCollection services)
    {
        // Register individual service implementations
        services.AddSingleton<LlamaEmbeddingService>();
        services.AddSingleton<OnnxEmbeddingService>();

        // Register factory
        services.AddSingleton<IEmbeddingServiceFactory, EmbeddingServiceFactory>();

        // Register model registry
        services.AddSingleton<IEmbeddingModelRegistry, EmbeddingModelRegistry>();

        return services;
    }
}
```

### Modification: `src/SeniorIntern.Services/SeniorIntern.Services.csproj`

Add LLamaSharp package reference if not already present:

```xml
<ItemGroup>
  <!-- Existing packages... -->
  <PackageReference Include="LLamaSharp" Version="0.18.0" />
  <PackageReference Include="LLamaSharp.Backend.Cpu" Version="0.18.0" Condition="'$(RuntimeIdentifier)' == ''" />
</ItemGroup>
```

### Acceptance Criteria (v0.7.1b)
- [ ] LlamaEmbeddingService loads GGUF embedding models
- [ ] Single text embedding generation works
- [ ] Batch embedding with progress reporting works
- [ ] Cosine similarity calculation uses SIMD when available
- [ ] Embedding normalization implemented
- [ ] GPU layer auto-detection for macOS Metal
- [ ] Model unloading frees resources properly
- [ ] Factory creates appropriate service based on model type

---

## v0.7.1c: ONNX Embedding Implementation

### Objective
Implement embedding generation using ONNX Runtime for ONNX-format models like all-MiniLM and BGE.

### File: `src/SeniorIntern.Services/Embedding/OnnxEmbeddingService.cs`

```csharp
using System;
using System.Collections.Generic;
using System.Diagnostics;
using System.IO;
using System.Linq;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.Extensions.Logging;
using Microsoft.ML.OnnxRuntime;
using Microsoft.ML.OnnxRuntime.Tensors;
using SeniorIntern.Core.Interfaces;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Services.Embedding;

/// <summary>
/// Embedding service implementation using ONNX Runtime.
/// Supports sentence transformer models exported to ONNX format.
/// </summary>
public sealed class OnnxEmbeddingService : IEmbeddingService
{
    private readonly ILogger<OnnxEmbeddingService> _logger;
    private readonly ITokenizerService _tokenizerService;
    private readonly SemaphoreSlim _loadLock = new(1, 1);
    private readonly SemaphoreSlim _embedLock = new(1, 1);

    private InferenceSession? _session;
    private EmbeddingModelOptions? _currentOptions;
    private string[]? _inputNames;
    private string? _outputName;
    private bool _isDisposed;

    public bool IsModelLoaded => _session is not null;
    public int EmbeddingDimension { get; private set; }
    public int MaxTokens { get; private set; } = 512;
    public string? CurrentModelName { get; private set; }

    public event EventHandler<EmbeddingModelStateChangedEventArgs>? ModelStateChanged;

    public OnnxEmbeddingService(
        ILogger<OnnxEmbeddingService> logger,
        ITokenizerService tokenizerService)
    {
        _logger = logger;
        _tokenizerService = tokenizerService;
    }

    public async Task LoadModelAsync(
        EmbeddingModelOptions options,
        IProgress<ModelLoadProgress>? progress = null,
        CancellationToken ct = default)
    {
        ObjectDisposedException.ThrowIf(_isDisposed, this);

        if (options.ModelType != EmbeddingModelType.Onnx &&
            options.ModelType != EmbeddingModelType.Auto)
        {
            throw new ArgumentException(
                $"OnnxEmbeddingService only supports ONNX models, not {options.ModelType}",
                nameof(options));
        }

        await _loadLock.WaitAsync(ct);
        try
        {
            if (_session is not null)
            {
                _logger.LogInformation("Unloading existing ONNX model");
                await UnloadModelInternalAsync();
            }

            _logger.LogInformation("Loading ONNX embedding model: {Path}", options.ModelPath);

            progress?.Report(new ModelLoadProgress
            {
                Stage = "Configuring session options",
                PercentComplete = 10
            });

            // Configure session options
            var sessionOptions = new SessionOptions();

            // Set thread count
            if (options.Threads > 0)
            {
                sessionOptions.IntraOpNumThreads = options.Threads;
                sessionOptions.InterOpNumThreads = options.Threads;
            }

            // Enable GPU if available and requested
            if (options.GpuLayers != 0)
            {
                try
                {
                    if (OperatingSystem.IsWindows() || OperatingSystem.IsLinux())
                    {
                        // Try CUDA first
                        sessionOptions.AppendExecutionProvider_CUDA();
                        _logger.LogInformation("CUDA execution provider enabled");
                    }
                    else if (OperatingSystem.IsMacOS())
                    {
                        // Use CoreML on macOS
                        sessionOptions.AppendExecutionProvider_CoreML();
                        _logger.LogInformation("CoreML execution provider enabled");
                    }
                }
                catch (Exception ex)
                {
                    _logger.LogWarning(ex, "GPU acceleration not available, using CPU");
                }
            }

            // Set optimization level
            sessionOptions.GraphOptimizationLevel = GraphOptimizationLevel.ORT_ENABLE_ALL;

            progress?.Report(new ModelLoadProgress
            {
                Stage = "Loading model",
                PercentComplete = 30
            });

            // Load the model
            _session = await Task.Run(
                () => new InferenceSession(options.ModelPath, sessionOptions),
                ct);

            // Discover input/output names
            _inputNames = _session.InputMetadata.Keys.ToArray();
            _outputName = _session.OutputMetadata.Keys.FirstOrDefault(k =>
                k.Contains("embedding", StringComparison.OrdinalIgnoreCase) ||
                k.Contains("output", StringComparison.OrdinalIgnoreCase) ||
                k.Contains("last_hidden", StringComparison.OrdinalIgnoreCase));

            if (_outputName is null)
                _outputName = _session.OutputMetadata.Keys.First();

            // Determine embedding dimension from output shape
            var outputMeta = _session.OutputMetadata[_outputName];
            EmbeddingDimension = (int)outputMeta.Dimensions.Last();

            progress?.Report(new ModelLoadProgress
            {
                Stage = "Loading tokenizer",
                PercentComplete = 70
            });

            // Load tokenizer for this model
            var tokenizerPath = Path.Combine(
                Path.GetDirectoryName(options.ModelPath)!,
                "tokenizer.json");

            if (File.Exists(tokenizerPath))
            {
                await _tokenizerService.LoadTokenizerAsync(tokenizerPath, ct);
            }
            else
            {
                _logger.LogWarning(
                    "Tokenizer not found at {Path}, using default",
                    tokenizerPath);
            }

            _currentOptions = options;
            MaxTokens = options.ContextSize;
            CurrentModelName = options.ModelName;

            progress?.Report(new ModelLoadProgress
            {
                Stage = "Model ready",
                PercentComplete = 100
            });

            _logger.LogInformation(
                "Loaded ONNX model: {Name}, Dimension: {Dim}, Inputs: {Inputs}",
                options.ModelName,
                EmbeddingDimension,
                string.Join(", ", _inputNames));

            ModelStateChanged?.Invoke(this, new EmbeddingModelStateChangedEventArgs
            {
                IsLoaded = true,
                ModelPath = options.ModelPath,
                ModelName = options.ModelName,
                EmbeddingDimension = EmbeddingDimension
            });
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Failed to load ONNX model: {Path}", options.ModelPath);

            ModelStateChanged?.Invoke(this, new EmbeddingModelStateChangedEventArgs
            {
                IsLoaded = false,
                Error = ex.Message
            });

            throw;
        }
        finally
        {
            _loadLock.Release();
        }
    }

    public async Task UnloadModelAsync()
    {
        await _loadLock.WaitAsync();
        try
        {
            await UnloadModelInternalAsync();
        }
        finally
        {
            _loadLock.Release();
        }
    }

    private Task UnloadModelInternalAsync()
    {
        _session?.Dispose();
        _session = null;

        _currentOptions = null;
        _inputNames = null;
        _outputName = null;
        EmbeddingDimension = 0;
        MaxTokens = 512;
        CurrentModelName = null;

        ModelStateChanged?.Invoke(this, new EmbeddingModelStateChangedEventArgs
        {
            IsLoaded = false
        });

        _logger.LogInformation("ONNX model unloaded");

        return Task.CompletedTask;
    }

    public async Task<float[]> EmbedAsync(string text, CancellationToken ct = default)
    {
        ObjectDisposedException.ThrowIf(_isDisposed, this);

        if (_session is null)
            throw new InvalidOperationException("No embedding model is loaded");

        if (string.IsNullOrEmpty(text))
            return new float[EmbeddingDimension];

        // Apply text prefix if configured
        var processedText = _currentOptions?.TextPrefix is not null
            ? _currentOptions.TextPrefix + text
            : text;

        await _embedLock.WaitAsync(ct);
        try
        {
            // Tokenize
            var tokens = _tokenizerService.Encode(processedText, MaxTokens);

            // Create input tensors
            var inputs = CreateInputTensors(tokens, 1);

            // Run inference
            using var results = _session.Run(inputs);

            // Extract embedding
            var outputTensor = results.First(r => r.Name == _outputName).AsTensor<float>();
            var embedding = ExtractEmbedding(outputTensor, _currentOptions?.PoolingType ?? EmbeddingPoolingType.Mean);

            // Normalize if configured
            if (_currentOptions?.NormalizeEmbeddings == true)
                NormalizeEmbedding(embedding);

            return embedding;
        }
        finally
        {
            _embedLock.Release();
        }
    }

    public async Task<IReadOnlyList<float[]>> EmbedBatchAsync(
        IEnumerable<string> texts,
        IProgress<EmbeddingProgress>? progress = null,
        CancellationToken ct = default)
    {
        ObjectDisposedException.ThrowIf(_isDisposed, this);

        if (_session is null)
            throw new InvalidOperationException("No embedding model is loaded");

        var textList = texts.ToList();
        if (textList.Count == 0)
            return Array.Empty<float[]>();

        var results = new List<float[]>(textList.Count);
        var stopwatch = Stopwatch.StartNew();
        var batchSize = _currentOptions?.BatchSize ?? 32;

        await _embedLock.WaitAsync(ct);
        try
        {
            // Process in batches
            for (int batchStart = 0; batchStart < textList.Count; batchStart += batchSize)
            {
                ct.ThrowIfCancellationRequested();

                var batchEnd = Math.Min(batchStart + batchSize, textList.Count);
                var batch = textList.GetRange(batchStart, batchEnd - batchStart);

                // Apply text prefix if configured
                if (_currentOptions?.TextPrefix is not null)
                {
                    batch = batch.Select(t => _currentOptions.TextPrefix + t).ToList();
                }

                // Tokenize batch
                var tokenizedBatch = batch
                    .Select(t => string.IsNullOrEmpty(t)
                        ? new TokenizedText { InputIds = Array.Empty<long>(), AttentionMask = Array.Empty<long>() }
                        : _tokenizerService.Encode(t, MaxTokens))
                    .ToList();

                // Pad to same length
                var maxLen = tokenizedBatch.Max(t => t.InputIds.Length);
                if (maxLen == 0)
                {
                    // All empty texts
                    results.AddRange(batch.Select(_ => new float[EmbeddingDimension]));
                    continue;
                }

                // Create batched input tensors
                var inputs = CreateBatchedInputTensors(tokenizedBatch, maxLen);

                // Run inference
                using var inferenceResults = _session.Run(inputs);

                // Extract embeddings for each item in batch
                var outputTensor = inferenceResults
                    .First(r => r.Name == _outputName)
                    .AsTensor<float>();

                for (int i = 0; i < batch.Count; i++)
                {
                    var embedding = ExtractBatchEmbedding(
                        outputTensor, i, maxLen,
                        _currentOptions?.PoolingType ?? EmbeddingPoolingType.Mean);

                    if (_currentOptions?.NormalizeEmbeddings == true)
                        NormalizeEmbedding(embedding);

                    results.Add(embedding);
                }

                // Report progress
                if (progress is not null)
                {
                    var elapsed = stopwatch.Elapsed;
                    var rate = results.Count / elapsed.TotalSeconds;
                    var remaining = TimeSpan.FromSeconds((textList.Count - results.Count) / rate);

                    progress.Report(new EmbeddingProgress
                    {
                        ProcessedCount = results.Count,
                        TotalCount = textList.Count,
                        TextsPerSecond = rate,
                        EstimatedRemaining = remaining
                    });
                }
            }
        }
        finally
        {
            _embedLock.Release();
        }

        _logger.LogDebug(
            "Generated {Count} embeddings in {Elapsed:F2}s ({Rate:F1}/s)",
            results.Count,
            stopwatch.Elapsed.TotalSeconds,
            results.Count / stopwatch.Elapsed.TotalSeconds);

        return results;
    }

    private IReadOnlyCollection<NamedOnnxValue> CreateInputTensors(
        TokenizedText tokens,
        int batchSize)
    {
        var seqLen = tokens.InputIds.Length;
        var inputs = new List<NamedOnnxValue>();

        // Input IDs
        var inputIdsTensor = new DenseTensor<long>(
            tokens.InputIds,
            new[] { batchSize, seqLen });
        inputs.Add(NamedOnnxValue.CreateFromTensor("input_ids", inputIdsTensor));

        // Attention mask
        if (_inputNames!.Contains("attention_mask"))
        {
            var attentionTensor = new DenseTensor<long>(
                tokens.AttentionMask,
                new[] { batchSize, seqLen });
            inputs.Add(NamedOnnxValue.CreateFromTensor("attention_mask", attentionTensor));
        }

        // Token type IDs (usually all zeros for single-sequence)
        if (_inputNames.Contains("token_type_ids"))
        {
            var typeIds = new long[seqLen];
            var typeTensor = new DenseTensor<long>(typeIds, new[] { batchSize, seqLen });
            inputs.Add(NamedOnnxValue.CreateFromTensor("token_type_ids", typeTensor));
        }

        return inputs;
    }

    private IReadOnlyCollection<NamedOnnxValue> CreateBatchedInputTensors(
        List<TokenizedText> batch,
        int maxLen)
    {
        var batchSize = batch.Count;
        var inputs = new List<NamedOnnxValue>();

        // Pad and create batched arrays
        var inputIds = new long[batchSize * maxLen];
        var attentionMask = new long[batchSize * maxLen];

        for (int i = 0; i < batch.Count; i++)
        {
            var tokens = batch[i];
            var offset = i * maxLen;

            for (int j = 0; j < tokens.InputIds.Length; j++)
            {
                inputIds[offset + j] = tokens.InputIds[j];
                attentionMask[offset + j] = tokens.AttentionMask[j];
            }
            // Rest is already 0 (padding)
        }

        var inputIdsTensor = new DenseTensor<long>(inputIds, new[] { batchSize, maxLen });
        inputs.Add(NamedOnnxValue.CreateFromTensor("input_ids", inputIdsTensor));

        if (_inputNames!.Contains("attention_mask"))
        {
            var attentionTensor = new DenseTensor<long>(attentionMask, new[] { batchSize, maxLen });
            inputs.Add(NamedOnnxValue.CreateFromTensor("attention_mask", attentionTensor));
        }

        if (_inputNames.Contains("token_type_ids"))
        {
            var typeIds = new long[batchSize * maxLen];
            var typeTensor = new DenseTensor<long>(typeIds, new[] { batchSize, maxLen });
            inputs.Add(NamedOnnxValue.CreateFromTensor("token_type_ids", typeTensor));
        }

        return inputs;
    }

    private float[] ExtractEmbedding(Tensor<float> outputTensor, EmbeddingPoolingType pooling)
    {
        // Output shape is typically [batch, seq_len, hidden_dim] or [batch, hidden_dim]
        var dimensions = outputTensor.Dimensions.ToArray();

        if (dimensions.Length == 2)
        {
            // Already pooled: [batch, hidden_dim]
            var embedding = new float[EmbeddingDimension];
            for (int i = 0; i < EmbeddingDimension; i++)
                embedding[i] = outputTensor[0, i];
            return embedding;
        }

        // Need to pool: [batch, seq_len, hidden_dim]
        var seqLen = dimensions[1];
        return ApplyPooling(outputTensor, 0, seqLen, pooling);
    }

    private float[] ExtractBatchEmbedding(
        Tensor<float> outputTensor,
        int batchIndex,
        int seqLen,
        EmbeddingPoolingType pooling)
    {
        var dimensions = outputTensor.Dimensions.ToArray();

        if (dimensions.Length == 2)
        {
            // Already pooled
            var embedding = new float[EmbeddingDimension];
            for (int i = 0; i < EmbeddingDimension; i++)
                embedding[i] = outputTensor[batchIndex, i];
            return embedding;
        }

        return ApplyPooling(outputTensor, batchIndex, seqLen, pooling);
    }

    private float[] ApplyPooling(
        Tensor<float> tensor,
        int batchIndex,
        int seqLen,
        EmbeddingPoolingType pooling)
    {
        var embedding = new float[EmbeddingDimension];

        switch (pooling)
        {
            case EmbeddingPoolingType.Mean:
                // Mean of all token embeddings
                for (int d = 0; d < EmbeddingDimension; d++)
                {
                    float sum = 0;
                    for (int s = 0; s < seqLen; s++)
                        sum += tensor[batchIndex, s, d];
                    embedding[d] = sum / seqLen;
                }
                break;

            case EmbeddingPoolingType.Cls:
                // First token ([CLS])
                for (int d = 0; d < EmbeddingDimension; d++)
                    embedding[d] = tensor[batchIndex, 0, d];
                break;

            case EmbeddingPoolingType.Last:
                // Last token
                for (int d = 0; d < EmbeddingDimension; d++)
                    embedding[d] = tensor[batchIndex, seqLen - 1, d];
                break;

            case EmbeddingPoolingType.Max:
                // Max pooling
                for (int d = 0; d < EmbeddingDimension; d++)
                {
                    float max = float.MinValue;
                    for (int s = 0; s < seqLen; s++)
                        max = Math.Max(max, tensor[batchIndex, s, d]);
                    embedding[d] = max;
                }
                break;
        }

        return embedding;
    }

    public float CosineSimilarity(ReadOnlySpan<float> embedding1, ReadOnlySpan<float> embedding2)
    {
        // Delegate to shared implementation
        return EmbeddingMath.CosineSimilarity(embedding1, embedding2);
    }

    public void NormalizeEmbedding(Span<float> embedding)
    {
        EmbeddingMath.Normalize(embedding);
    }

    public async ValueTask DisposeAsync()
    {
        if (_isDisposed)
            return;

        _isDisposed = true;

        await UnloadModelAsync();

        _loadLock.Dispose();
        _embedLock.Dispose();
    }
}
```

### File: `src/SeniorIntern.Services/Embedding/EmbeddingMath.cs`

```csharp
using System;
using System.Numerics;

namespace SeniorIntern.Services.Embedding;

/// <summary>
/// Shared mathematical operations for embeddings.
/// Uses SIMD acceleration when available.
/// </summary>
public static class EmbeddingMath
{
    /// <summary>
    /// Calculate cosine similarity between two vectors.
    /// </summary>
    public static float CosineSimilarity(ReadOnlySpan<float> a, ReadOnlySpan<float> b)
    {
        if (a.Length != b.Length)
            throw new ArgumentException("Vectors must have the same dimension");

        if (a.Length == 0)
            return 0f;

        float dot = 0f, normA = 0f, normB = 0f;

        if (Vector.IsHardwareAccelerated && a.Length >= Vector<float>.Count)
        {
            var vectorSize = Vector<float>.Count;
            var dotVec = Vector<float>.Zero;
            var normAVec = Vector<float>.Zero;
            var normBVec = Vector<float>.Zero;

            int i = 0;
            for (; i <= a.Length - vectorSize; i += vectorSize)
            {
                var va = new Vector<float>(a.Slice(i, vectorSize));
                var vb = new Vector<float>(b.Slice(i, vectorSize));

                dotVec += va * vb;
                normAVec += va * va;
                normBVec += vb * vb;
            }

            // Sum vector elements
            for (int j = 0; j < vectorSize; j++)
            {
                dot += dotVec[j];
                normA += normAVec[j];
                normB += normBVec[j];
            }

            // Handle remaining elements
            for (; i < a.Length; i++)
            {
                dot += a[i] * b[i];
                normA += a[i] * a[i];
                normB += b[i] * b[i];
            }
        }
        else
        {
            for (int i = 0; i < a.Length; i++)
            {
                dot += a[i] * b[i];
                normA += a[i] * a[i];
                normB += b[i] * b[i];
            }
        }

        var magnitude = MathF.Sqrt(normA) * MathF.Sqrt(normB);
        return magnitude > 0 ? dot / magnitude : 0f;
    }

    /// <summary>
    /// Normalize a vector to unit length in place.
    /// </summary>
    public static void Normalize(Span<float> vector)
    {
        if (vector.Length == 0)
            return;

        float sumSquares = 0f;

        if (Vector.IsHardwareAccelerated && vector.Length >= Vector<float>.Count)
        {
            var vectorSize = Vector<float>.Count;
            var sumVec = Vector<float>.Zero;
            int i = 0;

            for (; i <= vector.Length - vectorSize; i += vectorSize)
            {
                var v = new Vector<float>(vector.Slice(i, vectorSize));
                sumVec += v * v;
            }

            for (int j = 0; j < vectorSize; j++)
                sumSquares += sumVec[j];

            for (; i < vector.Length; i++)
                sumSquares += vector[i] * vector[i];
        }
        else
        {
            foreach (var val in vector)
                sumSquares += val * val;
        }

        var magnitude = MathF.Sqrt(sumSquares);
        if (magnitude <= 0)
            return;

        // Normalize in place using SIMD if possible
        if (Vector.IsHardwareAccelerated && vector.Length >= Vector<float>.Count)
        {
            var vectorSize = Vector<float>.Count;
            var magVec = new Vector<float>(magnitude);
            int i = 0;

            for (; i <= vector.Length - vectorSize; i += vectorSize)
            {
                var v = new Vector<float>(vector.Slice(i, vectorSize));
                (v / magVec).CopyTo(vector.Slice(i, vectorSize));
            }

            for (; i < vector.Length; i++)
                vector[i] /= magnitude;
        }
        else
        {
            for (int i = 0; i < vector.Length; i++)
                vector[i] /= magnitude;
        }
    }

    /// <summary>
    /// Calculate Euclidean distance between two vectors.
    /// </summary>
    public static float EuclideanDistance(ReadOnlySpan<float> a, ReadOnlySpan<float> b)
    {
        if (a.Length != b.Length)
            throw new ArgumentException("Vectors must have the same dimension");

        float sumSquares = 0f;

        for (int i = 0; i < a.Length; i++)
        {
            var diff = a[i] - b[i];
            sumSquares += diff * diff;
        }

        return MathF.Sqrt(sumSquares);
    }

    /// <summary>
    /// Calculate dot product of two vectors.
    /// </summary>
    public static float DotProduct(ReadOnlySpan<float> a, ReadOnlySpan<float> b)
    {
        if (a.Length != b.Length)
            throw new ArgumentException("Vectors must have the same dimension");

        float dot = 0f;

        if (Vector.IsHardwareAccelerated && a.Length >= Vector<float>.Count)
        {
            var vectorSize = Vector<float>.Count;
            var dotVec = Vector<float>.Zero;
            int i = 0;

            for (; i <= a.Length - vectorSize; i += vectorSize)
            {
                var va = new Vector<float>(a.Slice(i, vectorSize));
                var vb = new Vector<float>(b.Slice(i, vectorSize));
                dotVec += va * vb;
            }

            for (int j = 0; j < vectorSize; j++)
                dot += dotVec[j];

            for (; i < a.Length; i++)
                dot += a[i] * b[i];
        }
        else
        {
            for (int i = 0; i < a.Length; i++)
                dot += a[i] * b[i];
        }

        return dot;
    }
}
```

### File: `src/SeniorIntern.Core/Interfaces/ITokenizerService.cs`

```csharp
using System.Threading;
using System.Threading.Tasks;

namespace SeniorIntern.Core.Interfaces;

/// <summary>
/// Service for text tokenization.
/// </summary>
public interface ITokenizerService
{
    /// <summary>
    /// Whether a tokenizer is currently loaded.
    /// </summary>
    bool IsLoaded { get; }

    /// <summary>
    /// Load a tokenizer from a JSON file.
    /// </summary>
    Task LoadTokenizerAsync(string path, CancellationToken ct = default);

    /// <summary>
    /// Tokenize text and return token IDs.
    /// </summary>
    TokenizedText Encode(string text, int maxLength);

    /// <summary>
    /// Decode token IDs back to text.
    /// </summary>
    string Decode(long[] tokenIds);

    /// <summary>
    /// Count tokens in text without full encoding.
    /// </summary>
    int CountTokens(string text);
}

/// <summary>
/// Result of text tokenization.
/// </summary>
public sealed class TokenizedText
{
    /// <summary>
    /// Token IDs.
    /// </summary>
    public long[] InputIds { get; init; } = System.Array.Empty<long>();

    /// <summary>
    /// Attention mask (1 for real tokens, 0 for padding).
    /// </summary>
    public long[] AttentionMask { get; init; } = System.Array.Empty<long>();

    /// <summary>
    /// Token type IDs (for multi-sequence inputs).
    /// </summary>
    public long[]? TokenTypeIds { get; init; }
}
```

### File: `src/SeniorIntern.Services/Embedding/HuggingFaceTokenizerService.cs`

```csharp
using System;
using System.Collections.Generic;
using System.IO;
using System.Text.Json;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.Extensions.Logging;
using SeniorIntern.Core.Interfaces;

namespace SeniorIntern.Services.Embedding;

/// <summary>
/// Tokenizer service for HuggingFace-style tokenizer.json files.
/// </summary>
public sealed class HuggingFaceTokenizerService : ITokenizerService
{
    private readonly ILogger<HuggingFaceTokenizerService> _logger;

    private Dictionary<string, int>? _vocab;
    private Dictionary<int, string>? _reverseVocab;
    private int _padTokenId;
    private int _clsTokenId;
    private int _sepTokenId;
    private int _unkTokenId;

    public bool IsLoaded => _vocab is not null;

    public HuggingFaceTokenizerService(ILogger<HuggingFaceTokenizerService> logger)
    {
        _logger = logger;

        // Default special token IDs (common for BERT-style models)
        _padTokenId = 0;
        _clsTokenId = 101;
        _sepTokenId = 102;
        _unkTokenId = 100;
    }

    public async Task LoadTokenizerAsync(string path, CancellationToken ct = default)
    {
        _logger.LogInformation("Loading tokenizer from {Path}", path);

        var json = await File.ReadAllTextAsync(path, ct);
        var doc = JsonDocument.Parse(json);

        // Parse vocabulary
        _vocab = new Dictionary<string, int>();
        _reverseVocab = new Dictionary<int, string>();

        if (doc.RootElement.TryGetProperty("model", out var model) &&
            model.TryGetProperty("vocab", out var vocab))
        {
            foreach (var item in vocab.EnumerateObject())
            {
                var id = item.Value.GetInt32();
                _vocab[item.Name] = id;
                _reverseVocab[id] = item.Name;
            }
        }

        // Parse special tokens
        if (doc.RootElement.TryGetProperty("added_tokens", out var addedTokens))
        {
            foreach (var token in addedTokens.EnumerateArray())
            {
                var content = token.GetProperty("content").GetString();
                var id = token.GetProperty("id").GetInt32();

                if (content == "[PAD]") _padTokenId = id;
                else if (content == "[CLS]") _clsTokenId = id;
                else if (content == "[SEP]") _sepTokenId = id;
                else if (content == "[UNK]") _unkTokenId = id;
            }
        }

        _logger.LogInformation(
            "Loaded tokenizer with {Count} tokens",
            _vocab.Count);
    }

    public TokenizedText Encode(string text, int maxLength)
    {
        if (_vocab is null)
            throw new InvalidOperationException("Tokenizer not loaded");

        // Simple WordPiece-style tokenization
        var tokens = new List<long> { _clsTokenId };

        var words = text.Split(' ', StringSplitOptions.RemoveEmptyEntries);
        foreach (var word in words)
        {
            if (tokens.Count >= maxLength - 1)
                break;

            var normalizedWord = word.ToLowerInvariant();

            // Try to find exact match
            if (_vocab.TryGetValue(normalizedWord, out var tokenId))
            {
                tokens.Add(tokenId);
            }
            else
            {
                // Try WordPiece subword tokenization
                var subwords = TokenizeWordPiece(normalizedWord, maxLength - tokens.Count - 1);
                tokens.AddRange(subwords);
            }
        }

        tokens.Add(_sepTokenId);

        // Create attention mask
        var attentionMask = new long[tokens.Count];
        Array.Fill(attentionMask, 1L);

        return new TokenizedText
        {
            InputIds = tokens.ToArray(),
            AttentionMask = attentionMask
        };
    }

    private IEnumerable<long> TokenizeWordPiece(string word, int maxSubwords)
    {
        var subwords = new List<long>();
        var remaining = word;
        var isFirst = true;

        while (remaining.Length > 0 && subwords.Count < maxSubwords)
        {
            var found = false;

            // Try progressively shorter substrings
            for (int len = remaining.Length; len > 0; len--)
            {
                var subword = remaining[..len];
                if (!isFirst)
                    subword = "##" + subword;

                if (_vocab!.TryGetValue(subword, out var tokenId))
                {
                    subwords.Add(tokenId);
                    remaining = remaining[len..];
                    found = true;
                    isFirst = false;
                    break;
                }
            }

            if (!found)
            {
                // Unknown character - use UNK token
                subwords.Add(_unkTokenId);
                remaining = remaining.Length > 1 ? remaining[1..] : "";
            }
        }

        return subwords;
    }

    public string Decode(long[] tokenIds)
    {
        if (_reverseVocab is null)
            throw new InvalidOperationException("Tokenizer not loaded");

        var tokens = new List<string>();

        foreach (var id in tokenIds)
        {
            if (_reverseVocab.TryGetValue((int)id, out var token))
            {
                // Skip special tokens
                if (token is "[CLS]" or "[SEP]" or "[PAD]")
                    continue;

                // Handle WordPiece continuation
                if (token.StartsWith("##"))
                    token = token[2..];

                tokens.Add(token);
            }
        }

        return string.Join(" ", tokens);
    }

    public int CountTokens(string text)
    {
        if (_vocab is null)
            return text.Split(' ', StringSplitOptions.RemoveEmptyEntries).Length;

        var encoded = Encode(text, int.MaxValue);
        return encoded.InputIds.Length;
    }
}
```

### Modification: `src/SeniorIntern.Services/SeniorIntern.Services.csproj`

Add ONNX Runtime package references:

```xml
<ItemGroup>
  <!-- Existing packages... -->
  <PackageReference Include="Microsoft.ML.OnnxRuntime" Version="1.17.0" />
  <PackageReference Include="Microsoft.ML.OnnxRuntime.Managed" Version="1.17.0" />
  <!-- GPU support (optional, platform-specific) -->
  <PackageReference Include="Microsoft.ML.OnnxRuntime.Gpu" Version="1.17.0" Condition="'$(RuntimeIdentifier)' == 'win-x64' OR '$(RuntimeIdentifier)' == 'linux-x64'" />
</ItemGroup>
```

### Acceptance Criteria (v0.7.1c)
- [ ] OnnxEmbeddingService loads ONNX models
- [ ] Tokenizer service parses HuggingFace tokenizer.json
- [ ] Batch embedding with padding works correctly
- [ ] All pooling types implemented (Mean, CLS, Last, Max)
- [ ] GPU acceleration via CUDA/CoreML when available
- [ ] EmbeddingMath provides SIMD-optimized operations
- [ ] Proper resource cleanup on dispose

---

## v0.7.1d: Text Chunk Models

### Objective
Define comprehensive models for text chunks with metadata for code understanding and retrieval.

### File: `src/SeniorIntern.Core/Models/TextChunk.cs`

```csharp
using System;
using System.Collections.Generic;

namespace SeniorIntern.Core.Models;

/// <summary>
/// A chunk of text/code with metadata for embedding and retrieval.
/// </summary>
public sealed class TextChunk
{
    /// <summary>
    /// Unique identifier for this chunk.
    /// </summary>
    public Guid Id { get; init; } = Guid.NewGuid();

    /// <summary>
    /// The actual text content of the chunk.
    /// </summary>
    public string Content { get; init; } = string.Empty;

    /// <summary>
    /// Source file path (relative to workspace root).
    /// </summary>
    public string FilePath { get; init; } = string.Empty;

    /// <summary>
    /// Absolute file path (for file operations).
    /// </summary>
    public string? AbsolutePath { get; init; }

    /// <summary>
    /// Starting line number in the source file (1-based).
    /// </summary>
    public int StartLine { get; init; }

    /// <summary>
    /// Ending line number in the source file (1-based, inclusive).
    /// </summary>
    public int EndLine { get; init; }

    /// <summary>
    /// Character offset from the start of the file.
    /// </summary>
    public int StartOffset { get; init; }

    /// <summary>
    /// End character offset (exclusive).
    /// </summary>
    public int EndOffset { get; init; }

    /// <summary>
    /// Type of content in this chunk.
    /// </summary>
    public ChunkType Type { get; init; } = ChunkType.Code;

    /// <summary>
    /// Programming language of the content (if applicable).
    /// </summary>
    public string? Language { get; init; }

    /// <summary>
    /// Symbol name if this chunk represents a specific code symbol.
    /// </summary>
    public string? SymbolName { get; init; }

    /// <summary>
    /// Type of symbol (class, method, function, etc.).
    /// </summary>
    public SymbolType? SymbolType { get; init; }

    /// <summary>
    /// Fully qualified symbol name.
    /// </summary>
    public string? QualifiedName { get; init; }

    /// <summary>
    /// Parent symbol name (e.g., class containing a method).
    /// </summary>
    public string? ParentSymbol { get; init; }

    /// <summary>
    /// Namespace or module containing the symbol.
    /// </summary>
    public string? Namespace { get; init; }

    /// <summary>
    /// Approximate token count for this chunk.
    /// </summary>
    public int TokenCount { get; init; }

    /// <summary>
    /// Sequence number within the file (for ordering overlapping chunks).
    /// </summary>
    public int SequenceNumber { get; init; }

    /// <summary>
    /// Whether this chunk overlaps with the previous chunk.
    /// </summary>
    public bool HasOverlap { get; init; }

    /// <summary>
    /// Summary or docstring for this chunk (if available).
    /// </summary>
    public string? Summary { get; init; }

    /// <summary>
    /// Tags for additional categorization.
    /// </summary>
    public IReadOnlyList<string>? Tags { get; init; }

    /// <summary>
    /// Additional metadata as key-value pairs.
    /// </summary>
    public IReadOnlyDictionary<string, string>? Metadata { get; init; }

    /// <summary>
    /// Number of lines in this chunk.
    /// </summary>
    public int LineCount => EndLine - StartLine + 1;

    /// <summary>
    /// Character length of the content.
    /// </summary>
    public int Length => Content.Length;

    /// <summary>
    /// Create a location string for display (file:line).
    /// </summary>
    public string LocationString => StartLine == EndLine
        ? $"{FilePath}:{StartLine}"
        : $"{FilePath}:{StartLine}-{EndLine}";

    /// <summary>
    /// Create a copy with updated content (for preprocessing).
    /// </summary>
    public TextChunk WithContent(string newContent) => new()
    {
        Id = Id,
        Content = newContent,
        FilePath = FilePath,
        AbsolutePath = AbsolutePath,
        StartLine = StartLine,
        EndLine = EndLine,
        StartOffset = StartOffset,
        EndOffset = EndOffset,
        Type = Type,
        Language = Language,
        SymbolName = SymbolName,
        SymbolType = SymbolType,
        QualifiedName = QualifiedName,
        ParentSymbol = ParentSymbol,
        Namespace = Namespace,
        TokenCount = TokenCount,
        SequenceNumber = SequenceNumber,
        HasOverlap = HasOverlap,
        Summary = Summary,
        Tags = Tags,
        Metadata = Metadata
    };
}
```

### File: `src/SeniorIntern.Core/Models/ChunkType.cs`

```csharp
namespace SeniorIntern.Core.Models;

/// <summary>
/// Types of content that can be chunked.
/// </summary>
public enum ChunkType
{
    /// <summary>
    /// Executable code (functions, methods, etc.).
    /// </summary>
    Code,

    /// <summary>
    /// Code comments (single-line or block).
    /// </summary>
    Comment,

    /// <summary>
    /// Documentation (docstrings, XML docs, JSDoc, etc.).
    /// </summary>
    Documentation,

    /// <summary>
    /// Markdown content.
    /// </summary>
    Markdown,

    /// <summary>
    /// Plain text content.
    /// </summary>
    PlainText,

    /// <summary>
    /// Configuration files (JSON, YAML, XML, etc.).
    /// </summary>
    Config,

    /// <summary>
    /// Mixed content (code with inline comments).
    /// </summary>
    Mixed,

    /// <summary>
    /// Import/using statements.
    /// </summary>
    Import,

    /// <summary>
    /// Type definitions (classes, interfaces, etc.).
    /// </summary>
    TypeDefinition,

    /// <summary>
    /// Test code.
    /// </summary>
    Test
}

/// <summary>
/// Types of code symbols.
/// </summary>
public enum SymbolType
{
    /// <summary>
    /// Namespace declaration.
    /// </summary>
    Namespace,

    /// <summary>
    /// Class definition.
    /// </summary>
    Class,

    /// <summary>
    /// Struct/record definition.
    /// </summary>
    Struct,

    /// <summary>
    /// Interface definition.
    /// </summary>
    Interface,

    /// <summary>
    /// Enum definition.
    /// </summary>
    Enum,

    /// <summary>
    /// Method definition (in a class/struct).
    /// </summary>
    Method,

    /// <summary>
    /// Standalone function.
    /// </summary>
    Function,

    /// <summary>
    /// Property definition.
    /// </summary>
    Property,

    /// <summary>
    /// Field definition.
    /// </summary>
    Field,

    /// <summary>
    /// Constructor.
    /// </summary>
    Constructor,

    /// <summary>
    /// Destructor/finalizer.
    /// </summary>
    Destructor,

    /// <summary>
    /// Event definition.
    /// </summary>
    Event,

    /// <summary>
    /// Delegate type.
    /// </summary>
    Delegate,

    /// <summary>
    /// Operator overload.
    /// </summary>
    Operator,

    /// <summary>
    /// Indexer.
    /// </summary>
    Indexer,

    /// <summary>
    /// Import/using statement.
    /// </summary>
    Import,

    /// <summary>
    /// Constant definition.
    /// </summary>
    Constant,

    /// <summary>
    /// Type alias.
    /// </summary>
    TypeAlias,

    /// <summary>
    /// Module (JS/TS/Python).
    /// </summary>
    Module,

    /// <summary>
    /// Lambda/closure.
    /// </summary>
    Lambda,

    /// <summary>
    /// Unknown or unclassified symbol.
    /// </summary>
    Other
}
```

### File: `src/SeniorIntern.Core/Models/ChunkingOptions.cs`

```csharp
using System.Collections.Generic;

namespace SeniorIntern.Core.Models;

/// <summary>
/// Configuration options for text chunking.
/// </summary>
public sealed class ChunkingOptions
{
    /// <summary>
    /// Target chunk size in tokens (approximate).
    /// The chunker will try to create chunks close to this size.
    /// </summary>
    public int TargetChunkSize { get; init; } = 512;

    /// <summary>
    /// Number of tokens to overlap between consecutive chunks.
    /// Overlap helps maintain context across chunk boundaries.
    /// </summary>
    public int ChunkOverlap { get; init; } = 64;

    /// <summary>
    /// Minimum chunk size in tokens.
    /// Chunks smaller than this will be merged or discarded.
    /// </summary>
    public int MinChunkSize { get; init; } = 50;

    /// <summary>
    /// Maximum chunk size in tokens (hard limit).
    /// Chunks will be split if they exceed this size.
    /// </summary>
    public int MaxChunkSize { get; init; } = 1024;

    /// <summary>
    /// Whether to include file path in chunk metadata.
    /// </summary>
    public bool IncludeFilePath { get; init; } = true;

    /// <summary>
    /// Whether to include line numbers in chunk metadata.
    /// </summary>
    public bool IncludeLineNumbers { get; init; } = true;

    /// <summary>
    /// Whether to use semantic chunking (by code structure) when possible.
    /// Falls back to line-based chunking for unknown languages.
    /// </summary>
    public bool UseSemanticChunking { get; init; } = true;

    /// <summary>
    /// Whether to extract and index symbol names separately.
    /// </summary>
    public bool ExtractSymbols { get; init; } = true;

    /// <summary>
    /// Whether to preserve full method/function bodies in single chunks.
    /// If false, large functions will be split.
    /// </summary>
    public bool PreserveFunctionBodies { get; init; } = true;

    /// <summary>
    /// Whether to include documentation/comments with code.
    /// </summary>
    public bool IncludeDocumentation { get; init; } = true;

    /// <summary>
    /// Whether to create separate chunks for comments/docs.
    /// </summary>
    public bool SeparateDocumentationChunks { get; init; } = false;

    /// <summary>
    /// Preprocessing steps to apply to text before chunking.
    /// </summary>
    public ChunkPreprocessing Preprocessing { get; init; } = ChunkPreprocessing.Default;

    /// <summary>
    /// File patterns to include (if using glob patterns).
    /// </summary>
    public IReadOnlyList<string>? IncludePatterns { get; init; }

    /// <summary>
    /// File patterns to exclude.
    /// </summary>
    public IReadOnlyList<string>? ExcludePatterns { get; init; }
}

/// <summary>
/// Text preprocessing options.
/// </summary>
public sealed class ChunkPreprocessing
{
    /// <summary>
    /// Default preprocessing settings.
    /// </summary>
    public static ChunkPreprocessing Default { get; } = new()
    {
        NormalizeWhitespace = true,
        RemoveEmptyLines = false,
        CollapseMultipleNewlines = true,
        TrimLines = true,
        RemoveLicenseHeaders = true,
        MaxConsecutiveEmptyLines = 2
    };

    /// <summary>
    /// Whether to normalize whitespace (tabs to spaces, etc.).
    /// </summary>
    public bool NormalizeWhitespace { get; init; }

    /// <summary>
    /// Whether to remove empty lines entirely.
    /// </summary>
    public bool RemoveEmptyLines { get; init; }

    /// <summary>
    /// Whether to collapse multiple consecutive newlines.
    /// </summary>
    public bool CollapseMultipleNewlines { get; init; }

    /// <summary>
    /// Whether to trim leading/trailing whitespace from lines.
    /// </summary>
    public bool TrimLines { get; init; }

    /// <summary>
    /// Whether to remove license/copyright headers.
    /// </summary>
    public bool RemoveLicenseHeaders { get; init; }

    /// <summary>
    /// Maximum consecutive empty lines to allow.
    /// </summary>
    public int MaxConsecutiveEmptyLines { get; init; }
}
```

### Acceptance Criteria (v0.7.1d)
- [ ] TextChunk model contains all necessary properties
- [ ] ChunkType enum covers all content types
- [ ] SymbolType enum covers all code symbol types
- [ ] ChunkingOptions provides comprehensive configuration
- [ ] Preprocessing options defined
- [ ] Helper properties (LineCount, LocationString) work correctly

---

## v0.7.1e: Chunking Service Interface

### Objective
Define the interfaces for the chunking service and chunking strategies.

### File: `src/SeniorIntern.Core/Interfaces/IChunkingService.cs`

```csharp
using System.Collections.Generic;
using System.Threading;
using System.Threading.Tasks;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Core.Interfaces;

/// <summary>
/// Service for chunking text and code into embeddable segments.
/// </summary>
public interface IChunkingService
{
    /// <summary>
    /// Chunk a document into segments based on content type and options.
    /// </summary>
    /// <param name="content">The document content to chunk.</param>
    /// <param name="filePath">Path to the source file (for language detection).</param>
    /// <param name="options">Chunking configuration.</param>
    /// <returns>List of text chunks with metadata.</returns>
    IReadOnlyList<TextChunk> ChunkDocument(
        string content,
        string filePath,
        ChunkingOptions options);

    /// <summary>
    /// Chunk a document asynchronously (for large files).
    /// </summary>
    Task<IReadOnlyList<TextChunk>> ChunkDocumentAsync(
        string content,
        string filePath,
        ChunkingOptions options,
        CancellationToken ct = default);

    /// <summary>
    /// Get the appropriate chunking strategy for a file type.
    /// </summary>
    /// <param name="filePath">File path or extension.</param>
    /// <returns>The chunking strategy to use.</returns>
    IChunkingStrategy GetStrategy(string filePath);

    /// <summary>
    /// Get the detected language for a file.
    /// </summary>
    /// <param name="filePath">File path or extension.</param>
    /// <returns>Language identifier (e.g., "csharp", "python").</returns>
    string? DetectLanguage(string filePath);

    /// <summary>
    /// Check if a file type is supported for chunking.
    /// </summary>
    bool IsSupported(string filePath);

    /// <summary>
    /// Preprocess text according to options.
    /// </summary>
    string Preprocess(string content, ChunkPreprocessing options);

    /// <summary>
    /// Estimate the token count for text.
    /// </summary>
    int EstimateTokenCount(string text);
}
```

### File: `src/SeniorIntern.Core/Interfaces/IChunkingStrategy.cs`

```csharp
using System.Collections.Generic;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Core.Interfaces;

/// <summary>
/// Strategy for chunking specific types of content.
/// </summary>
public interface IChunkingStrategy
{
    /// <summary>
    /// Name of this strategy (for logging/debugging).
    /// </summary>
    string Name { get; }

    /// <summary>
    /// File extensions this strategy handles.
    /// </summary>
    IReadOnlySet<string> SupportedExtensions { get; }

    /// <summary>
    /// Priority when multiple strategies could handle a file.
    /// Higher values = higher priority.
    /// </summary>
    int Priority { get; }

    /// <summary>
    /// Check if this strategy can handle the given file.
    /// </summary>
    bool CanHandle(string filePath);

    /// <summary>
    /// Chunk content using this strategy.
    /// </summary>
    /// <param name="content">The content to chunk.</param>
    /// <param name="filePath">Source file path.</param>
    /// <param name="options">Chunking options.</param>
    /// <returns>List of text chunks.</returns>
    IReadOnlyList<TextChunk> Chunk(
        string content,
        string filePath,
        ChunkingOptions options);
}

/// <summary>
/// Strategy that supports code structure analysis.
/// </summary>
public interface ICodeChunkingStrategy : IChunkingStrategy
{
    /// <summary>
    /// Extract code symbols from content.
    /// </summary>
    IReadOnlyList<CodeSymbol> ExtractSymbols(string content, string filePath);
}

/// <summary>
/// Represents a code symbol extracted from source.
/// </summary>
public sealed class CodeSymbol
{
    /// <summary>
    /// Symbol name.
    /// </summary>
    public string Name { get; init; } = string.Empty;

    /// <summary>
    /// Type of symbol.
    /// </summary>
    public SymbolType Type { get; init; }

    /// <summary>
    /// Fully qualified name.
    /// </summary>
    public string? QualifiedName { get; init; }

    /// <summary>
    /// Parent symbol (containing class/namespace).
    /// </summary>
    public string? Parent { get; init; }

    /// <summary>
    /// Namespace containing this symbol.
    /// </summary>
    public string? Namespace { get; init; }

    /// <summary>
    /// Start line (1-based).
    /// </summary>
    public int StartLine { get; init; }

    /// <summary>
    /// End line (1-based).
    /// </summary>
    public int EndLine { get; init; }

    /// <summary>
    /// Character offset.
    /// </summary>
    public int StartOffset { get; init; }

    /// <summary>
    /// Character length.
    /// </summary>
    public int Length { get; init; }

    /// <summary>
    /// Documentation/docstring if available.
    /// </summary>
    public string? Documentation { get; init; }

    /// <summary>
    /// Access modifier (public, private, etc.).
    /// </summary>
    public string? AccessModifier { get; init; }

    /// <summary>
    /// Return type (for methods/functions).
    /// </summary>
    public string? ReturnType { get; init; }

    /// <summary>
    /// Parameters (for methods/functions).
    /// </summary>
    public IReadOnlyList<ParameterInfo>? Parameters { get; init; }
}

/// <summary>
/// Parameter information for methods/functions.
/// </summary>
public sealed class ParameterInfo
{
    public string Name { get; init; } = string.Empty;
    public string? Type { get; init; }
    public string? DefaultValue { get; init; }
}
```

### File: `src/SeniorIntern.Services/Chunking/ChunkingService.cs`

```csharp
using System;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Text.RegularExpressions;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.Extensions.Logging;
using SeniorIntern.Core.Interfaces;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Services.Chunking;

/// <summary>
/// Main chunking service that orchestrates different strategies.
/// </summary>
public sealed class ChunkingService : IChunkingService
{
    private readonly ILogger<ChunkingService> _logger;
    private readonly IEnumerable<IChunkingStrategy> _strategies;
    private readonly ITokenizerService? _tokenizer;

    private static readonly Dictionary<string, string> _extensionToLanguage = new(StringComparer.OrdinalIgnoreCase)
    {
        // .NET
        [".cs"] = "csharp",
        [".fs"] = "fsharp",
        [".vb"] = "vb",

        // JavaScript/TypeScript
        [".ts"] = "typescript",
        [".tsx"] = "typescript",
        [".js"] = "javascript",
        [".jsx"] = "javascript",
        [".mjs"] = "javascript",

        // Python
        [".py"] = "python",
        [".pyi"] = "python",

        // JVM
        [".java"] = "java",
        [".kt"] = "kotlin",
        [".scala"] = "scala",

        // Systems
        [".go"] = "go",
        [".rs"] = "rust",
        [".c"] = "c",
        [".h"] = "c",
        [".cpp"] = "cpp",
        [".hpp"] = "cpp",
        [".cc"] = "cpp",

        // Other languages
        [".swift"] = "swift",
        [".rb"] = "ruby",
        [".php"] = "php",
        [".lua"] = "lua",
        [".r"] = "r",
        [".jl"] = "julia",

        // Markup/Config
        [".md"] = "markdown",
        [".markdown"] = "markdown",
        [".json"] = "json",
        [".yaml"] = "yaml",
        [".yml"] = "yaml",
        [".xml"] = "xml",
        [".html"] = "html",
        [".htm"] = "html",
        [".css"] = "css",
        [".scss"] = "scss",
        [".sass"] = "sass",

        // Shell
        [".sh"] = "bash",
        [".bash"] = "bash",
        [".zsh"] = "zsh",
        [".ps1"] = "powershell",

        // Other
        [".sql"] = "sql",
        [".txt"] = "text",
        [".rst"] = "rst",
    };

    public ChunkingService(
        ILogger<ChunkingService> logger,
        IEnumerable<IChunkingStrategy> strategies,
        ITokenizerService? tokenizer = null)
    {
        _logger = logger;
        _strategies = strategies.OrderByDescending(s => s.Priority).ToList();
        _tokenizer = tokenizer;
    }

    public IReadOnlyList<TextChunk> ChunkDocument(
        string content,
        string filePath,
        ChunkingOptions options)
    {
        if (string.IsNullOrEmpty(content))
            return Array.Empty<TextChunk>();

        // Preprocess content
        var processed = Preprocess(content, options.Preprocessing);

        // Get appropriate strategy
        var strategy = GetStrategy(filePath);

        _logger.LogDebug(
            "Chunking {File} with strategy {Strategy}",
            filePath, strategy.Name);

        // Perform chunking
        var chunks = strategy.Chunk(processed, filePath, options);

        _logger.LogDebug(
            "Created {Count} chunks from {File}",
            chunks.Count, filePath);

        return chunks;
    }

    public Task<IReadOnlyList<TextChunk>> ChunkDocumentAsync(
        string content,
        string filePath,
        ChunkingOptions options,
        CancellationToken ct = default)
    {
        return Task.Run(() => ChunkDocument(content, filePath, options), ct);
    }

    public IChunkingStrategy GetStrategy(string filePath)
    {
        foreach (var strategy in _strategies)
        {
            if (strategy.CanHandle(filePath))
                return strategy;
        }

        // Default to plain text strategy
        return _strategies.FirstOrDefault(s => s.Name == "PlainText")
            ?? throw new InvalidOperationException("No default chunking strategy found");
    }

    public string? DetectLanguage(string filePath)
    {
        var extension = Path.GetExtension(filePath);
        return _extensionToLanguage.TryGetValue(extension, out var language)
            ? language
            : null;
    }

    public bool IsSupported(string filePath)
    {
        return _strategies.Any(s => s.CanHandle(filePath));
    }

    public string Preprocess(string content, ChunkPreprocessing options)
    {
        if (options is null)
            return content;

        var result = content;

        // Normalize whitespace
        if (options.NormalizeWhitespace)
        {
            result = result.Replace("\t", "    "); // Tabs to 4 spaces
            result = result.Replace("\r\n", "\n"); // Normalize line endings
        }

        // Remove license headers
        if (options.RemoveLicenseHeaders)
        {
            result = RemoveLicenseHeader(result);
        }

        // Trim lines
        if (options.TrimLines)
        {
            var lines = result.Split('\n');
            result = string.Join('\n', lines.Select(l => l.TrimEnd()));
        }

        // Collapse multiple newlines
        if (options.CollapseMultipleNewlines)
        {
            var maxNewlines = options.MaxConsecutiveEmptyLines + 1;
            var pattern = $"\n{{{maxNewlines + 1},}}";
            result = Regex.Replace(result, pattern, new string('\n', maxNewlines));
        }

        // Remove empty lines
        if (options.RemoveEmptyLines)
        {
            var lines = result.Split('\n');
            result = string.Join('\n', lines.Where(l => !string.IsNullOrWhiteSpace(l)));
        }

        return result;
    }

    public int EstimateTokenCount(string text)
    {
        if (_tokenizer?.IsLoaded == true)
        {
            return _tokenizer.CountTokens(text);
        }

        // Rough estimate: ~4 characters per token for code
        // This is a common heuristic for code/technical text
        return text.Length / 4;
    }

    private static string RemoveLicenseHeader(string content)
    {
        // Common license header patterns
        var patterns = new[]
        {
            @"^/\*[\s\S]*?(?:license|copyright|mit|apache|gpl|bsd)[\s\S]*?\*/\s*",
            @"^//[^\n]*(?:license|copyright)[^\n]*\n(?://[^\n]*\n)*",
            @"^#[^\n]*(?:license|copyright)[^\n]*\n(?:#[^\n]*\n)*",
        };

        foreach (var pattern in patterns)
        {
            var match = Regex.Match(content, pattern, RegexOptions.IgnoreCase);
            if (match.Success && match.Index == 0)
            {
                return content[match.Length..].TrimStart('\n');
            }
        }

        return content;
    }
}
```

### Acceptance Criteria (v0.7.1e)
- [ ] IChunkingService interface defined with all methods
- [ ] IChunkingStrategy interface supports extension
- [ ] ICodeChunkingStrategy adds symbol extraction
- [ ] CodeSymbol model captures code structure
- [ ] ChunkingService orchestrates strategies
- [ ] Language detection from file extensions works
- [ ] Preprocessing operations implemented
- [ ] Token count estimation available

---

## v0.7.1f: Code-Aware Chunking Strategy

### Objective
Implement a chunking strategy that respects code structure using regex-based symbol extraction.

### File: `src/SeniorIntern.Services/Chunking/CodeAwareChunkingStrategy.cs`

```csharp
using System;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Text;
using System.Text.RegularExpressions;
using Microsoft.Extensions.Logging;
using SeniorIntern.Core.Interfaces;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Services.Chunking;

/// <summary>
/// Chunking strategy that respects code structure (classes, methods, functions).
/// Uses regex-based parsing for language-agnostic symbol detection.
/// </summary>
public class CodeAwareChunkingStrategy : ICodeChunkingStrategy
{
    protected readonly ILogger _logger;
    protected readonly IChunkingService _chunkingService;

    private static readonly HashSet<string> _supportedExtensions = new(StringComparer.OrdinalIgnoreCase)
    {
        ".cs", ".fs", ".vb",           // .NET
        ".ts", ".tsx", ".js", ".jsx",  // JavaScript/TypeScript
        ".py",                          // Python
        ".java", ".kt",                 // JVM
        ".go",                          // Go
        ".rs",                          // Rust
        ".cpp", ".hpp", ".c", ".h",    // C/C++
        ".swift",                       // Swift
        ".rb",                          // Ruby
        ".php"                          // PHP
    };

    public virtual string Name => "CodeAware";
    public virtual IReadOnlySet<string> SupportedExtensions => _supportedExtensions;
    public virtual int Priority => 100;

    public CodeAwareChunkingStrategy(
        ILogger<CodeAwareChunkingStrategy> logger,
        IChunkingService chunkingService)
    {
        _logger = logger;
        _chunkingService = chunkingService;
    }

    public virtual bool CanHandle(string filePath)
    {
        var extension = Path.GetExtension(filePath);
        return SupportedExtensions.Contains(extension);
    }

    public virtual IReadOnlyList<TextChunk> Chunk(
        string content,
        string filePath,
        ChunkingOptions options)
    {
        var extension = Path.GetExtension(filePath).ToLowerInvariant();
        var language = GetLanguageFromExtension(extension);

        // Try semantic chunking first (by symbols)
        if (options.UseSemanticChunking)
        {
            var semanticChunks = TrySemanticChunk(content, filePath, language, options);
            if (semanticChunks.Count > 0)
                return semanticChunks;
        }

        // Fall back to line-based chunking with overlap
        return LineBasedChunk(content, filePath, language, options);
    }

    public virtual IReadOnlyList<CodeSymbol> ExtractSymbols(string content, string filePath)
    {
        var extension = Path.GetExtension(filePath).ToLowerInvariant();
        var language = GetLanguageFromExtension(extension);

        return ExtractSymbolsForLanguage(content, language);
    }

    protected virtual IReadOnlyList<TextChunk> TrySemanticChunk(
        string content,
        string filePath,
        string language,
        ChunkingOptions options)
    {
        var chunks = new List<TextChunk>();
        var symbols = ExtractSymbolsForLanguage(content, language);

        if (symbols.Count == 0)
            return chunks;

        var sequenceNumber = 0;

        foreach (var symbol in symbols.OrderBy(s => s.StartOffset))
        {
            var symbolContent = content.Substring(symbol.StartOffset, symbol.Length);
            var tokenCount = _chunkingService.EstimateTokenCount(symbolContent);

            if (tokenCount < options.MinChunkSize)
                continue;

            if (tokenCount <= options.MaxChunkSize)
            {
                // Symbol fits in one chunk
                chunks.Add(CreateChunk(
                    symbolContent, filePath, symbol, language, tokenCount, sequenceNumber++));
            }
            else if (options.PreserveFunctionBodies && IsCallableSymbol(symbol.Type))
            {
                // Try to keep function bodies together with smart splitting
                chunks.AddRange(SplitLargeSymbol(
                    content, filePath, symbol, language, options, ref sequenceNumber));
            }
            else
            {
                // Split the symbol into smaller chunks
                chunks.AddRange(SplitLargeSymbol(
                    content, filePath, symbol, language, options, ref sequenceNumber));
            }
        }

        // Handle content between symbols (imports, top-level code, etc.)
        var interSymbolChunks = ChunkInterSymbolContent(
            content, filePath, symbols, language, options, ref sequenceNumber);
        chunks.AddRange(interSymbolChunks);

        return chunks.OrderBy(c => c.StartOffset).ToList();
    }

    protected virtual IReadOnlyList<TextChunk> LineBasedChunk(
        string content,
        string filePath,
        string language,
        ChunkingOptions options)
    {
        var chunks = new List<TextChunk>();
        var lines = content.Split('\n');
        var currentChunk = new StringBuilder();
        var startLine = 1;
        var startOffset = 0;
        var currentTokenCount = 0;
        var sequenceNumber = 0;
        var currentOffset = 0;

        for (int i = 0; i < lines.Length; i++)
        {
            var line = lines[i];
            var lineTokens = _chunkingService.EstimateTokenCount(line);

            if (currentTokenCount + lineTokens > options.TargetChunkSize && currentChunk.Length > 0)
            {
                // Emit current chunk
                chunks.Add(new TextChunk
                {
                    Content = currentChunk.ToString().TrimEnd(),
                    FilePath = filePath,
                    StartLine = startLine,
                    EndLine = i,
                    StartOffset = startOffset,
                    EndOffset = currentOffset,
                    Language = language,
                    Type = ChunkType.Code,
                    TokenCount = currentTokenCount,
                    SequenceNumber = sequenceNumber++,
                    HasOverlap = sequenceNumber > 0
                });

                // Start new chunk with overlap
                var overlapLines = CalculateOverlapLines(lines, i, options.ChunkOverlap);
                currentChunk.Clear();

                foreach (var ol in overlapLines)
                    currentChunk.AppendLine(ol);

                startLine = Math.Max(1, i - overlapLines.Count + 1);
                startOffset = currentOffset - string.Join('\n', overlapLines).Length;
                currentTokenCount = _chunkingService.EstimateTokenCount(currentChunk.ToString());
            }

            currentChunk.AppendLine(line);
            currentOffset += line.Length + 1; // +1 for newline
            currentTokenCount += lineTokens;
        }

        // Don't forget the last chunk
        if (currentChunk.Length > 0 && currentTokenCount >= options.MinChunkSize)
        {
            chunks.Add(new TextChunk
            {
                Content = currentChunk.ToString().TrimEnd(),
                FilePath = filePath,
                StartLine = startLine,
                EndLine = lines.Length,
                StartOffset = startOffset,
                EndOffset = content.Length,
                Language = language,
                Type = ChunkType.Code,
                TokenCount = currentTokenCount,
                SequenceNumber = sequenceNumber
            });
        }

        return chunks;
    }

    protected virtual IReadOnlyList<CodeSymbol> ExtractSymbolsForLanguage(
        string content,
        string language)
    {
        // This is a simplified regex-based extractor
        // Language-specific implementations can override for better accuracy
        var symbols = new List<CodeSymbol>();
        var lines = content.Split('\n');

        // Common patterns for different languages
        var patterns = GetSymbolPatterns(language);

        foreach (var (pattern, symbolType) in patterns)
        {
            var matches = Regex.Matches(content, pattern, RegexOptions.Multiline);

            foreach (Match match in matches)
            {
                var nameGroup = match.Groups["name"];
                if (!nameGroup.Success)
                    continue;

                var startOffset = match.Index;
                var endOffset = FindSymbolEnd(content, startOffset, language);
                var startLine = GetLineNumber(content, startOffset);
                var endLine = GetLineNumber(content, endOffset);

                symbols.Add(new CodeSymbol
                {
                    Name = nameGroup.Value,
                    Type = symbolType,
                    StartLine = startLine,
                    EndLine = endLine,
                    StartOffset = startOffset,
                    Length = endOffset - startOffset
                });
            }
        }

        return symbols;
    }

    protected virtual IEnumerable<(string Pattern, SymbolType Type)> GetSymbolPatterns(string language)
    {
        return language switch
        {
            "csharp" => new[]
            {
                (@"(?:public|private|protected|internal)?\s*(?:static\s+)?(?:partial\s+)?class\s+(?<name>\w+)", SymbolType.Class),
                (@"(?:public|private|protected|internal)?\s*(?:static\s+)?interface\s+(?<name>\w+)", SymbolType.Interface),
                (@"(?:public|private|protected|internal)?\s*(?:static\s+)?(?:async\s+)?(?:\w+(?:<[^>]+>)?)\s+(?<name>\w+)\s*\(", SymbolType.Method),
            },
            "typescript" or "javascript" => new[]
            {
                (@"(?:export\s+)?(?:default\s+)?class\s+(?<name>\w+)", SymbolType.Class),
                (@"(?:export\s+)?(?:async\s+)?function\s+(?<name>\w+)\s*\(", SymbolType.Function),
                (@"(?:const|let|var)\s+(?<name>\w+)\s*=\s*(?:async\s+)?(?:\([^)]*\)|[^=])\s*=>", SymbolType.Function),
                (@"(?<name>\w+)\s*:\s*(?:async\s+)?(?:\([^)]*\)|[^=])\s*=>", SymbolType.Method),
            },
            "python" => new[]
            {
                (@"class\s+(?<name>\w+)\s*(?:\([^)]*\))?\s*:", SymbolType.Class),
                (@"(?:async\s+)?def\s+(?<name>\w+)\s*\(", SymbolType.Function),
            },
            "go" => new[]
            {
                (@"type\s+(?<name>\w+)\s+struct\s*\{", SymbolType.Struct),
                (@"type\s+(?<name>\w+)\s+interface\s*\{", SymbolType.Interface),
                (@"func\s+(?:\([^)]+\)\s+)?(?<name>\w+)\s*\(", SymbolType.Function),
            },
            "rust" => new[]
            {
                (@"(?:pub\s+)?struct\s+(?<name>\w+)", SymbolType.Struct),
                (@"(?:pub\s+)?trait\s+(?<name>\w+)", SymbolType.Interface),
                (@"(?:pub\s+)?(?:async\s+)?fn\s+(?<name>\w+)", SymbolType.Function),
                (@"impl(?:<[^>]+>)?\s+(?<name>\w+)", SymbolType.Class),
            },
            _ => new[]
            {
                (@"(?:class|struct|interface)\s+(?<name>\w+)", SymbolType.Class),
                (@"(?:function|func|def|fn)\s+(?<name>\w+)", SymbolType.Function),
            }
        };
    }

    protected virtual int FindSymbolEnd(string content, int startOffset, string language)
    {
        // Simple brace-matching for most C-style languages
        var braceLanguages = new HashSet<string>
        {
            "csharp", "typescript", "javascript", "java", "go", "rust", "cpp", "c", "swift", "kotlin"
        };

        if (braceLanguages.Contains(language))
        {
            return FindMatchingBrace(content, startOffset);
        }

        // Indentation-based for Python
        if (language == "python")
        {
            return FindPythonBlockEnd(content, startOffset);
        }

        // Default: find next double newline or end of file
        var nextBlank = content.IndexOf("\n\n", startOffset);
        return nextBlank > 0 ? nextBlank : content.Length;
    }

    protected static int FindMatchingBrace(string content, int startOffset)
    {
        var braceStart = content.IndexOf('{', startOffset);
        if (braceStart < 0)
            return content.Length;

        var depth = 0;
        var inString = false;
        var stringChar = '\0';
        var escaped = false;

        for (int i = braceStart; i < content.Length; i++)
        {
            var c = content[i];

            if (escaped)
            {
                escaped = false;
                continue;
            }

            if (c == '\\' && inString)
            {
                escaped = true;
                continue;
            }

            if ((c == '"' || c == '\'' || c == '`') && !inString)
            {
                inString = true;
                stringChar = c;
            }
            else if (c == stringChar && inString)
            {
                inString = false;
            }
            else if (!inString)
            {
                if (c == '{')
                    depth++;
                else if (c == '}')
                {
                    depth--;
                    if (depth == 0)
                        return i + 1;
                }
            }
        }

        return content.Length;
    }

    protected static int FindPythonBlockEnd(string content, int startOffset)
    {
        var lines = content[startOffset..].Split('\n');
        if (lines.Length == 0)
            return content.Length;

        // Get initial indentation
        var firstLine = lines[0];
        var baseIndent = GetIndentation(firstLine);
        var offset = startOffset + firstLine.Length + 1;

        for (int i = 1; i < lines.Length; i++)
        {
            var line = lines[i];

            // Skip empty lines
            if (string.IsNullOrWhiteSpace(line))
            {
                offset += line.Length + 1;
                continue;
            }

            var indent = GetIndentation(line);

            // Block ends when we return to same or lower indentation
            if (indent <= baseIndent && !line.TrimStart().StartsWith('#'))
            {
                return offset;
            }

            offset += line.Length + 1;
        }

        return content.Length;
    }

    protected static int GetIndentation(string line)
    {
        var count = 0;
        foreach (var c in line)
        {
            if (c == ' ')
                count++;
            else if (c == '\t')
                count += 4;
            else
                break;
        }
        return count;
    }

    protected static int GetLineNumber(string content, int offset)
    {
        var line = 1;
        for (int i = 0; i < offset && i < content.Length; i++)
        {
            if (content[i] == '\n')
                line++;
        }
        return line;
    }

    protected TextChunk CreateChunk(
        string content,
        string filePath,
        CodeSymbol symbol,
        string language,
        int tokenCount,
        int sequenceNumber)
    {
        return new TextChunk
        {
            Content = content,
            FilePath = filePath,
            StartLine = symbol.StartLine,
            EndLine = symbol.EndLine,
            StartOffset = symbol.StartOffset,
            EndOffset = symbol.StartOffset + symbol.Length,
            Language = language,
            Type = GetChunkTypeFromSymbol(symbol.Type),
            SymbolName = symbol.Name,
            SymbolType = symbol.Type,
            QualifiedName = symbol.QualifiedName,
            ParentSymbol = symbol.Parent,
            Namespace = symbol.Namespace,
            Summary = symbol.Documentation,
            TokenCount = tokenCount,
            SequenceNumber = sequenceNumber
        };
    }

    protected IReadOnlyList<TextChunk> SplitLargeSymbol(
        string content,
        string filePath,
        CodeSymbol symbol,
        string language,
        ChunkingOptions options,
        ref int sequenceNumber)
    {
        // Extract symbol content and use line-based chunking
        var symbolContent = content.Substring(symbol.StartOffset, symbol.Length);

        var subChunks = LineBasedChunk(symbolContent, filePath, language, options);

        // Adjust offsets and add symbol metadata
        var result = new List<TextChunk>();
        foreach (var chunk in subChunks)
        {
            result.Add(chunk with
            {
                StartOffset = symbol.StartOffset + chunk.StartOffset,
                EndOffset = symbol.StartOffset + chunk.EndOffset,
                StartLine = symbol.StartLine + chunk.StartLine - 1,
                EndLine = symbol.StartLine + chunk.EndLine - 1,
                SymbolName = symbol.Name,
                SymbolType = symbol.Type,
                ParentSymbol = symbol.Parent,
                SequenceNumber = sequenceNumber++
            });
        }

        return result;
    }

    protected IReadOnlyList<TextChunk> ChunkInterSymbolContent(
        string content,
        string filePath,
        IReadOnlyList<CodeSymbol> symbols,
        string language,
        ChunkingOptions options,
        ref int sequenceNumber)
    {
        var chunks = new List<TextChunk>();
        var sortedSymbols = symbols.OrderBy(s => s.StartOffset).ToList();
        var lastEnd = 0;

        foreach (var symbol in sortedSymbols)
        {
            if (symbol.StartOffset > lastEnd)
            {
                var gapContent = content[lastEnd..symbol.StartOffset].Trim();

                if (!string.IsNullOrWhiteSpace(gapContent))
                {
                    var tokenCount = _chunkingService.EstimateTokenCount(gapContent);

                    if (tokenCount >= options.MinChunkSize)
                    {
                        var startLine = GetLineNumber(content, lastEnd);
                        var endLine = GetLineNumber(content, symbol.StartOffset);

                        chunks.Add(new TextChunk
                        {
                            Content = gapContent,
                            FilePath = filePath,
                            StartLine = startLine,
                            EndLine = endLine,
                            StartOffset = lastEnd,
                            EndOffset = symbol.StartOffset,
                            Language = language,
                            Type = DetectChunkType(gapContent),
                            TokenCount = tokenCount,
                            SequenceNumber = sequenceNumber++
                        });
                    }
                }
            }

            lastEnd = symbol.StartOffset + symbol.Length;
        }

        // Handle trailing content
        if (lastEnd < content.Length)
        {
            var trailingContent = content[lastEnd..].Trim();

            if (!string.IsNullOrWhiteSpace(trailingContent))
            {
                var tokenCount = _chunkingService.EstimateTokenCount(trailingContent);

                if (tokenCount >= options.MinChunkSize)
                {
                    chunks.Add(new TextChunk
                    {
                        Content = trailingContent,
                        FilePath = filePath,
                        StartLine = GetLineNumber(content, lastEnd),
                        EndLine = GetLineNumber(content, content.Length),
                        StartOffset = lastEnd,
                        EndOffset = content.Length,
                        Language = language,
                        Type = DetectChunkType(trailingContent),
                        TokenCount = tokenCount,
                        SequenceNumber = sequenceNumber++
                    });
                }
            }
        }

        return chunks;
    }

    protected static ChunkType GetChunkTypeFromSymbol(SymbolType symbolType)
    {
        return symbolType switch
        {
            SymbolType.Class or SymbolType.Struct or SymbolType.Interface or SymbolType.Enum =>
                ChunkType.TypeDefinition,
            SymbolType.Import => ChunkType.Import,
            _ => ChunkType.Code
        };
    }

    protected static ChunkType DetectChunkType(string content)
    {
        var trimmed = content.TrimStart();

        if (trimmed.StartsWith("//") || trimmed.StartsWith("/*") ||
            trimmed.StartsWith("#") || trimmed.StartsWith("'''") ||
            trimmed.StartsWith("\"\"\""))
        {
            return ChunkType.Comment;
        }

        if (trimmed.StartsWith("using ") || trimmed.StartsWith("import ") ||
            trimmed.StartsWith("from ") || trimmed.StartsWith("require("))
        {
            return ChunkType.Import;
        }

        return ChunkType.Code;
    }

    protected static bool IsCallableSymbol(SymbolType type)
    {
        return type is SymbolType.Method
            or SymbolType.Function
            or SymbolType.Constructor
            or SymbolType.Lambda;
    }

    protected static List<string> CalculateOverlapLines(string[] lines, int currentIndex, int overlapTokens)
    {
        var overlapLines = new List<string>();
        var tokens = 0;

        for (int i = currentIndex - 1; i >= 0 && tokens < overlapTokens; i--)
        {
            overlapLines.Insert(0, lines[i]);
            tokens += lines[i].Length / 4; // Rough estimate
        }

        return overlapLines;
    }

    protected static string GetLanguageFromExtension(string extension) => extension switch
    {
        ".cs" => "csharp",
        ".fs" => "fsharp",
        ".ts" or ".tsx" => "typescript",
        ".js" or ".jsx" => "javascript",
        ".py" => "python",
        ".java" => "java",
        ".go" => "go",
        ".rs" => "rust",
        ".cpp" or ".hpp" or ".c" or ".h" => "cpp",
        ".swift" => "swift",
        ".rb" => "ruby",
        ".kt" => "kotlin",
        ".php" => "php",
        _ => "unknown"
    };
}
```

### File: `src/SeniorIntern.Services/Chunking/ChunkingStrategyExtensions.cs`

```csharp
using Microsoft.Extensions.DependencyInjection;
using SeniorIntern.Core.Interfaces;

namespace SeniorIntern.Services.Chunking;

/// <summary>
/// Extension methods for registering chunking strategies.
/// </summary>
public static class ChunkingStrategyExtensions
{
    /// <summary>
    /// Add all chunking services and strategies.
    /// </summary>
    public static IServiceCollection AddChunkingServices(this IServiceCollection services)
    {
        // Register main service
        services.AddSingleton<IChunkingService, ChunkingService>();

        // Register strategies
        services.AddSingleton<IChunkingStrategy, CodeAwareChunkingStrategy>();
        services.AddSingleton<IChunkingStrategy, MarkdownChunkingStrategy>();
        services.AddSingleton<IChunkingStrategy, PlainTextChunkingStrategy>();

        // Register language-specific strategies
        services.AddSingleton<IChunkingStrategy, CSharpChunkingStrategy>();
        services.AddSingleton<IChunkingStrategy, TypeScriptChunkingStrategy>();
        services.AddSingleton<IChunkingStrategy, PythonChunkingStrategy>();

        return services;
    }
}
```

### Acceptance Criteria (v0.7.1f)
- [ ] CodeAwareChunkingStrategy extracts symbols via regex
- [ ] Semantic chunking by code structure works
- [ ] Line-based fallback with overlap works
- [ ] Brace matching for C-style languages
- [ ] Indentation-based block detection for Python
- [ ] Inter-symbol content handled
- [ ] Large symbols split appropriately
- [ ] Token estimation used for chunk sizing

---

## v0.7.1g: Language-Specific Chunkers

### Objective
Implement specialized chunking strategies for specific languages with better accuracy.

### File: `src/SeniorIntern.Services/Chunking/CSharpChunkingStrategy.cs`

```csharp
using System.Collections.Generic;
using System.IO;
using System.Text.RegularExpressions;
using Microsoft.Extensions.Logging;
using SeniorIntern.Core.Interfaces;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Services.Chunking;

/// <summary>
/// Specialized chunking strategy for C# code.
/// </summary>
public sealed class CSharpChunkingStrategy : CodeAwareChunkingStrategy
{
    private static readonly HashSet<string> _extensions = new(StringComparer.OrdinalIgnoreCase)
    {
        ".cs"
    };

    public override string Name => "CSharp";
    public override IReadOnlySet<string> SupportedExtensions => _extensions;
    public override int Priority => 200; // Higher than generic CodeAware

    public CSharpChunkingStrategy(
        ILogger<CSharpChunkingStrategy> logger,
        IChunkingService chunkingService)
        : base(logger, chunkingService)
    {
    }

    public override bool CanHandle(string filePath)
    {
        var extension = Path.GetExtension(filePath);
        return extension.Equals(".cs", System.StringComparison.OrdinalIgnoreCase);
    }

    protected override IEnumerable<(string Pattern, SymbolType Type)> GetSymbolPatterns(string language)
    {
        return new[]
        {
            // Namespaces
            (@"namespace\s+(?<name>[\w.]+)\s*[{;]", SymbolType.Namespace),

            // Classes (including records, generics)
            (@"(?:public|private|protected|internal)?\s*(?:static\s+)?(?:abstract\s+)?(?:sealed\s+)?(?:partial\s+)?(?:class|record)\s+(?<name>\w+)(?:<[^>]+>)?", SymbolType.Class),

            // Structs
            (@"(?:public|private|protected|internal)?\s*(?:readonly\s+)?(?:ref\s+)?struct\s+(?<name>\w+)", SymbolType.Struct),

            // Interfaces
            (@"(?:public|private|protected|internal)?\s*(?:partial\s+)?interface\s+(?<name>\w+)(?:<[^>]+>)?", SymbolType.Interface),

            // Enums
            (@"(?:public|private|protected|internal)?\s*enum\s+(?<name>\w+)", SymbolType.Enum),

            // Methods
            (@"(?:public|private|protected|internal)?\s*(?:static\s+)?(?:virtual\s+)?(?:override\s+)?(?:abstract\s+)?(?:async\s+)?(?:[\w<>[\],\s]+)\s+(?<name>\w+)\s*(?:<[^>]+>)?\s*\([^)]*\)\s*(?:where\s+[^{]+)?(?=\s*[{;=>])", SymbolType.Method),

            // Properties
            (@"(?:public|private|protected|internal)?\s*(?:static\s+)?(?:virtual\s+)?(?:override\s+)?(?:required\s+)?(?:[\w<>[\],?\s]+)\s+(?<name>\w+)\s*(?=\s*[{=>])", SymbolType.Property),

            // Constructors
            (@"(?:public|private|protected|internal)?\s*(?<name>\w+)\s*\([^)]*\)\s*(?::\s*(?:base|this)\s*\([^)]*\))?\s*(?=[{])", SymbolType.Constructor),

            // Events
            (@"(?:public|private|protected|internal)?\s*event\s+[\w<>]+\s+(?<name>\w+)", SymbolType.Event),

            // Delegates
            (@"(?:public|private|protected|internal)?\s*delegate\s+[\w<>]+\s+(?<name>\w+)\s*\(", SymbolType.Delegate),
        };
    }

    protected override IReadOnlyList<CodeSymbol> ExtractSymbolsForLanguage(string content, string language)
    {
        var symbols = new List<CodeSymbol>();
        var currentNamespace = "";
        var classStack = new Stack<string>();

        var patterns = GetSymbolPatterns(language);

        foreach (var (pattern, symbolType) in patterns)
        {
            var matches = Regex.Matches(content, pattern, RegexOptions.Multiline);

            foreach (Match match in matches)
            {
                var nameGroup = match.Groups["name"];
                if (!nameGroup.Success)
                    continue;

                var name = nameGroup.Value;
                var startOffset = match.Index;
                var endOffset = FindSymbolEnd(content, startOffset, "csharp");
                var startLine = GetLineNumber(content, startOffset);
                var endLine = GetLineNumber(content, endOffset);

                // Track namespace context
                if (symbolType == SymbolType.Namespace)
                {
                    currentNamespace = name;
                }

                // Track class context
                if (symbolType is SymbolType.Class or SymbolType.Struct or SymbolType.Interface)
                {
                    classStack.Push(name);
                }

                var parent = classStack.Count > 0 ? classStack.Peek() : null;

                // Extract XML documentation if present
                var documentation = ExtractXmlDocumentation(content, startOffset);

                symbols.Add(new CodeSymbol
                {
                    Name = name,
                    Type = symbolType,
                    QualifiedName = BuildQualifiedName(currentNamespace, classStack, name),
                    Parent = symbolType is SymbolType.Method or SymbolType.Property or SymbolType.Constructor
                        ? parent
                        : null,
                    Namespace = currentNamespace,
                    StartLine = startLine,
                    EndLine = endLine,
                    StartOffset = startOffset,
                    Length = endOffset - startOffset,
                    Documentation = documentation
                });
            }
        }

        return symbols;
    }

    private static string? ExtractXmlDocumentation(string content, int symbolOffset)
    {
        // Look for /// comments before the symbol
        var beforeSymbol = content[..symbolOffset];
        var lines = beforeSymbol.Split('\n');

        var docLines = new List<string>();

        // Walk backwards through lines
        for (int i = lines.Length - 1; i >= 0; i--)
        {
            var line = lines[i].Trim();

            if (line.StartsWith("///"))
            {
                docLines.Insert(0, line[3..].Trim());
            }
            else if (string.IsNullOrWhiteSpace(line))
            {
                continue;
            }
            else
            {
                break;
            }
        }

        if (docLines.Count == 0)
            return null;

        // Extract summary from XML
        var fullDoc = string.Join("\n", docLines);
        var summaryMatch = Regex.Match(fullDoc, @"<summary>\s*(.*?)\s*</summary>", RegexOptions.Singleline);

        return summaryMatch.Success
            ? summaryMatch.Groups[1].Value.Trim()
            : string.Join(" ", docLines);
    }

    private static string BuildQualifiedName(string ns, Stack<string> classStack, string name)
    {
        var parts = new List<string>();

        if (!string.IsNullOrEmpty(ns))
            parts.Add(ns);

        parts.AddRange(classStack.Reverse());
        parts.Add(name);

        return string.Join(".", parts);
    }
}
```

### File: `src/SeniorIntern.Services/Chunking/TypeScriptChunkingStrategy.cs`

```csharp
using System.Collections.Generic;
using System.IO;
using System.Text.RegularExpressions;
using Microsoft.Extensions.Logging;
using SeniorIntern.Core.Interfaces;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Services.Chunking;

/// <summary>
/// Specialized chunking strategy for TypeScript/JavaScript code.
/// </summary>
public sealed class TypeScriptChunkingStrategy : CodeAwareChunkingStrategy
{
    private static readonly HashSet<string> _extensions = new(StringComparer.OrdinalIgnoreCase)
    {
        ".ts", ".tsx", ".js", ".jsx", ".mjs"
    };

    public override string Name => "TypeScript";
    public override IReadOnlySet<string> SupportedExtensions => _extensions;
    public override int Priority => 200;

    public TypeScriptChunkingStrategy(
        ILogger<TypeScriptChunkingStrategy> logger,
        IChunkingService chunkingService)
        : base(logger, chunkingService)
    {
    }

    protected override IEnumerable<(string Pattern, SymbolType Type)> GetSymbolPatterns(string language)
    {
        return new[]
        {
            // Classes
            (@"(?:export\s+)?(?:default\s+)?(?:abstract\s+)?class\s+(?<name>\w+)(?:<[^>]+>)?(?:\s+extends\s+[\w.<>]+)?(?:\s+implements\s+[\w.<>,\s]+)?", SymbolType.Class),

            // Interfaces
            (@"(?:export\s+)?interface\s+(?<name>\w+)(?:<[^>]+>)?(?:\s+extends\s+[\w.<>,\s]+)?", SymbolType.Interface),

            // Type aliases
            (@"(?:export\s+)?type\s+(?<name>\w+)(?:<[^>]+>)?\s*=", SymbolType.TypeAlias),

            // Enums
            (@"(?:export\s+)?(?:const\s+)?enum\s+(?<name>\w+)", SymbolType.Enum),

            // Functions (including async)
            (@"(?:export\s+)?(?:default\s+)?(?:async\s+)?function\s*\*?\s*(?<name>\w+)\s*(?:<[^>]+>)?\s*\(", SymbolType.Function),

            // Arrow functions assigned to const/let
            (@"(?:export\s+)?(?:const|let|var)\s+(?<name>\w+)\s*(?::\s*[^=]+)?\s*=\s*(?:async\s+)?\([^)]*\)\s*(?::\s*[^=]+)?\s*=>", SymbolType.Function),

            // Methods (inside class)
            (@"(?:public|private|protected)?\s*(?:static\s+)?(?:readonly\s+)?(?:async\s+)?(?<name>\w+)\s*(?:<[^>]+>)?\s*\([^)]*\)\s*(?::\s*[\w<>[\]|&\s]+)?\s*{", SymbolType.Method),

            // Properties/Fields
            (@"(?:public|private|protected)?\s*(?:static\s+)?(?:readonly\s+)?(?<name>\w+)\s*(?:\?)?:\s*[\w<>[\]|&\s]+\s*[;=]", SymbolType.Property),

            // React components (function components)
            (@"(?:export\s+)?(?:default\s+)?(?:const|function)\s+(?<name>[A-Z]\w+)\s*(?::\s*[\w<>]+)?\s*=?\s*(?:\([^)]*\)|props)\s*(?::\s*[\w<>]+)?\s*=>?\s*{?\s*(?:return)?\s*\(?\s*<", SymbolType.Function),
        };
    }

    protected override IReadOnlyList<CodeSymbol> ExtractSymbolsForLanguage(string content, string language)
    {
        var symbols = new List<CodeSymbol>();
        var patterns = GetSymbolPatterns(language);

        foreach (var (pattern, symbolType) in patterns)
        {
            var matches = Regex.Matches(content, pattern, RegexOptions.Multiline);

            foreach (Match match in matches)
            {
                var nameGroup = match.Groups["name"];
                if (!nameGroup.Success)
                    continue;

                var name = nameGroup.Value;
                var startOffset = match.Index;
                var endOffset = FindSymbolEnd(content, startOffset, language);
                var startLine = GetLineNumber(content, startOffset);
                var endLine = GetLineNumber(content, endOffset);

                // Extract JSDoc if present
                var documentation = ExtractJsDoc(content, startOffset);

                symbols.Add(new CodeSymbol
                {
                    Name = name,
                    Type = symbolType,
                    StartLine = startLine,
                    EndLine = endLine,
                    StartOffset = startOffset,
                    Length = endOffset - startOffset,
                    Documentation = documentation
                });
            }
        }

        return symbols;
    }

    private static string? ExtractJsDoc(string content, int symbolOffset)
    {
        // Look for /** */ comments before the symbol
        var beforeSymbol = content[..symbolOffset];
        var docMatch = Regex.Match(beforeSymbol, @"/\*\*\s*([\s\S]*?)\s*\*/\s*$");

        if (!docMatch.Success)
            return null;

        var docContent = docMatch.Groups[1].Value;

        // Clean up the doc
        var lines = docContent.Split('\n')
            .Select(l => Regex.Replace(l.Trim(), @"^\*\s*", "").Trim())
            .Where(l => !string.IsNullOrEmpty(l) && !l.StartsWith("@"));

        return string.Join(" ", lines);
    }
}
```

### File: `src/SeniorIntern.Services/Chunking/PythonChunkingStrategy.cs`

```csharp
using System.Collections.Generic;
using System.IO;
using System.Text.RegularExpressions;
using Microsoft.Extensions.Logging;
using SeniorIntern.Core.Interfaces;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Services.Chunking;

/// <summary>
/// Specialized chunking strategy for Python code.
/// </summary>
public sealed class PythonChunkingStrategy : CodeAwareChunkingStrategy
{
    private static readonly HashSet<string> _extensions = new(StringComparer.OrdinalIgnoreCase)
    {
        ".py", ".pyi"
    };

    public override string Name => "Python";
    public override IReadOnlySet<string> SupportedExtensions => _extensions;
    public override int Priority => 200;

    public PythonChunkingStrategy(
        ILogger<PythonChunkingStrategy> logger,
        IChunkingService chunkingService)
        : base(logger, chunkingService)
    {
    }

    protected override IEnumerable<(string Pattern, SymbolType Type)> GetSymbolPatterns(string language)
    {
        return new[]
        {
            // Classes
            (@"^class\s+(?<name>\w+)\s*(?:\([^)]*\))?\s*:", SymbolType.Class),

            // Functions (including async)
            (@"^(?:async\s+)?def\s+(?<name>\w+)\s*\([^)]*\)\s*(?:->\s*[\w\[\],\s]+)?\s*:", SymbolType.Function),

            // Methods (indented functions inside classes)
            (@"^\s+(?:async\s+)?def\s+(?<name>\w+)\s*\(self[^)]*\)\s*(?:->\s*[\w\[\],\s]+)?\s*:", SymbolType.Method),

            // Class methods and static methods
            (@"^\s+@(?:classmethod|staticmethod)\s*\n\s*(?:async\s+)?def\s+(?<name>\w+)", SymbolType.Method),
        };
    }

    protected override IReadOnlyList<CodeSymbol> ExtractSymbolsForLanguage(string content, string language)
    {
        var symbols = new List<CodeSymbol>();
        string? currentClass = null;

        var lines = content.Split('\n');
        var offset = 0;

        for (int i = 0; i < lines.Length; i++)
        {
            var line = lines[i];
            var lineStart = offset;
            offset += line.Length + 1;

            // Check for class definition
            var classMatch = Regex.Match(line, @"^class\s+(\w+)\s*(?:\([^)]*\))?\s*:");
            if (classMatch.Success)
            {
                currentClass = classMatch.Groups[1].Value;
                var endOffset = FindPythonBlockEnd(content, lineStart);

                symbols.Add(new CodeSymbol
                {
                    Name = currentClass,
                    Type = SymbolType.Class,
                    StartLine = i + 1,
                    EndLine = GetLineNumber(content, endOffset),
                    StartOffset = lineStart,
                    Length = endOffset - lineStart,
                    Documentation = ExtractDocstring(content, lineStart)
                });

                continue;
            }

            // Check for function/method definition
            var funcMatch = Regex.Match(line, @"^(\s*)(?:async\s+)?def\s+(\w+)\s*\(([^)]*)\)");
            if (funcMatch.Success)
            {
                var indent = funcMatch.Groups[1].Value;
                var name = funcMatch.Groups[2].Value;
                var params_ = funcMatch.Groups[3].Value;

                var endOffset = FindPythonBlockEnd(content, lineStart);
                var isMethod = indent.Length > 0 && params_.Contains("self");
                var symbolType = isMethod ? SymbolType.Method : SymbolType.Function;

                symbols.Add(new CodeSymbol
                {
                    Name = name,
                    Type = symbolType,
                    Parent = isMethod ? currentClass : null,
                    StartLine = i + 1,
                    EndLine = GetLineNumber(content, endOffset),
                    StartOffset = lineStart,
                    Length = endOffset - lineStart,
                    Documentation = ExtractDocstring(content, lineStart)
                });
            }

            // Reset class context when leaving indentation
            if (!string.IsNullOrWhiteSpace(line) && !line.StartsWith(" ") && !line.StartsWith("\t"))
            {
                if (!classMatch.Success && !funcMatch.Success)
                    currentClass = null;
            }
        }

        return symbols;
    }

    private static string? ExtractDocstring(string content, int symbolOffset)
    {
        // Find the line with the definition
        var colonIndex = content.IndexOf(':', symbolOffset);
        if (colonIndex < 0)
            return null;

        // Look for docstring after the colon
        var afterColon = content[(colonIndex + 1)..];
        var docMatch = Regex.Match(afterColon, @"^\s*(?:\"\"\"([\s\S]*?)\"\"\"|'''([\s\S]*?)''')");

        if (!docMatch.Success)
            return null;

        var doc = docMatch.Groups[1].Success
            ? docMatch.Groups[1].Value
            : docMatch.Groups[2].Value;

        // Clean up the docstring
        var lines = doc.Split('\n')
            .Select(l => l.Trim())
            .Where(l => !string.IsNullOrEmpty(l));

        return string.Join(" ", lines);
    }
}
```

### File: `src/SeniorIntern.Services/Chunking/MarkdownChunkingStrategy.cs`

```csharp
using System.Collections.Generic;
using System.IO;
using System.Text;
using System.Text.RegularExpressions;
using Microsoft.Extensions.Logging;
using SeniorIntern.Core.Interfaces;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Services.Chunking;

/// <summary>
/// Chunking strategy for Markdown documents.
/// Chunks by headings and sections.
/// </summary>
public sealed class MarkdownChunkingStrategy : IChunkingStrategy
{
    private readonly ILogger<MarkdownChunkingStrategy> _logger;
    private readonly IChunkingService _chunkingService;

    private static readonly HashSet<string> _extensions = new(StringComparer.OrdinalIgnoreCase)
    {
        ".md", ".markdown", ".mdx"
    };

    public string Name => "Markdown";
    public IReadOnlySet<string> SupportedExtensions => _extensions;
    public int Priority => 150;

    public MarkdownChunkingStrategy(
        ILogger<MarkdownChunkingStrategy> logger,
        IChunkingService chunkingService)
    {
        _logger = logger;
        _chunkingService = chunkingService;
    }

    public bool CanHandle(string filePath)
    {
        var extension = Path.GetExtension(filePath);
        return SupportedExtensions.Contains(extension);
    }

    public IReadOnlyList<TextChunk> Chunk(
        string content,
        string filePath,
        ChunkingOptions options)
    {
        var chunks = new List<TextChunk>();
        var sections = SplitByHeadings(content);
        var sequenceNumber = 0;

        foreach (var section in sections)
        {
            var tokenCount = _chunkingService.EstimateTokenCount(section.Content);

            if (tokenCount < options.MinChunkSize)
                continue;

            if (tokenCount <= options.MaxChunkSize)
            {
                // Section fits in one chunk
                chunks.Add(new TextChunk
                {
                    Content = section.Content,
                    FilePath = filePath,
                    StartLine = section.StartLine,
                    EndLine = section.EndLine,
                    StartOffset = section.StartOffset,
                    EndOffset = section.EndOffset,
                    Type = ChunkType.Markdown,
                    Language = "markdown",
                    SymbolName = section.Heading,
                    SymbolType = SymbolType.Other,
                    TokenCount = tokenCount,
                    SequenceNumber = sequenceNumber++
                });
            }
            else
            {
                // Section too large, split by paragraphs
                var subChunks = SplitByParagraphs(
                    section, filePath, options, ref sequenceNumber);
                chunks.AddRange(subChunks);
            }
        }

        return chunks;
    }

    private static List<MarkdownSection> SplitByHeadings(string content)
    {
        var sections = new List<MarkdownSection>();
        var lines = content.Split('\n');
        var currentSection = new StringBuilder();
        var currentHeading = "";
        var startLine = 1;
        var startOffset = 0;
        var currentOffset = 0;

        for (int i = 0; i < lines.Length; i++)
        {
            var line = lines[i];
            var headingMatch = Regex.Match(line, @"^(#{1,6})\s+(.+)$");

            if (headingMatch.Success)
            {
                // Save previous section
                if (currentSection.Length > 0)
                {
                    sections.Add(new MarkdownSection
                    {
                        Heading = currentHeading,
                        Content = currentSection.ToString().Trim(),
                        StartLine = startLine,
                        EndLine = i,
                        StartOffset = startOffset,
                        EndOffset = currentOffset
                    });
                }

                // Start new section
                currentHeading = headingMatch.Groups[2].Value;
                currentSection.Clear();
                startLine = i + 1;
                startOffset = currentOffset;
            }

            currentSection.AppendLine(line);
            currentOffset += line.Length + 1;
        }

        // Don't forget the last section
        if (currentSection.Length > 0)
        {
            sections.Add(new MarkdownSection
            {
                Heading = currentHeading,
                Content = currentSection.ToString().Trim(),
                StartLine = startLine,
                EndLine = lines.Length,
                StartOffset = startOffset,
                EndOffset = content.Length
            });
        }

        return sections;
    }

    private IReadOnlyList<TextChunk> SplitByParagraphs(
        MarkdownSection section,
        string filePath,
        ChunkingOptions options,
        ref int sequenceNumber)
    {
        var chunks = new List<TextChunk>();
        var paragraphs = Regex.Split(section.Content, @"\n\n+");
        var currentChunk = new StringBuilder();
        var currentTokens = 0;
        var startLine = section.StartLine;
        var startOffset = section.StartOffset;
        var currentOffset = section.StartOffset;

        foreach (var para in paragraphs)
        {
            var paraTokens = _chunkingService.EstimateTokenCount(para);

            if (currentTokens + paraTokens > options.TargetChunkSize && currentChunk.Length > 0)
            {
                // Emit current chunk
                chunks.Add(new TextChunk
                {
                    Content = currentChunk.ToString().Trim(),
                    FilePath = filePath,
                    StartLine = startLine,
                    EndLine = startLine + currentChunk.ToString().Count(c => c == '\n'),
                    StartOffset = startOffset,
                    EndOffset = currentOffset,
                    Type = ChunkType.Markdown,
                    Language = "markdown",
                    SymbolName = section.Heading,
                    TokenCount = currentTokens,
                    SequenceNumber = sequenceNumber++
                });

                currentChunk.Clear();
                startLine = startLine + currentChunk.ToString().Count(c => c == '\n') + 1;
                startOffset = currentOffset;
                currentTokens = 0;
            }

            currentChunk.AppendLine(para);
            currentChunk.AppendLine();
            currentOffset += para.Length + 2;
            currentTokens += paraTokens;
        }

        // Last chunk
        if (currentChunk.Length > 0 && currentTokens >= options.MinChunkSize)
        {
            chunks.Add(new TextChunk
            {
                Content = currentChunk.ToString().Trim(),
                FilePath = filePath,
                StartLine = startLine,
                EndLine = section.EndLine,
                StartOffset = startOffset,
                EndOffset = section.EndOffset,
                Type = ChunkType.Markdown,
                Language = "markdown",
                SymbolName = section.Heading,
                TokenCount = currentTokens,
                SequenceNumber = sequenceNumber++
            });
        }

        return chunks;
    }

    private sealed class MarkdownSection
    {
        public string Heading { get; init; } = "";
        public string Content { get; init; } = "";
        public int StartLine { get; init; }
        public int EndLine { get; init; }
        public int StartOffset { get; init; }
        public int EndOffset { get; init; }
    }
}
```

### File: `src/SeniorIntern.Services/Chunking/PlainTextChunkingStrategy.cs`

```csharp
using System.Collections.Generic;
using System.Text;
using Microsoft.Extensions.Logging;
using SeniorIntern.Core.Interfaces;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Services.Chunking;

/// <summary>
/// Default chunking strategy for plain text and unknown file types.
/// </summary>
public sealed class PlainTextChunkingStrategy : IChunkingStrategy
{
    private readonly ILogger<PlainTextChunkingStrategy> _logger;
    private readonly IChunkingService _chunkingService;

    private static readonly HashSet<string> _extensions = new(StringComparer.OrdinalIgnoreCase)
    {
        ".txt", ".text", ".log"
    };

    public string Name => "PlainText";
    public IReadOnlySet<string> SupportedExtensions => _extensions;
    public int Priority => 0; // Lowest priority - fallback

    public PlainTextChunkingStrategy(
        ILogger<PlainTextChunkingStrategy> logger,
        IChunkingService chunkingService)
    {
        _logger = logger;
        _chunkingService = chunkingService;
    }

    public bool CanHandle(string filePath)
    {
        // Can handle anything as a fallback
        return true;
    }

    public IReadOnlyList<TextChunk> Chunk(
        string content,
        string filePath,
        ChunkingOptions options)
    {
        var chunks = new List<TextChunk>();
        var lines = content.Split('\n');
        var currentChunk = new StringBuilder();
        var startLine = 1;
        var startOffset = 0;
        var currentOffset = 0;
        var currentTokenCount = 0;
        var sequenceNumber = 0;

        for (int i = 0; i < lines.Length; i++)
        {
            var line = lines[i];
            var lineTokens = _chunkingService.EstimateTokenCount(line);

            if (currentTokenCount + lineTokens > options.TargetChunkSize && currentChunk.Length > 0)
            {
                // Emit current chunk
                chunks.Add(new TextChunk
                {
                    Content = currentChunk.ToString().TrimEnd(),
                    FilePath = filePath,
                    StartLine = startLine,
                    EndLine = i,
                    StartOffset = startOffset,
                    EndOffset = currentOffset,
                    Type = ChunkType.PlainText,
                    TokenCount = currentTokenCount,
                    SequenceNumber = sequenceNumber++,
                    HasOverlap = sequenceNumber > 0
                });

                // Start new chunk with overlap
                var overlapLines = CalculateOverlapLines(lines, i, options.ChunkOverlap);
                currentChunk.Clear();

                foreach (var ol in overlapLines)
                    currentChunk.AppendLine(ol);

                startLine = System.Math.Max(1, i - overlapLines.Count + 1);
                startOffset = currentOffset - string.Join('\n', overlapLines).Length;
                currentTokenCount = _chunkingService.EstimateTokenCount(currentChunk.ToString());
            }

            currentChunk.AppendLine(line);
            currentOffset += line.Length + 1;
            currentTokenCount += lineTokens;
        }

        // Last chunk
        if (currentChunk.Length > 0 && currentTokenCount >= options.MinChunkSize)
        {
            chunks.Add(new TextChunk
            {
                Content = currentChunk.ToString().TrimEnd(),
                FilePath = filePath,
                StartLine = startLine,
                EndLine = lines.Length,
                StartOffset = startOffset,
                EndOffset = content.Length,
                Type = ChunkType.PlainText,
                TokenCount = currentTokenCount,
                SequenceNumber = sequenceNumber
            });
        }

        return chunks;
    }

    private static List<string> CalculateOverlapLines(string[] lines, int currentIndex, int overlapTokens)
    {
        var overlapLines = new List<string>();
        var tokens = 0;

        for (int i = currentIndex - 1; i >= 0 && tokens < overlapTokens; i--)
        {
            overlapLines.Insert(0, lines[i]);
            tokens += lines[i].Length / 4;
        }

        return overlapLines;
    }
}
```

### Acceptance Criteria (v0.7.1g)
- [ ] CSharpChunkingStrategy extracts C# symbols accurately
- [ ] TypeScriptChunkingStrategy handles TS/JS patterns
- [ ] PythonChunkingStrategy handles indentation-based blocks
- [ ] XML/JSDoc documentation extraction works
- [ ] Each strategy has higher priority than generic CodeAware
- [ ] Qualified names built correctly with namespaces/classes

---

## v0.7.1h: Markdown & Plain Text Chunking

This section was covered in v0.7.1g with `MarkdownChunkingStrategy` and `PlainTextChunkingStrategy`.

### Additional File: `src/SeniorIntern.Services/Chunking/ConfigFileChunkingStrategy.cs`

```csharp
using System.Collections.Generic;
using System.IO;
using System.Text;
using Microsoft.Extensions.Logging;
using SeniorIntern.Core.Interfaces;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Services.Chunking;

/// <summary>
/// Chunking strategy for configuration files (JSON, YAML, XML).
/// </summary>
public sealed class ConfigFileChunkingStrategy : IChunkingStrategy
{
    private readonly ILogger<ConfigFileChunkingStrategy> _logger;
    private readonly IChunkingService _chunkingService;

    private static readonly HashSet<string> _extensions = new(StringComparer.OrdinalIgnoreCase)
    {
        ".json", ".yaml", ".yml", ".xml", ".toml", ".ini", ".env"
    };

    public string Name => "ConfigFile";
    public IReadOnlySet<string> SupportedExtensions => _extensions;
    public int Priority => 120;

    public ConfigFileChunkingStrategy(
        ILogger<ConfigFileChunkingStrategy> logger,
        IChunkingService chunkingService)
    {
        _logger = logger;
        _chunkingService = chunkingService;
    }

    public bool CanHandle(string filePath)
    {
        var extension = Path.GetExtension(filePath);
        return SupportedExtensions.Contains(extension);
    }

    public IReadOnlyList<TextChunk> Chunk(
        string content,
        string filePath,
        ChunkingOptions options)
    {
        // For config files, try to keep them as single chunks if possible
        var tokenCount = _chunkingService.EstimateTokenCount(content);

        if (tokenCount <= options.MaxChunkSize)
        {
            return new[]
            {
                new TextChunk
                {
                    Content = content,
                    FilePath = filePath,
                    StartLine = 1,
                    EndLine = content.Count(c => c == '\n') + 1,
                    StartOffset = 0,
                    EndOffset = content.Length,
                    Type = ChunkType.Config,
                    Language = Path.GetExtension(filePath).TrimStart('.'),
                    TokenCount = tokenCount,
                    SequenceNumber = 0
                }
            };
        }

        // If too large, fall back to line-based chunking
        return ChunkByLines(content, filePath, options);
    }

    private IReadOnlyList<TextChunk> ChunkByLines(
        string content,
        string filePath,
        ChunkingOptions options)
    {
        var chunks = new List<TextChunk>();
        var lines = content.Split('\n');
        var currentChunk = new StringBuilder();
        var startLine = 1;
        var startOffset = 0;
        var currentOffset = 0;
        var currentTokens = 0;
        var sequenceNumber = 0;

        foreach (var line in lines)
        {
            var lineTokens = _chunkingService.EstimateTokenCount(line);

            if (currentTokens + lineTokens > options.TargetChunkSize && currentChunk.Length > 0)
            {
                chunks.Add(new TextChunk
                {
                    Content = currentChunk.ToString().TrimEnd(),
                    FilePath = filePath,
                    StartLine = startLine,
                    EndLine = startLine + currentChunk.ToString().Count(c => c == '\n'),
                    StartOffset = startOffset,
                    EndOffset = currentOffset,
                    Type = ChunkType.Config,
                    Language = Path.GetExtension(filePath).TrimStart('.'),
                    TokenCount = currentTokens,
                    SequenceNumber = sequenceNumber++
                });

                currentChunk.Clear();
                startLine += currentChunk.ToString().Count(c => c == '\n') + 1;
                startOffset = currentOffset;
                currentTokens = 0;
            }

            currentChunk.AppendLine(line);
            currentOffset += line.Length + 1;
            currentTokens += lineTokens;
        }

        if (currentChunk.Length > 0 && currentTokens >= options.MinChunkSize)
        {
            chunks.Add(new TextChunk
            {
                Content = currentChunk.ToString().TrimEnd(),
                FilePath = filePath,
                StartLine = startLine,
                EndLine = lines.Length,
                StartOffset = startOffset,
                EndOffset = content.Length,
                Type = ChunkType.Config,
                Language = Path.GetExtension(filePath).TrimStart('.'),
                TokenCount = currentTokens,
                SequenceNumber = sequenceNumber
            });
        }

        return chunks;
    }
}
```

### Acceptance Criteria (v0.7.1h)
- [ ] MarkdownChunkingStrategy splits by headings
- [ ] PlainTextChunkingStrategy handles generic text
- [ ] ConfigFileChunkingStrategy preserves config files when possible
- [ ] Overlap maintained between chunks
- [ ] All strategies use token estimation

---

## v0.7.1i: Tokenization Utilities

### Objective
Provide tokenization utilities for accurate chunk sizing.

### File: `src/SeniorIntern.Services/Embedding/SimpleTokenizer.cs`

```csharp
using System;
using System.Collections.Generic;
using System.Linq;
using System.Text.RegularExpressions;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.Extensions.Logging;
using SeniorIntern.Core.Interfaces;

namespace SeniorIntern.Services.Embedding;

/// <summary>
/// Simple tokenizer for token count estimation when no model tokenizer is loaded.
/// Uses a rule-based approach similar to GPT-style tokenization.
/// </summary>
public sealed class SimpleTokenizer : ITokenizerService
{
    private readonly ILogger<SimpleTokenizer> _logger;

    // Common programming tokens
    private static readonly Regex _tokenPattern = new(
        @"(?:
            \d+\.?\d*              |  # Numbers
            [a-zA-Z_]\w*          |  # Identifiers
            ""(?:[^""\\]|\\.)*""  |  # Double-quoted strings
            '(?:[^'\\]|\\.)*'     |  # Single-quoted strings
            //.*$                  |  # Single-line comments
            /\*[\s\S]*?\*/        |  # Multi-line comments
            [+\-*/=<>!&|^~%]+     |  # Operators
            [{}()\[\];:,.]        |  # Punctuation
            \s+                      # Whitespace
        )",
        RegexOptions.Compiled | RegexOptions.IgnorePatternWhitespace | RegexOptions.Multiline);

    public bool IsLoaded => true; // Simple tokenizer is always "loaded"

    public SimpleTokenizer(ILogger<SimpleTokenizer> logger)
    {
        _logger = logger;
    }

    public Task LoadTokenizerAsync(string path, CancellationToken ct = default)
    {
        // Simple tokenizer doesn't need loading
        _logger.LogDebug("SimpleTokenizer doesn't require loading");
        return Task.CompletedTask;
    }

    public TokenizedText Encode(string text, int maxLength)
    {
        if (string.IsNullOrEmpty(text))
        {
            return new TokenizedText
            {
                InputIds = Array.Empty<long>(),
                AttentionMask = Array.Empty<long>()
            };
        }

        var matches = _tokenPattern.Matches(text);
        var tokenIds = new List<long>();

        foreach (Match match in matches)
        {
            if (tokenIds.Count >= maxLength)
                break;

            // Use hash as pseudo-token ID
            var tokenId = (long)match.Value.GetHashCode();
            tokenIds.Add(tokenId);
        }

        var attentionMask = Enumerable.Repeat(1L, tokenIds.Count).ToArray();

        return new TokenizedText
        {
            InputIds = tokenIds.ToArray(),
            AttentionMask = attentionMask
        };
    }

    public string Decode(long[] tokenIds)
    {
        // Simple tokenizer can't decode
        throw new NotSupportedException("SimpleTokenizer does not support decoding");
    }

    public int CountTokens(string text)
    {
        if (string.IsNullOrEmpty(text))
            return 0;

        return _tokenPattern.Matches(text).Count;
    }
}
```

### File: `src/SeniorIntern.Services/Embedding/TokenizerFactory.cs`

```csharp
using System;
using System.IO;
using Microsoft.Extensions.DependencyInjection;
using Microsoft.Extensions.Logging;
using SeniorIntern.Core.Interfaces;

namespace SeniorIntern.Services.Embedding;

/// <summary>
/// Factory for creating tokenizer instances.
/// </summary>
public sealed class TokenizerFactory
{
    private readonly IServiceProvider _serviceProvider;
    private readonly ILogger<TokenizerFactory> _logger;

    public TokenizerFactory(
        IServiceProvider serviceProvider,
        ILogger<TokenizerFactory> logger)
    {
        _serviceProvider = serviceProvider;
        _logger = logger;
    }

    /// <summary>
    /// Create a tokenizer for the given model path.
    /// </summary>
    public ITokenizerService CreateTokenizer(string? modelPath = null)
    {
        // Try to load HuggingFace tokenizer if path provided
        if (modelPath is not null)
        {
            var tokenizerPath = Path.Combine(
                Path.GetDirectoryName(modelPath)!,
                "tokenizer.json");

            if (File.Exists(tokenizerPath))
            {
                _logger.LogInformation(
                    "Creating HuggingFace tokenizer from {Path}",
                    tokenizerPath);

                return _serviceProvider.GetRequiredService<HuggingFaceTokenizerService>();
            }
        }

        // Fall back to simple tokenizer
        _logger.LogDebug("Using simple tokenizer");
        return _serviceProvider.GetRequiredService<SimpleTokenizer>();
    }
}
```

### File: `src/SeniorIntern.Services/Embedding/TokenEstimator.cs`

```csharp
using System;

namespace SeniorIntern.Services.Embedding;

/// <summary>
/// Utility for estimating token counts without a full tokenizer.
/// </summary>
public static class TokenEstimator
{
    /// <summary>
    /// Estimate token count for code/technical text.
    /// Uses approximately 4 characters per token.
    /// </summary>
    public static int EstimateForCode(string text)
    {
        if (string.IsNullOrEmpty(text))
            return 0;

        // Code typically has ~4 chars per token
        return (int)Math.Ceiling(text.Length / 4.0);
    }

    /// <summary>
    /// Estimate token count for natural language text.
    /// Uses approximately 4-5 characters per token for English.
    /// </summary>
    public static int EstimateForText(string text)
    {
        if (string.IsNullOrEmpty(text))
            return 0;

        // Natural language: ~4.5 chars per token
        return (int)Math.Ceiling(text.Length / 4.5);
    }

    /// <summary>
    /// Estimate token count with more accuracy by analyzing content.
    /// </summary>
    public static int EstimateAdaptive(string text)
    {
        if (string.IsNullOrEmpty(text))
            return 0;

        // Count different character types
        int letters = 0, digits = 0, symbols = 0, spaces = 0;

        foreach (var c in text)
        {
            if (char.IsLetter(c))
                letters++;
            else if (char.IsDigit(c))
                digits++;
            else if (char.IsWhiteSpace(c))
                spaces++;
            else
                symbols++;
        }

        var total = text.Length;

        // Adjust ratio based on content type
        double charsPerToken;

        if (symbols > total * 0.15)
        {
            // Lots of symbols = likely code
            charsPerToken = 3.5;
        }
        else if (digits > total * 0.2)
        {
            // Lots of numbers = data/config
            charsPerToken = 3.0;
        }
        else
        {
            // Mostly text
            charsPerToken = 4.5;
        }

        return (int)Math.Ceiling(total / charsPerToken);
    }

    /// <summary>
    /// Calculate how many tokens fit in a given character limit.
    /// </summary>
    public static int CharactersToTokens(int characterCount, TokenEstimationMode mode = TokenEstimationMode.Code)
    {
        var charsPerToken = mode switch
        {
            TokenEstimationMode.Code => 4.0,
            TokenEstimationMode.Text => 4.5,
            TokenEstimationMode.Mixed => 4.0,
            _ => 4.0
        };

        return (int)Math.Ceiling(characterCount / charsPerToken);
    }

    /// <summary>
    /// Calculate character count for a target token count.
    /// </summary>
    public static int TokensToCharacters(int tokenCount, TokenEstimationMode mode = TokenEstimationMode.Code)
    {
        var charsPerToken = mode switch
        {
            TokenEstimationMode.Code => 4.0,
            TokenEstimationMode.Text => 4.5,
            TokenEstimationMode.Mixed => 4.0,
            _ => 4.0
        };

        return (int)(tokenCount * charsPerToken);
    }
}

/// <summary>
/// Mode for token estimation.
/// </summary>
public enum TokenEstimationMode
{
    /// <summary>
    /// Code/technical content (~4 chars/token).
    /// </summary>
    Code,

    /// <summary>
    /// Natural language text (~4.5 chars/token).
    /// </summary>
    Text,

    /// <summary>
    /// Mixed content (~4 chars/token).
    /// </summary>
    Mixed
}
```

### Acceptance Criteria (v0.7.1i)
- [ ] SimpleTokenizer provides basic tokenization
- [ ] HuggingFaceTokenizerService loads tokenizer.json
- [ ] TokenizerFactory selects appropriate tokenizer
- [ ] TokenEstimator provides quick estimates
- [ ] Adaptive estimation based on content type

---

## v0.7.1j: Unit Testing & Integration

### Objective
Create comprehensive tests for all embedding and chunking functionality.

### File: `tests/SeniorIntern.Tests.Unit/Embedding/EmbeddingMathTests.cs`

```csharp
using SeniorIntern.Services.Embedding;
using Xunit;

namespace SeniorIntern.Tests.Unit.Embedding;

public class EmbeddingMathTests
{
    [Fact]
    public void CosineSimilarity_IdenticalVectors_ReturnsOne()
    {
        var v1 = new float[] { 1, 2, 3, 4, 5 };
        var v2 = new float[] { 1, 2, 3, 4, 5 };

        var similarity = EmbeddingMath.CosineSimilarity(v1, v2);

        Assert.Equal(1.0f, similarity, precision: 5);
    }

    [Fact]
    public void CosineSimilarity_OrthogonalVectors_ReturnsZero()
    {
        var v1 = new float[] { 1, 0, 0 };
        var v2 = new float[] { 0, 1, 0 };

        var similarity = EmbeddingMath.CosineSimilarity(v1, v2);

        Assert.Equal(0.0f, similarity, precision: 5);
    }

    [Fact]
    public void CosineSimilarity_OppositeVectors_ReturnsNegativeOne()
    {
        var v1 = new float[] { 1, 2, 3 };
        var v2 = new float[] { -1, -2, -3 };

        var similarity = EmbeddingMath.CosineSimilarity(v1, v2);

        Assert.Equal(-1.0f, similarity, precision: 5);
    }

    [Fact]
    public void Normalize_CreatesUnitVector()
    {
        var v = new float[] { 3, 4 };

        EmbeddingMath.Normalize(v);

        var magnitude = MathF.Sqrt(v[0] * v[0] + v[1] * v[1]);
        Assert.Equal(1.0f, magnitude, precision: 5);
    }

    [Fact]
    public void DotProduct_ReturnsCorrectValue()
    {
        var v1 = new float[] { 1, 2, 3 };
        var v2 = new float[] { 4, 5, 6 };

        var dot = EmbeddingMath.DotProduct(v1, v2);

        Assert.Equal(32.0f, dot); // 1*4 + 2*5 + 3*6 = 32
    }
}
```

### File: `tests/SeniorIntern.Tests.Unit/Chunking/ChunkingServiceTests.cs`

```csharp
using Microsoft.Extensions.Logging.Abstractions;
using SeniorIntern.Core.Models;
using SeniorIntern.Services.Chunking;
using Xunit;

namespace SeniorIntern.Tests.Unit.Chunking;

public class ChunkingServiceTests
{
    private readonly ChunkingService _service;

    public ChunkingServiceTests()
    {
        var strategies = new IChunkingStrategy[]
        {
            new PlainTextChunkingStrategy(
                NullLogger<PlainTextChunkingStrategy>.Instance, null!)
        };

        _service = new ChunkingService(
            NullLogger<ChunkingService>.Instance,
            strategies,
            null);
    }

    [Theory]
    [InlineData(".cs", "csharp")]
    [InlineData(".ts", "typescript")]
    [InlineData(".py", "python")]
    [InlineData(".go", "go")]
    [InlineData(".md", "markdown")]
    public void DetectLanguage_ReturnsCorrectLanguage(string extension, string expected)
    {
        var language = _service.DetectLanguage($"test{extension}");

        Assert.Equal(expected, language);
    }

    [Fact]
    public void EstimateTokenCount_ReturnsReasonableEstimate()
    {
        var text = "Hello, this is a test sentence.";

        var estimate = _service.EstimateTokenCount(text);

        // Should be roughly text.Length / 4
        Assert.InRange(estimate, 5, 15);
    }

    [Fact]
    public void Preprocess_NormalizesWhitespace()
    {
        var options = new ChunkPreprocessing
        {
            NormalizeWhitespace = true
        };
        var content = "line1\r\nline2\tindented";

        var result = _service.Preprocess(content, options);

        Assert.Contains("\n", result);
        Assert.DoesNotContain("\r", result);
        Assert.Contains("    ", result); // Tab converted to spaces
    }

    [Fact]
    public void Preprocess_CollapsesMultipleNewlines()
    {
        var options = new ChunkPreprocessing
        {
            CollapseMultipleNewlines = true,
            MaxConsecutiveEmptyLines = 2
        };
        var content = "line1\n\n\n\n\nline2";

        var result = _service.Preprocess(content, options);

        Assert.DoesNotContain("\n\n\n\n", result);
    }
}
```

### File: `tests/SeniorIntern.Tests.Unit/Chunking/CodeAwareChunkingTests.cs`

```csharp
using Microsoft.Extensions.Logging.Abstractions;
using SeniorIntern.Core.Models;
using SeniorIntern.Services.Chunking;
using Xunit;

namespace SeniorIntern.Tests.Unit.Chunking;

public class CodeAwareChunkingTests
{
    [Fact]
    public void Chunk_CSharpClass_ExtractsClassChunk()
    {
        var strategy = CreateStrategy();
        var code = @"
namespace Test
{
    public class MyClass
    {
        public void MyMethod()
        {
        }
    }
}";

        var chunks = strategy.Chunk(code, "test.cs", new ChunkingOptions());

        Assert.NotEmpty(chunks);
        Assert.Contains(chunks, c => c.SymbolName == "MyClass");
    }

    [Fact]
    public void Chunk_TypeScriptFunction_ExtractsFunctionChunk()
    {
        var strategy = CreateStrategy();
        var code = @"
export function calculateSum(a: number, b: number): number {
    return a + b;
}";

        var chunks = strategy.Chunk(code, "test.ts", new ChunkingOptions());

        Assert.NotEmpty(chunks);
        Assert.Contains(chunks, c => c.SymbolType == SymbolType.Function);
    }

    [Fact]
    public void Chunk_PythonFunction_UsesPythonStrategy()
    {
        var strategy = new PythonChunkingStrategy(
            NullLogger<PythonChunkingStrategy>.Instance,
            CreateMockChunkingService());

        var code = @"
def my_function(x, y):
    '''Calculate the sum'''
    return x + y

class MyClass:
    def __init__(self):
        pass
";

        var chunks = strategy.Chunk(code, "test.py", new ChunkingOptions());

        Assert.NotEmpty(chunks);
        Assert.Contains(chunks, c => c.SymbolName == "my_function");
        Assert.Contains(chunks, c => c.SymbolName == "MyClass");
    }

    [Fact]
    public void Chunk_LargeFile_SplitsIntoMultipleChunks()
    {
        var strategy = CreateStrategy();
        var code = string.Join("\n", Enumerable.Range(1, 1000)
            .Select(i => $"// Line {i}"));

        var options = new ChunkingOptions
        {
            TargetChunkSize = 100,
            MaxChunkSize = 200
        };

        var chunks = strategy.Chunk(code, "test.cs", options);

        Assert.True(chunks.Count > 1);
    }

    [Fact]
    public void Chunk_WithOverlap_ChunksOverlap()
    {
        var strategy = CreateStrategy();
        var code = string.Join("\n", Enumerable.Range(1, 100)
            .Select(i => $"// Line {i}"));

        var options = new ChunkingOptions
        {
            TargetChunkSize = 20,
            ChunkOverlap = 5
        };

        var chunks = strategy.Chunk(code, "test.cs", options);

        // Check that later chunks have overlap flag
        Assert.Contains(chunks, c => c.HasOverlap);
    }

    private static CodeAwareChunkingStrategy CreateStrategy()
    {
        return new CodeAwareChunkingStrategy(
            NullLogger<CodeAwareChunkingStrategy>.Instance,
            CreateMockChunkingService());
    }

    private static IChunkingService CreateMockChunkingService()
    {
        var strategies = new IChunkingStrategy[]
        {
            new PlainTextChunkingStrategy(
                NullLogger<PlainTextChunkingStrategy>.Instance, null!)
        };

        return new ChunkingService(
            NullLogger<ChunkingService>.Instance,
            strategies,
            null);
    }
}
```

### File: `tests/SeniorIntern.Tests.Unit/Chunking/MarkdownChunkingTests.cs`

```csharp
using Microsoft.Extensions.Logging.Abstractions;
using SeniorIntern.Core.Models;
using SeniorIntern.Services.Chunking;
using Xunit;

namespace SeniorIntern.Tests.Unit.Chunking;

public class MarkdownChunkingTests
{
    [Fact]
    public void Chunk_MarkdownWithHeadings_SplitsByHeadings()
    {
        var service = CreateMockChunkingService();
        var strategy = new MarkdownChunkingStrategy(
            NullLogger<MarkdownChunkingStrategy>.Instance,
            service);

        var content = @"
# Introduction

This is the intro.

## Section One

Content for section one.

## Section Two

Content for section two.
";

        var chunks = strategy.Chunk(content, "test.md", new ChunkingOptions());

        Assert.True(chunks.Count >= 2);
        Assert.Contains(chunks, c => c.SymbolName == "Introduction");
        Assert.Contains(chunks, c => c.SymbolName == "Section One");
    }

    [Fact]
    public void Chunk_SmallMarkdown_SingleChunk()
    {
        var service = CreateMockChunkingService();
        var strategy = new MarkdownChunkingStrategy(
            NullLogger<MarkdownChunkingStrategy>.Instance,
            service);

        var content = @"
# Title

A short paragraph.
";

        var chunks = strategy.Chunk(content, "test.md", new ChunkingOptions
        {
            MinChunkSize = 1
        });

        Assert.Single(chunks);
        Assert.Equal(ChunkType.Markdown, chunks[0].Type);
    }

    private static IChunkingService CreateMockChunkingService()
    {
        return new ChunkingService(
            NullLogger<ChunkingService>.Instance,
            Array.Empty<IChunkingStrategy>(),
            null);
    }
}
```

### File: `tests/SeniorIntern.Tests.Unit/Embedding/TokenEstimatorTests.cs`

```csharp
using SeniorIntern.Services.Embedding;
using Xunit;

namespace SeniorIntern.Tests.Unit.Embedding;

public class TokenEstimatorTests
{
    [Fact]
    public void EstimateForCode_ReturnsReasonableEstimate()
    {
        var code = "public void Method() { }"; // 24 chars

        var estimate = TokenEstimator.EstimateForCode(code);

        Assert.Equal(6, estimate); // 24/4 = 6
    }

    [Fact]
    public void EstimateForText_ReturnsReasonableEstimate()
    {
        var text = "Hello, this is a test."; // 22 chars

        var estimate = TokenEstimator.EstimateForText(text);

        Assert.Equal(5, estimate); // ceil(22/4.5) = 5
    }

    [Fact]
    public void EstimateAdaptive_HighSymbolContent_UsesLowerRatio()
    {
        var code = "a + b * c / d - e % f";

        var estimate = TokenEstimator.EstimateAdaptive(code);

        // Should use lower chars-per-token for symbol-heavy content
        Assert.True(estimate > code.Length / 5);
    }

    [Fact]
    public void CharactersToTokens_ConvertCorrectly()
    {
        var chars = 100;

        var tokens = TokenEstimator.CharactersToTokens(chars, TokenEstimationMode.Code);

        Assert.Equal(25, tokens); // 100/4 = 25
    }

    [Fact]
    public void TokensToCharacters_ConvertCorrectly()
    {
        var tokens = 25;

        var chars = TokenEstimator.TokensToCharacters(tokens, TokenEstimationMode.Code);

        Assert.Equal(100, chars); // 25*4 = 100
    }
}
```

### File: `tests/SeniorIntern.Tests.Integration/Embedding/LlamaEmbeddingIntegrationTests.cs`

```csharp
using Microsoft.Extensions.DependencyInjection;
using SeniorIntern.Core.Interfaces;
using SeniorIntern.Core.Models;
using Xunit;

namespace SeniorIntern.Tests.Integration.Embedding;

/// <summary>
/// Integration tests for LlamaEmbeddingService.
/// Requires an actual embedding model to run.
/// </summary>
[Trait("Category", "Integration")]
public class LlamaEmbeddingIntegrationTests : IAsyncLifetime
{
    private IEmbeddingService? _service;
    private readonly string? _modelPath;

    public LlamaEmbeddingIntegrationTests()
    {
        _modelPath = Environment.GetEnvironmentVariable("EMBEDDING_MODEL_PATH");
    }

    public async Task InitializeAsync()
    {
        if (_modelPath is null)
            return;

        var services = new ServiceCollection();
        services.AddLogging();
        services.AddEmbeddingServices();

        var provider = services.BuildServiceProvider();
        _service = provider.GetRequiredService<LlamaEmbeddingService>();

        await _service.LoadModelAsync(new EmbeddingModelOptions
        {
            ModelPath = _modelPath,
            ModelName = "Test Model"
        });
    }

    public async Task DisposeAsync()
    {
        if (_service is not null)
            await _service.DisposeAsync();
    }

    [Fact]
    public async Task EmbedAsync_GeneratesEmbedding()
    {
        Skip.If(_service is null, "No embedding model available");

        var embedding = await _service!.EmbedAsync("Hello, world!");

        Assert.NotNull(embedding);
        Assert.Equal(_service.EmbeddingDimension, embedding.Length);
    }

    [Fact]
    public async Task EmbedBatchAsync_GeneratesMultipleEmbeddings()
    {
        Skip.If(_service is null, "No embedding model available");

        var texts = new[] { "Hello", "World", "Test" };

        var embeddings = await _service!.EmbedBatchAsync(texts);

        Assert.Equal(3, embeddings.Count);
        Assert.All(embeddings, e => Assert.Equal(_service.EmbeddingDimension, e.Length));
    }

    [Fact]
    public async Task SimilarTexts_HaveHighSimilarity()
    {
        Skip.If(_service is null, "No embedding model available");

        var e1 = await _service!.EmbedAsync("How to implement a REST API in C#");
        var e2 = await _service.EmbedAsync("Creating RESTful services with C#");
        var e3 = await _service.EmbedAsync("The history of the Roman Empire");

        var similarity12 = _service.CosineSimilarity(e1, e2);
        var similarity13 = _service.CosineSimilarity(e1, e3);

        Assert.True(similarity12 > similarity13, "Similar texts should have higher similarity");
    }
}
```

### Acceptance Criteria (v0.7.1j)
- [ ] EmbeddingMath unit tests pass
- [ ] ChunkingService unit tests pass
- [ ] CodeAwareChunking tests pass for multiple languages
- [ ] MarkdownChunking tests pass
- [ ] TokenEstimator tests pass
- [ ] Integration tests run with actual model (when available)
- [ ] Similar texts show higher similarity than dissimilar texts

---

## File Summary

### Files to Create (40 total)

| File | Sub-version |
|------|-------------|
| `src/SeniorIntern.Core/Interfaces/IEmbeddingService.cs` | v0.7.1a |
| `src/SeniorIntern.Core/Models/EmbeddingModels.cs` | v0.7.1a |
| `src/SeniorIntern.Core/Models/EmbeddingModelInfo.cs` | v0.7.1a |
| `src/SeniorIntern.Core/Interfaces/IEmbeddingModelRegistry.cs` | v0.7.1a |
| `src/SeniorIntern.Services/Embedding/LlamaEmbeddingService.cs` | v0.7.1b |
| `src/SeniorIntern.Services/Embedding/EmbeddingServiceFactory.cs` | v0.7.1b |
| `src/SeniorIntern.Services/Embedding/EmbeddingServiceExtensions.cs` | v0.7.1b |
| `src/SeniorIntern.Services/Embedding/OnnxEmbeddingService.cs` | v0.7.1c |
| `src/SeniorIntern.Services/Embedding/EmbeddingMath.cs` | v0.7.1c |
| `src/SeniorIntern.Core/Interfaces/ITokenizerService.cs` | v0.7.1c |
| `src/SeniorIntern.Services/Embedding/HuggingFaceTokenizerService.cs` | v0.7.1c |
| `src/SeniorIntern.Core/Models/TextChunk.cs` | v0.7.1d |
| `src/SeniorIntern.Core/Models/ChunkType.cs` | v0.7.1d |
| `src/SeniorIntern.Core/Models/ChunkingOptions.cs` | v0.7.1d |
| `src/SeniorIntern.Core/Interfaces/IChunkingService.cs` | v0.7.1e |
| `src/SeniorIntern.Core/Interfaces/IChunkingStrategy.cs` | v0.7.1e |
| `src/SeniorIntern.Services/Chunking/ChunkingService.cs` | v0.7.1e |
| `src/SeniorIntern.Services/Chunking/CodeAwareChunkingStrategy.cs` | v0.7.1f |
| `src/SeniorIntern.Services/Chunking/ChunkingStrategyExtensions.cs` | v0.7.1f |
| `src/SeniorIntern.Services/Chunking/CSharpChunkingStrategy.cs` | v0.7.1g |
| `src/SeniorIntern.Services/Chunking/TypeScriptChunkingStrategy.cs` | v0.7.1g |
| `src/SeniorIntern.Services/Chunking/PythonChunkingStrategy.cs` | v0.7.1g |
| `src/SeniorIntern.Services/Chunking/MarkdownChunkingStrategy.cs` | v0.7.1g |
| `src/SeniorIntern.Services/Chunking/PlainTextChunkingStrategy.cs` | v0.7.1g |
| `src/SeniorIntern.Services/Chunking/ConfigFileChunkingStrategy.cs` | v0.7.1h |
| `src/SeniorIntern.Services/Embedding/SimpleTokenizer.cs` | v0.7.1i |
| `src/SeniorIntern.Services/Embedding/TokenizerFactory.cs` | v0.7.1i |
| `src/SeniorIntern.Services/Embedding/TokenEstimator.cs` | v0.7.1i |
| `tests/SeniorIntern.Tests.Unit/Embedding/EmbeddingMathTests.cs` | v0.7.1j |
| `tests/SeniorIntern.Tests.Unit/Chunking/ChunkingServiceTests.cs` | v0.7.1j |
| `tests/SeniorIntern.Tests.Unit/Chunking/CodeAwareChunkingTests.cs` | v0.7.1j |
| `tests/SeniorIntern.Tests.Unit/Chunking/MarkdownChunkingTests.cs` | v0.7.1j |
| `tests/SeniorIntern.Tests.Unit/Embedding/TokenEstimatorTests.cs` | v0.7.1j |
| `tests/SeniorIntern.Tests.Integration/Embedding/LlamaEmbeddingIntegrationTests.cs` | v0.7.1j |

### Files to Modify (2 total)

| File | Sub-version | Changes |
|------|-------------|---------|
| `src/SeniorIntern.Services/SeniorIntern.Services.csproj` | v0.7.1b | Add LLamaSharp package |
| `src/SeniorIntern.Services/SeniorIntern.Services.csproj` | v0.7.1c | Add ONNX Runtime packages |

---

## NuGet Packages

| Package | Version | Purpose |
|---------|---------|---------|
| `LLamaSharp` | 0.18.0 | GGUF model loading and embedding |
| `LLamaSharp.Backend.Cpu` | 0.18.0 | CPU backend (conditional) |
| `Microsoft.ML.OnnxRuntime` | 1.17.0 | ONNX model inference |
| `Microsoft.ML.OnnxRuntime.Managed` | 1.17.0 | Managed ONNX Runtime |
| `Microsoft.ML.OnnxRuntime.Gpu` | 1.17.0 | GPU acceleration (optional) |

---

## Acceptance Criteria Summary

### v0.7.1 Complete
- [ ] Embedding service loads GGUF models via LLamaSharp
- [ ] Embedding service loads ONNX models via ONNX Runtime
- [ ] Batch embedding generation with progress reporting
- [ ] SIMD-optimized cosine similarity and normalization
- [ ] Tokenizer support for HuggingFace tokenizer.json
- [ ] Code-aware chunking extracts symbols from code
- [ ] Language-specific chunkers for C#, TypeScript, Python
- [ ] Markdown chunking splits by headings
- [ ] Plain text fallback chunking
- [ ] Token estimation without full tokenizer
- [ ] All unit tests pass
- [ ] Integration tests pass with real models
