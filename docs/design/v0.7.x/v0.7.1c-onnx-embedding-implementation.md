# Design Specification: AIntern v0.7.1c "ONNX Embedding Implementation"

## Overview

**Version**: v0.7.1c
**Parent**: v0.7.1 Embedding Foundation
**Focus**: Implementing ONNX Runtime embedding generation with HuggingFace tokenizer support

### Purpose

This sub-version provides an alternative embedding service implementation using ONNX Runtime:
1. `OnnxEmbeddingService` - Full implementation of `IEmbeddingService` for ONNX models
2. `ITokenizerService` - Interface for text tokenization
3. `HuggingFaceTokenizerService` - Tokenizer for HuggingFace-style tokenizer.json files
4. `EmbeddingMath` - SIMD-optimized vector operations shared across services
5. Package configuration for ONNX Runtime in the Services project

### Dependencies

**From v0.7.1a (Embedding Service Interface)**:
- `IEmbeddingService` interface contract
- `EmbeddingModelOptions` configuration class
- `EmbeddingModelType` and `EmbeddingPoolingType` enums
- `ModelLoadProgress` and `EmbeddingProgress` models
- `EmbeddingModelStateChangedEventArgs` event arguments

**From v0.7.1b (LLamaSharp Implementation)**:
- `EmbeddingServiceFactory` (to be updated with ONNX support)
- `EmbeddingServiceExtensions` (already registers OnnxEmbeddingService)

**External Dependencies**:
- Microsoft.ML.OnnxRuntime (v1.17.0) - ONNX model inference
- Microsoft.ML.OnnxRuntime.Managed (v1.17.0) - Managed API
- Microsoft.ML.OnnxRuntime.Gpu (v1.17.0) - GPU support (conditional)
- System.Numerics - SIMD-accelerated vector operations
- System.Text.Json - Tokenizer JSON parsing
- Microsoft.Extensions.Logging - Structured logging

---

## Architecture

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                   v0.7.1c ONNX Embedding Implementation                       │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │                        Service Implementation Layer                       │ │
│  │  src/AIntern.Services/Embedding/                                         │ │
│  ├─────────────────────────────────────────────────────────────────────────┤ │
│  │                                                                          │ │
│  │  ┌────────────────────────────────────────────────────────────────────┐ │ │
│  │  │                      OnnxEmbeddingService                           │ │ │
│  │  │  Implements: IEmbeddingService                                      │ │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐   │ │ │
│  │  │  │  Private Fields:                                             │   │ │ │
│  │  │  │  ├── _logger: ILogger<OnnxEmbeddingService>                 │   │ │ │
│  │  │  │  ├── _tokenizerService: ITokenizerService                   │   │ │ │
│  │  │  │  ├── _loadLock: SemaphoreSlim (thread-safe loading)         │   │ │ │
│  │  │  │  ├── _embedLock: SemaphoreSlim (thread-safe embedding)      │   │ │ │
│  │  │  │  ├── _session: InferenceSession?                            │   │ │ │
│  │  │  │  ├── _inputNames: string[]?                                 │   │ │ │
│  │  │  │  ├── _outputName: string?                                   │   │ │ │
│  │  │  │  ├── _currentOptions: EmbeddingModelOptions?                │   │ │ │
│  │  │  │  └── _isDisposed: bool                                      │   │ │ │
│  │  │  └─────────────────────────────────────────────────────────────┘   │ │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐   │ │ │
│  │  │  │  Model Lifecycle:                                            │   │ │ │
│  │  │  │  ├── LoadModelAsync() → InferenceSession + Tokenizer        │   │ │ │
│  │  │  │  ├── UnloadModelAsync() → Dispose session                   │   │ │ │
│  │  │  │  └── UnloadModelInternalAsync() → Internal cleanup          │   │ │ │
│  │  │  └─────────────────────────────────────────────────────────────┘   │ │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐   │ │ │
│  │  │  │  Embedding Generation:                                       │   │ │ │
│  │  │  │  ├── EmbedAsync() → Single text embedding                    │   │ │ │
│  │  │  │  ├── EmbedBatchAsync() → Batch with progress reporting       │   │ │ │
│  │  │  │  ├── CreateInputTensors() → Single sequence tensors          │   │ │ │
│  │  │  │  └── CreateBatchedInputTensors() → Batched + padded tensors  │   │ │ │
│  │  │  └─────────────────────────────────────────────────────────────┘   │ │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐   │ │ │
│  │  │  │  Pooling Strategies:                                         │   │ │ │
│  │  │  │  ├── ExtractEmbedding() → Single sequence pooling            │   │ │ │
│  │  │  │  ├── ExtractBatchEmbedding() → Batch item pooling            │   │ │ │
│  │  │  │  └── ApplyPooling() → Mean/CLS/Last/Max strategies           │   │ │ │
│  │  │  └─────────────────────────────────────────────────────────────┘   │ │ │
│  │  └────────────────────────────────────────────────────────────────────┘ │ │
│  │                                                                          │ │
│  │  ┌────────────────────────────────────────────────────────────────────┐ │ │
│  │  │                        EmbeddingMath                                │ │ │
│  │  │  Static class with SIMD-accelerated operations                      │ │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐   │ │ │
│  │  │  │  Static Methods:                                             │   │ │ │
│  │  │  │  ├── CosineSimilarity() → SIMD dot product / magnitude       │   │ │ │
│  │  │  │  ├── Normalize() → In-place unit length normalization        │   │ │ │
│  │  │  │  ├── EuclideanDistance() → L2 distance calculation           │   │ │ │
│  │  │  │  └── DotProduct() → SIMD-accelerated dot product             │   │ │ │
│  │  │  └─────────────────────────────────────────────────────────────┘   │ │ │
│  │  └────────────────────────────────────────────────────────────────────┘ │ │
│  │                                                                          │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │                        Tokenizer Layer                                    │ │
│  │  src/AIntern.Core/Interfaces/ + src/AIntern.Services/Embedding/          │ │
│  ├─────────────────────────────────────────────────────────────────────────┤ │
│  │                                                                          │ │
│  │  ┌────────────────────────────────────────────────────────────────────┐ │ │
│  │  │                      ITokenizerService                              │ │ │
│  │  │  Interface for text tokenization                                    │ │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐   │ │ │
│  │  │  │  Properties:                                                 │   │ │ │
│  │  │  │  └── IsLoaded: bool                                          │   │ │ │
│  │  │  ├─────────────────────────────────────────────────────────────┤   │ │ │
│  │  │  │  Methods:                                                    │   │ │ │
│  │  │  │  ├── LoadTokenizerAsync(path, ct)                            │   │ │ │
│  │  │  │  ├── Encode(text, maxLength) → TokenizedText                 │   │ │ │
│  │  │  │  ├── Decode(tokenIds) → string                               │   │ │ │
│  │  │  │  └── CountTokens(text) → int                                 │   │ │ │
│  │  │  └─────────────────────────────────────────────────────────────┘   │ │ │
│  │  └────────────────────────────────────────────────────────────────────┘ │ │
│  │                                                                          │ │
│  │  ┌────────────────────────────────────────────────────────────────────┐ │ │
│  │  │                      TokenizedText                                  │ │ │
│  │  │  Model class for tokenization results                               │ │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐   │ │ │
│  │  │  │  Properties:                                                 │   │ │ │
│  │  │  │  ├── InputIds: long[]                                        │   │ │ │
│  │  │  │  ├── AttentionMask: long[]                                   │   │ │ │
│  │  │  │  └── TokenTypeIds: long[]?                                   │   │ │ │
│  │  │  └─────────────────────────────────────────────────────────────┘   │ │ │
│  │  └────────────────────────────────────────────────────────────────────┘ │ │
│  │                                                                          │ │
│  │  ┌────────────────────────────────────────────────────────────────────┐ │ │
│  │  │                  HuggingFaceTokenizerService                        │ │ │
│  │  │  Implements: ITokenizerService                                      │ │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐   │ │ │
│  │  │  │  Private Fields:                                             │   │ │ │
│  │  │  │  ├── _logger: ILogger<HuggingFaceTokenizerService>          │   │ │ │
│  │  │  │  ├── _vocab: Dictionary<string, int>?                        │   │ │ │
│  │  │  │  ├── _reverseVocab: Dictionary<int, string>?                 │   │ │ │
│  │  │  │  ├── _padTokenId: int (default: 0)                           │   │ │ │
│  │  │  │  ├── _clsTokenId: int (default: 101)                         │   │ │ │
│  │  │  │  ├── _sepTokenId: int (default: 102)                         │   │ │ │
│  │  │  │  └── _unkTokenId: int (default: 100)                         │   │ │ │
│  │  │  └─────────────────────────────────────────────────────────────┘   │ │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐   │ │ │
│  │  │  │  Tokenization:                                               │   │ │ │
│  │  │  │  ├── LoadTokenizerAsync() → Parse tokenizer.json             │   │ │ │
│  │  │  │  ├── Encode() → [CLS] + tokens + [SEP]                       │   │ │ │
│  │  │  │  ├── TokenizeWordPiece() → ##subword tokenization            │   │ │ │
│  │  │  │  ├── Decode() → Reconstruct text from tokens                 │   │ │ │
│  │  │  │  └── CountTokens() → Token count estimation                  │   │ │ │
│  │  │  └─────────────────────────────────────────────────────────────┘   │ │ │
│  │  └────────────────────────────────────────────────────────────────────┘ │ │
│  │                                                                          │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │                        ONNX Runtime Integration                          │ │
│  │  NuGet Packages                                                          │ │
│  ├─────────────────────────────────────────────────────────────────────────┤ │
│  │                                                                          │ │
│  │  ┌────────────────────────────────────────────────────────────────────┐ │ │
│  │  │  Package Dependencies                                               │ │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐   │ │ │
│  │  │  │  Microsoft.ML.OnnxRuntime (v1.17.0)                          │   │ │ │
│  │  │  │  ├── InferenceSession - Model loading and inference          │   │ │ │
│  │  │  │  ├── SessionOptions - Execution provider configuration       │   │ │ │
│  │  │  │  └── NamedOnnxValue - Input/output tensor handling           │   │ │ │
│  │  │  └─────────────────────────────────────────────────────────────┘   │ │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐   │ │ │
│  │  │  │  Microsoft.ML.OnnxRuntime.Managed (v1.17.0)                  │   │ │ │
│  │  │  │  └── DenseTensor<T> - Managed tensor creation                │   │ │ │
│  │  │  └─────────────────────────────────────────────────────────────┘   │ │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐   │ │ │
│  │  │  │  Microsoft.ML.OnnxRuntime.Gpu (v1.17.0) [Conditional]        │   │ │ │
│  │  │  │  ├── CUDA execution provider (Windows/Linux)                 │   │ │ │
│  │  │  │  └── Condition: win-x64 OR linux-x64                         │   │ │ │
│  │  │  └─────────────────────────────────────────────────────────────┘   │ │ │
│  │  └────────────────────────────────────────────────────────────────────┘ │ │
│  │                                                                          │ │
│  │  ┌────────────────────────────────────────────────────────────────────┐ │ │
│  │  │  Execution Providers                                                │ │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐   │ │ │
│  │  │  │                                                              │   │ │ │
│  │  │  │  Platform     │ Provider     │ Method                        │   │ │ │
│  │  │  │  ─────────────┼──────────────┼──────────────────────────────│   │ │ │
│  │  │  │  Windows x64  │ CUDA         │ AppendExecutionProvider_CUDA  │   │ │ │
│  │  │  │  Linux x64    │ CUDA         │ AppendExecutionProvider_CUDA  │   │ │ │
│  │  │  │  macOS        │ CoreML       │ AppendExecutionProvider_CoreML│   │ │ │
│  │  │  │  (fallback)   │ CPU          │ Default (no explicit append)  │   │ │ │
│  │  │  │                                                              │   │ │ │
│  │  │  └─────────────────────────────────────────────────────────────┘   │ │ │
│  │  └────────────────────────────────────────────────────────────────────┘ │ │
│  │                                                                          │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
└──────────────────────────────────────────────────────────────────────────────┘
```

---

## Pooling Strategies

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                        Embedding Pooling Strategies                           │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  Transformer Output Shape                                                │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │                                                                    │  │ │
│  │  │  Input: "Hello world"                                              │  │ │
│  │  │  Tokens: [CLS] Hello world [SEP]                                   │  │ │
│  │  │                                                                    │  │ │
│  │  │  Transformer Output Shape: [batch, seq_len, hidden_dim]            │  │ │
│  │  │                                                                    │  │ │
│  │  │  Example: [1, 4, 768]                                              │  │ │
│  │  │                                                                    │  │ │
│  │  │     Token 0: [CLS]   → [v0_0, v0_1, ..., v0_767]  (768 dims)       │  │ │
│  │  │     Token 1: Hello   → [v1_0, v1_1, ..., v1_767]  (768 dims)       │  │ │
│  │  │     Token 2: world   → [v2_0, v2_1, ..., v2_767]  (768 dims)       │  │ │
│  │  │     Token 3: [SEP]   → [v3_0, v3_1, ..., v3_767]  (768 dims)       │  │ │
│  │  │                                                                    │  │ │
│  │  │  Goal: Reduce [4, 768] → [768] (single embedding)                  │  │ │
│  │  │                                                                    │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  Pooling Type: Mean                                                      │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │                                                                    │  │ │
│  │  │  Description: Average of all token embeddings                      │  │ │
│  │  │                                                                    │  │ │
│  │  │  Formula: embedding[d] = Σ(tensor[s, d]) / seq_len                 │  │ │
│  │  │           for d in 0..hidden_dim, s in 0..seq_len                  │  │ │
│  │  │                                                                    │  │ │
│  │  │  Implementation:                                                   │  │ │
│  │  │  ┌──────────────────────────────────────────────────────────────┐ │  │ │
│  │  │  │  for (int d = 0; d < EmbeddingDimension; d++)                │ │  │ │
│  │  │  │  {                                                           │ │  │ │
│  │  │  │      float sum = 0;                                          │ │  │ │
│  │  │  │      for (int s = 0; s < seqLen; s++)                        │ │  │ │
│  │  │  │          sum += tensor[batchIndex, s, d];                    │ │  │ │
│  │  │  │      embedding[d] = sum / seqLen;                            │ │  │ │
│  │  │  │  }                                                           │ │  │ │
│  │  │  └──────────────────────────────────────────────────────────────┘ │  │ │
│  │  │                                                                    │  │ │
│  │  │  Best for: General semantic similarity, document embedding        │  │ │
│  │  │                                                                    │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  Pooling Type: CLS                                                       │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │                                                                    │  │ │
│  │  │  Description: Use first token ([CLS]) embedding only               │  │ │
│  │  │                                                                    │  │ │
│  │  │  Formula: embedding[d] = tensor[0, d]                              │  │ │
│  │  │                                                                    │  │ │
│  │  │  Implementation:                                                   │  │ │
│  │  │  ┌──────────────────────────────────────────────────────────────┐ │  │ │
│  │  │  │  for (int d = 0; d < EmbeddingDimension; d++)                │ │  │ │
│  │  │  │      embedding[d] = tensor[batchIndex, 0, d];                │ │  │ │
│  │  │  └──────────────────────────────────────────────────────────────┘ │  │ │
│  │  │                                                                    │  │ │
│  │  │  Best for: BERT-style models trained with [CLS] for classification │  │ │
│  │  │                                                                    │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  Pooling Type: Last                                                      │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │                                                                    │  │ │
│  │  │  Description: Use last token embedding                             │  │ │
│  │  │                                                                    │  │ │
│  │  │  Formula: embedding[d] = tensor[seq_len - 1, d]                    │  │ │
│  │  │                                                                    │  │ │
│  │  │  Implementation:                                                   │  │ │
│  │  │  ┌──────────────────────────────────────────────────────────────┐ │  │ │
│  │  │  │  for (int d = 0; d < EmbeddingDimension; d++)                │ │  │ │
│  │  │  │      embedding[d] = tensor[batchIndex, seqLen - 1, d];       │ │  │ │
│  │  │  └──────────────────────────────────────────────────────────────┘ │  │ │
│  │  │                                                                    │  │ │
│  │  │  Best for: Decoder-only models, causal language models            │  │ │
│  │  │                                                                    │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  Pooling Type: Max                                                       │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │                                                                    │  │ │
│  │  │  Description: Maximum value for each dimension across all tokens   │  │ │
│  │  │                                                                    │  │ │
│  │  │  Formula: embedding[d] = max(tensor[s, d]) for s in 0..seq_len     │  │ │
│  │  │                                                                    │  │ │
│  │  │  Implementation:                                                   │  │ │
│  │  │  ┌──────────────────────────────────────────────────────────────┐ │  │ │
│  │  │  │  for (int d = 0; d < EmbeddingDimension; d++)                │ │  │ │
│  │  │  │  {                                                           │ │  │ │
│  │  │  │      float max = float.MinValue;                             │ │  │ │
│  │  │  │      for (int s = 0; s < seqLen; s++)                        │ │  │ │
│  │  │  │          max = Math.Max(max, tensor[batchIndex, s, d]);      │ │  │ │
│  │  │  │      embedding[d] = max;                                     │ │  │ │
│  │  │  │  }                                                           │ │  │ │
│  │  │  └──────────────────────────────────────────────────────────────┘ │  │ │
│  │  │                                                                    │  │ │
│  │  │  Best for: Capturing salient features, sparse representations      │  │ │
│  │  │                                                                    │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  Already Pooled Output                                                   │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │                                                                    │  │ │
│  │  │  Some ONNX models output pre-pooled embeddings:                    │  │ │
│  │  │  Shape: [batch, hidden_dim] instead of [batch, seq_len, hidden_dim]│  │ │
│  │  │                                                                    │  │ │
│  │  │  Detection:                                                        │  │ │
│  │  │  ┌──────────────────────────────────────────────────────────────┐ │  │ │
│  │  │  │  if (dimensions.Length == 2)                                 │ │  │ │
│  │  │  │  {                                                           │ │  │ │
│  │  │  │      // Already pooled: [batch, hidden_dim]                  │ │  │ │
│  │  │  │      var embedding = new float[EmbeddingDimension];          │ │  │ │
│  │  │  │      for (int i = 0; i < EmbeddingDimension; i++)            │ │  │ │
│  │  │  │          embedding[i] = outputTensor[0, i];                  │ │  │ │
│  │  │  │      return embedding;                                       │ │  │ │
│  │  │  │  }                                                           │ │  │ │
│  │  │  │  // else: Need to apply pooling                              │ │  │ │
│  │  │  └──────────────────────────────────────────────────────────────┘ │  │ │
│  │  │                                                                    │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
└──────────────────────────────────────────────────────────────────────────────┘
```

---

## Tokenization Flow

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                      HuggingFace Tokenizer Flow                               │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  Loading tokenizer.json                                                  │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │                                                                    │  │ │
│  │  │  LoadTokenizerAsync(path, ct)                                      │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │  1. Read tokenizer.json file                                │  │  │ │
│  │  │  │     json = await File.ReadAllTextAsync(path, ct)            │  │  │ │
│  │  │  │     doc = JsonDocument.Parse(json)                          │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │  2. Parse vocabulary from "model.vocab"                     │  │  │ │
│  │  │  │     foreach (item in vocab.EnumerateObject())               │  │  │ │
│  │  │  │     {                                                       │  │  │ │
│  │  │  │         _vocab[item.Name] = item.Value.GetInt32();          │  │  │ │
│  │  │  │         _reverseVocab[id] = item.Name;                      │  │  │ │
│  │  │  │     }                                                       │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │  3. Parse special tokens from "added_tokens"                │  │  │ │
│  │  │  │     foreach (token in addedTokens.EnumerateArray())         │  │  │ │
│  │  │  │     {                                                       │  │  │ │
│  │  │  │         if (content == "[PAD]") _padTokenId = id;           │  │  │ │
│  │  │  │         if (content == "[CLS]") _clsTokenId = id;           │  │  │ │
│  │  │  │         if (content == "[SEP]") _sepTokenId = id;           │  │  │ │
│  │  │  │         if (content == "[UNK]") _unkTokenId = id;           │  │  │ │
│  │  │  │     }                                                       │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │                                                                    │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  Encode() Flow                                                           │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │                                                                    │  │ │
│  │  │  Input: "Hello world" (maxLength: 512)                             │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │  1. Initialize with [CLS] token                             │  │  │ │
│  │  │  │     tokens = [101]  // _clsTokenId                          │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │  2. Split into words and process each                       │  │  │ │
│  │  │  │     words = ["Hello", "world"]                              │  │  │ │
│  │  │  │                                                             │  │  │ │
│  │  │  │     For "hello" (normalized):                               │  │  │ │
│  │  │  │       └── Check vocab: found → add token ID                 │  │  │ │
│  │  │  │                                                             │  │  │ │
│  │  │  │     For "world" (normalized):                               │  │  │ │
│  │  │  │       └── Check vocab: found → add token ID                 │  │  │ │
│  │  │  │                                                             │  │  │ │
│  │  │  │     tokens = [101, 7592, 2088]                              │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │  3. Add [SEP] token                                         │  │  │ │
│  │  │  │     tokens = [101, 7592, 2088, 102]  // _sepTokenId         │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │  4. Create attention mask (all 1s for real tokens)          │  │  │ │
│  │  │  │     attentionMask = [1, 1, 1, 1]                            │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │  5. Return TokenizedText                                    │  │  │ │
│  │  │  │     {                                                       │  │  │ │
│  │  │  │       InputIds: [101, 7592, 2088, 102],                     │  │  │ │
│  │  │  │       AttentionMask: [1, 1, 1, 1],                          │  │  │ │
│  │  │  │       TokenTypeIds: null                                    │  │  │ │
│  │  │  │     }                                                       │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │                                                                    │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  WordPiece Tokenization                                                  │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │                                                                    │  │ │
│  │  │  Input: "embedding" (not in vocab)                                 │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │  Algorithm: Greedy longest-match subword tokenization       │  │  │ │
│  │  │  │                                                             │  │  │ │
│  │  │  │  1. Try "embedding" → not found                             │  │  │ │
│  │  │  │  2. Try "embeddin" → not found                              │  │  │ │
│  │  │  │  3. ...                                                     │  │  │ │
│  │  │  │  4. Try "embed" → found! tokenId = 7861                     │  │  │ │
│  │  │  │     remaining = "ding"                                      │  │  │ │
│  │  │  │                                                             │  │  │ │
│  │  │  │  5. Try "##ding" → found! tokenId = 4667                    │  │  │ │
│  │  │  │     remaining = ""                                          │  │  │ │
│  │  │  │                                                             │  │  │ │
│  │  │  │  Result: [7861, 4667] → "embed" + "##ding"                  │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │                                                                    │  │ │
│  │  │  The "##" prefix indicates subword continuation                    │  │ │
│  │  │                                                                    │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
└──────────────────────────────────────────────────────────────────────────────┘
```

---

## Batched Tensor Creation

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                      Batched Input Tensor Creation                            │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  Batch Processing Challenge                                              │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │                                                                    │  │ │
│  │  │  Texts with different lengths:                                     │  │ │
│  │  │  ├── "Hello"        → 3 tokens: [CLS, Hello, SEP]                 │  │ │
│  │  │  ├── "Hello world"  → 4 tokens: [CLS, Hello, world, SEP]          │  │ │
│  │  │  └── "AI assistant" → 5 tokens: [CLS, AI, assist, ##ant, SEP]     │  │ │
│  │  │                                                                    │  │ │
│  │  │  Problem: ONNX requires rectangular tensors (same length per row) │  │ │
│  │  │                                                                    │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  Solution: Padding to Max Length                                         │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │                                                                    │  │ │
│  │  │  Step 1: Find maximum length in batch                              │  │ │
│  │  │  ┌──────────────────────────────────────────────────────────────┐ │  │ │
│  │  │  │  maxLen = batch.Max(t => t.InputIds.Length)  // = 5          │ │  │ │
│  │  │  └──────────────────────────────────────────────────────────────┘ │  │ │
│  │  │                                                                    │  │ │
│  │  │  Step 2: Create padded arrays                                      │  │ │
│  │  │  ┌──────────────────────────────────────────────────────────────┐ │  │ │
│  │  │  │  inputIds      = new long[batchSize * maxLen]  // 3 * 5 = 15 │ │  │ │
│  │  │  │  attentionMask = new long[batchSize * maxLen]  // all 0s     │ │  │ │
│  │  │  └──────────────────────────────────────────────────────────────┘ │  │ │
│  │  │                                                                    │  │ │
│  │  │  Step 3: Copy tokens and set attention mask                        │  │ │
│  │  │  ┌──────────────────────────────────────────────────────────────┐ │  │ │
│  │  │  │                                                              │ │  │ │
│  │  │  │  Row 0 (3 tokens): [CLS, Hello, SEP, PAD, PAD]               │ │  │ │
│  │  │  │  Attention:        [  1,     1,   1,   0,   0]               │ │  │ │
│  │  │  │                                                              │ │  │ │
│  │  │  │  Row 1 (4 tokens): [CLS, Hello, world, SEP, PAD]             │ │  │ │
│  │  │  │  Attention:        [  1,     1,     1,   1,   0]             │ │  │ │
│  │  │  │                                                              │ │  │ │
│  │  │  │  Row 2 (5 tokens): [CLS, AI, assist, ##ant, SEP]             │ │  │ │
│  │  │  │  Attention:        [  1,  1,      1,     1,   1]             │ │  │ │
│  │  │  │                                                              │ │  │ │
│  │  │  └──────────────────────────────────────────────────────────────┘ │  │ │
│  │  │                                                                    │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  CreateBatchedInputTensors Implementation                                │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │                                                                    │  │ │
│  │  │  // Initialize flat arrays (batchSize * maxLen)                    │  │ │
│  │  │  var inputIds = new long[batchSize * maxLen];      // All 0s       │  │ │
│  │  │  var attentionMask = new long[batchSize * maxLen]; // All 0s       │  │ │
│  │  │                                                                    │  │ │
│  │  │  // Copy each sequence into its row                                │  │ │
│  │  │  for (int i = 0; i < batch.Count; i++)                             │  │ │
│  │  │  {                                                                 │  │ │
│  │  │      var tokens = batch[i];                                        │  │ │
│  │  │      var offset = i * maxLen;  // Row start position               │  │ │
│  │  │                                                                    │  │ │
│  │  │      for (int j = 0; j < tokens.InputIds.Length; j++)              │  │ │
│  │  │      {                                                             │  │ │
│  │  │          inputIds[offset + j] = tokens.InputIds[j];                │  │ │
│  │  │          attentionMask[offset + j] = tokens.AttentionMask[j];      │  │ │
│  │  │      }                                                             │  │ │
│  │  │      // Remaining positions stay 0 (padding)                       │  │ │
│  │  │  }                                                                 │  │ │
│  │  │                                                                    │  │ │
│  │  │  // Reshape to [batchSize, maxLen] tensor                          │  │ │
│  │  │  var inputIdsTensor = new DenseTensor<long>(                       │  │ │
│  │  │      inputIds, new[] { batchSize, maxLen });                       │  │ │
│  │  │                                                                    │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  ONNX Input Tensor Names                                                 │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │                                                                    │  │ │
│  │  │  Common input names for transformer models:                        │  │ │
│  │  │                                                                    │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │  "input_ids"       → Token IDs [batch, seq_len]             │  │  │ │
│  │  │  │                       Always required                        │  │  │ │
│  │  │  │                                                              │  │  │ │
│  │  │  │  "attention_mask"  → Real tokens vs padding [batch, seq_len]│  │  │ │
│  │  │  │                       1 = attend, 0 = ignore padding         │  │  │ │
│  │  │  │                                                              │  │  │ │
│  │  │  │  "token_type_ids"  → Segment IDs [batch, seq_len]           │  │  │ │
│  │  │  │                       Usually all 0s for single sequences    │  │  │ │
│  │  │  │                       Used for sentence-pair tasks           │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │                                                                    │  │ │
│  │  │  Dynamic discovery at load time:                                   │  │ │
│  │  │  ┌──────────────────────────────────────────────────────────────┐ │  │ │
│  │  │  │  _inputNames = _session.InputMetadata.Keys.ToArray();       │ │  │ │
│  │  │  │  // Check which inputs are expected                          │ │  │ │
│  │  │  │  if (_inputNames.Contains("attention_mask")) { ... }         │ │  │ │
│  │  │  │  if (_inputNames.Contains("token_type_ids")) { ... }         │ │  │ │
│  │  │  └──────────────────────────────────────────────────────────────┘ │  │ │
│  │  │                                                                    │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
└──────────────────────────────────────────────────────────────────────────────┘
```

---

## SIMD Vector Operations

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                  EmbeddingMath SIMD-Accelerated Operations                    │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  CosineSimilarity Algorithm                                              │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │                                                                    │  │ │
│  │  │  Formula: cos(θ) = (A · B) / (||A|| × ||B||)                       │  │ │
│  │  │                                                                    │  │ │
│  │  │  SIMD Implementation:                                              │  │ │
│  │  │  ┌──────────────────────────────────────────────────────────────┐ │  │ │
│  │  │  │                                                              │ │  │ │
│  │  │  │  if (Vector.IsHardwareAccelerated && len >= Vector<float>.Count)│ │  │ │
│  │  │  │  {                                                           │ │  │ │
│  │  │  │      var vectorSize = Vector<float>.Count;  // 4-16 floats   │ │  │ │
│  │  │  │      var dotVec = Vector<float>.Zero;                        │ │  │ │
│  │  │  │      var normAVec = Vector<float>.Zero;                      │ │  │ │
│  │  │  │      var normBVec = Vector<float>.Zero;                      │ │  │ │
│  │  │  │                                                              │ │  │ │
│  │  │  │      // Process vectorSize floats per iteration              │ │  │ │
│  │  │  │      for (i = 0; i <= len - vectorSize; i += vectorSize)     │ │  │ │
│  │  │  │      {                                                       │ │  │ │
│  │  │  │          var va = new Vector<float>(a.Slice(i, vectorSize)); │ │  │ │
│  │  │  │          var vb = new Vector<float>(b.Slice(i, vectorSize)); │ │  │ │
│  │  │  │                                                              │ │  │ │
│  │  │  │          dotVec += va * vb;      // Parallel multiply-add    │ │  │ │
│  │  │  │          normAVec += va * va;    // Squared norms            │ │  │ │
│  │  │  │          normBVec += vb * vb;                                │ │  │ │
│  │  │  │      }                                                       │ │  │ │
│  │  │  │                                                              │ │  │ │
│  │  │  │      // Horizontal sum of vector elements                    │ │  │ │
│  │  │  │      for (j = 0; j < vectorSize; j++)                        │ │  │ │
│  │  │  │      {                                                       │ │  │ │
│  │  │  │          dot += dotVec[j];                                   │ │  │ │
│  │  │  │          normA += normAVec[j];                               │ │  │ │
│  │  │  │          normB += normBVec[j];                               │ │  │ │
│  │  │  │      }                                                       │ │  │ │
│  │  │  │                                                              │ │  │ │
│  │  │  │      // Handle remaining elements with scalar loop           │ │  │ │
│  │  │  │  }                                                           │ │  │ │
│  │  │  │                                                              │ │  │ │
│  │  │  │  return dot / (sqrt(normA) * sqrt(normB));                   │ │  │ │
│  │  │  │                                                              │ │  │ │
│  │  │  └──────────────────────────────────────────────────────────────┘ │  │ │
│  │  │                                                                    │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  Normalize Algorithm                                                     │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │                                                                    │  │ │
│  │  │  Formula: v_normalized = v / ||v||                                 │  │ │
│  │  │                                                                    │  │ │
│  │  │  Two-pass SIMD Implementation:                                     │  │ │
│  │  │  ┌──────────────────────────────────────────────────────────────┐ │  │ │
│  │  │  │                                                              │ │  │ │
│  │  │  │  // Pass 1: Calculate sum of squares (SIMD)                  │ │  │ │
│  │  │  │  float sumSquares = 0f;                                      │ │  │ │
│  │  │  │  var sumVec = Vector<float>.Zero;                            │ │  │ │
│  │  │  │                                                              │ │  │ │
│  │  │  │  for (i = 0; i <= len - vectorSize; i += vectorSize)         │ │  │ │
│  │  │  │  {                                                           │ │  │ │
│  │  │  │      var v = new Vector<float>(vector.Slice(i, vectorSize)); │ │  │ │
│  │  │  │      sumVec += v * v;                                        │ │  │ │
│  │  │  │  }                                                           │ │  │ │
│  │  │  │                                                              │ │  │ │
│  │  │  │  // Horizontal sum                                           │ │  │ │
│  │  │  │  for (j = 0; j < vectorSize; j++)                            │ │  │ │
│  │  │  │      sumSquares += sumVec[j];                                │ │  │ │
│  │  │  │                                                              │ │  │ │
│  │  │  │  var magnitude = MathF.Sqrt(sumSquares);                     │ │  │ │
│  │  │  │                                                              │ │  │ │
│  │  │  │  // Pass 2: Divide by magnitude (SIMD)                       │ │  │ │
│  │  │  │  var magVec = new Vector<float>(magnitude);                  │ │  │ │
│  │  │  │                                                              │ │  │ │
│  │  │  │  for (i = 0; i <= len - vectorSize; i += vectorSize)         │ │  │ │
│  │  │  │  {                                                           │ │  │ │
│  │  │  │      var v = new Vector<float>(vector.Slice(i, vectorSize)); │ │  │ │
│  │  │  │      (v / magVec).CopyTo(vector.Slice(i, vectorSize));       │ │  │ │
│  │  │  │  }                                                           │ │  │ │
│  │  │  │                                                              │ │  │ │
│  │  │  └──────────────────────────────────────────────────────────────┘ │  │ │
│  │  │                                                                    │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  Performance Impact                                                      │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │                                                                    │  │ │
│  │  │  Instruction Set │ Vector Width │ Floats/op │ Speedup             │  │ │
│  │  │  ────────────────┼──────────────┼───────────┼───────────────────── │  │ │
│  │  │  SSE (x86)       │ 128-bit      │     4     │ ~4x                 │  │ │
│  │  │  AVX2 (x86-64)   │ 256-bit      │     8     │ ~8x                 │  │ │
│  │  │  AVX-512         │ 512-bit      │    16     │ ~16x                │  │ │
│  │  │  ARM NEON        │ 128-bit      │     4     │ ~4x                 │  │ │
│  │  │                                                                    │  │ │
│  │  │  Example: 768-dimension embedding                                  │  │ │
│  │  │  ├── Scalar: 768 iterations                                       │  │ │
│  │  │  ├── AVX2:   768/8 = 96 iterations (8x fewer)                     │  │ │
│  │  │  └── AVX-512: 768/16 = 48 iterations (16x fewer)                  │  │ │
│  │  │                                                                    │  │ │
│  │  │  Critical for:                                                     │  │ │
│  │  │  ├── Batch similarity calculations                                │  │ │
│  │  │  ├── Real-time semantic search                                    │  │ │
│  │  │  └── Embedding normalization during indexing                      │  │ │
│  │  │                                                                    │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
└──────────────────────────────────────────────────────────────────────────────┘
```

---

## Model Loading Flow

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                    OnnxEmbeddingService.LoadModelAsync() Flow                 │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  Loading Sequence                                                        │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │                                                                    │  │ │
│  │  │  LoadModelAsync(options, progress, ct)                             │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │ 1. ObjectDisposedException.ThrowIf(_isDisposed, this)       │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │ 2. Validate ModelType (must be Onnx or Auto)                │  │  │ │
│  │  │  │    Throw ArgumentException if invalid                        │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │ 3. await _loadLock.WaitAsync(ct)                            │  │  │ │
│  │  │  │    Acquire exclusive load access                             │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │ 4. If _session is not null:                                 │  │  │ │
│  │  │  │    await UnloadModelInternalAsync()                          │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │ 5. Report Progress: "Configuring session options" (10%)     │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │ 6. Configure SessionOptions:                                 │  │  │ │
│  │  │  │    var sessionOptions = new SessionOptions();                │  │  │ │
│  │  │  │    ├── IntraOpNumThreads = options.Threads                   │  │  │ │
│  │  │  │    ├── InterOpNumThreads = options.Threads                   │  │  │ │
│  │  │  │    └── GraphOptimizationLevel = ORT_ENABLE_ALL               │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │ 7. Configure GPU execution provider (if GpuLayers != 0):    │  │  │ │
│  │  │  │    ├── Windows/Linux: AppendExecutionProvider_CUDA()         │  │  │ │
│  │  │  │    └── macOS: AppendExecutionProvider_CoreML()               │  │  │ │
│  │  │  │    (Catch and log if GPU not available)                      │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │ 8. Report Progress: "Loading model" (30%)                   │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │ 9. _session = await Task.Run(() =>                          │  │  │ │
│  │  │  │        new InferenceSession(options.ModelPath, sessionOptions),│  │  │ │
│  │  │  │        ct);                                                   │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │ 10. Discover input/output names:                             │  │  │ │
│  │  │  │     _inputNames = _session.InputMetadata.Keys.ToArray()      │  │  │ │
│  │  │  │     _outputName = Find output containing "embedding",        │  │  │ │
│  │  │  │                   "output", or "last_hidden"                 │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │ 11. Determine embedding dimension from output metadata:      │  │  │ │
│  │  │  │     EmbeddingDimension = outputMeta.Dimensions.Last()        │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │ 12. Report Progress: "Loading tokenizer" (70%)               │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │ 13. Load tokenizer from sibling tokenizer.json:              │  │  │ │
│  │  │  │     var tokenizerPath = Path.Combine(                        │  │  │ │
│  │  │  │         Path.GetDirectoryName(options.ModelPath)!,           │  │  │ │
│  │  │  │         "tokenizer.json");                                   │  │  │ │
│  │  │  │     if (File.Exists(tokenizerPath))                          │  │  │ │
│  │  │  │         await _tokenizerService.LoadTokenizerAsync(...)      │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │ 14. Store configuration and report "Model ready" (100%)      │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │ 15. Raise ModelStateChanged event (IsLoaded = true)          │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │ finally: _loadLock.Release()                                 │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │                                                                    │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
└──────────────────────────────────────────────────────────────────────────────┘
```

---

## File Specifications

### 1. ITokenizerService.cs

**Location**: `src/AIntern.Core/Interfaces/ITokenizerService.cs`
**Type**: `interface` + `class`
**Purpose**: Define contract for text tokenization and result model

```csharp
namespace AIntern.Core.Interfaces;

using System.Threading;
using System.Threading.Tasks;

/// <summary>
/// Service for text tokenization.
/// </summary>
public interface ITokenizerService
{
    /// <summary>
    /// Whether a tokenizer is currently loaded.
    /// </summary>
    bool IsLoaded { get; }

    /// <summary>
    /// Load a tokenizer from a JSON file.
    /// </summary>
    /// <param name="path">Path to the tokenizer.json file.</param>
    /// <param name="ct">Cancellation token.</param>
    Task LoadTokenizerAsync(string path, CancellationToken ct = default);

    /// <summary>
    /// Tokenize text and return token IDs.
    /// </summary>
    /// <param name="text">Text to tokenize.</param>
    /// <param name="maxLength">Maximum sequence length.</param>
    /// <returns>Tokenization result with IDs and attention mask.</returns>
    TokenizedText Encode(string text, int maxLength);

    /// <summary>
    /// Decode token IDs back to text.
    /// </summary>
    /// <param name="tokenIds">Token IDs to decode.</param>
    /// <returns>Decoded text string.</returns>
    string Decode(long[] tokenIds);

    /// <summary>
    /// Count tokens in text without full encoding.
    /// </summary>
    /// <param name="text">Text to count tokens for.</param>
    /// <returns>Approximate token count.</returns>
    int CountTokens(string text);
}

/// <summary>
/// Result of text tokenization.
/// </summary>
public sealed class TokenizedText
{
    /// <summary>
    /// Token IDs.
    /// </summary>
    public long[] InputIds { get; init; } = System.Array.Empty<long>();

    /// <summary>
    /// Attention mask (1 for real tokens, 0 for padding).
    /// </summary>
    public long[] AttentionMask { get; init; } = System.Array.Empty<long>();

    /// <summary>
    /// Token type IDs (for multi-sequence inputs).
    /// </summary>
    public long[]? TokenTypeIds { get; init; }
}
```

---

### 2. HuggingFaceTokenizerService.cs

**Location**: `src/AIntern.Services/Embedding/HuggingFaceTokenizerService.cs`
**Type**: `sealed class`
**Implements**: `ITokenizerService`
**Purpose**: Parse HuggingFace tokenizer.json files and perform WordPiece tokenization

```csharp
namespace AIntern.Services.Embedding;

using System;
using System.Collections.Generic;
using System.IO;
using System.Text.Json;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.Extensions.Logging;
using AIntern.Core.Interfaces;

/// <summary>
/// Tokenizer service for HuggingFace-style tokenizer.json files.
/// </summary>
public sealed class HuggingFaceTokenizerService : ITokenizerService
{
    private readonly ILogger<HuggingFaceTokenizerService> _logger;

    private Dictionary<string, int>? _vocab;
    private Dictionary<int, string>? _reverseVocab;
    private int _padTokenId;
    private int _clsTokenId;
    private int _sepTokenId;
    private int _unkTokenId;

    /// <inheritdoc />
    public bool IsLoaded => _vocab is not null;

    /// <summary>
    /// Initializes a new instance of the HuggingFaceTokenizerService.
    /// </summary>
    /// <param name="logger">Logger for diagnostic output.</param>
    public HuggingFaceTokenizerService(ILogger<HuggingFaceTokenizerService> logger)
    {
        _logger = logger;

        // Default special token IDs (common for BERT-style models)
        _padTokenId = 0;
        _clsTokenId = 101;
        _sepTokenId = 102;
        _unkTokenId = 100;
    }

    /// <inheritdoc />
    public async Task LoadTokenizerAsync(string path, CancellationToken ct = default)
    {
        _logger.LogInformation("Loading tokenizer from {Path}", path);

        var json = await File.ReadAllTextAsync(path, ct);
        var doc = JsonDocument.Parse(json);

        // Parse vocabulary
        _vocab = new Dictionary<string, int>();
        _reverseVocab = new Dictionary<int, string>();

        if (doc.RootElement.TryGetProperty("model", out var model) &&
            model.TryGetProperty("vocab", out var vocab))
        {
            foreach (var item in vocab.EnumerateObject())
            {
                var id = item.Value.GetInt32();
                _vocab[item.Name] = id;
                _reverseVocab[id] = item.Name;
            }
        }

        // Parse special tokens
        if (doc.RootElement.TryGetProperty("added_tokens", out var addedTokens))
        {
            foreach (var token in addedTokens.EnumerateArray())
            {
                var content = token.GetProperty("content").GetString();
                var id = token.GetProperty("id").GetInt32();

                if (content == "[PAD]") _padTokenId = id;
                else if (content == "[CLS]") _clsTokenId = id;
                else if (content == "[SEP]") _sepTokenId = id;
                else if (content == "[UNK]") _unkTokenId = id;
            }
        }

        _logger.LogInformation(
            "Loaded tokenizer with {Count} tokens",
            _vocab.Count);
    }

    /// <inheritdoc />
    public TokenizedText Encode(string text, int maxLength)
    {
        if (_vocab is null)
            throw new InvalidOperationException("Tokenizer not loaded");

        // Simple WordPiece-style tokenization
        var tokens = new List<long> { _clsTokenId };

        var words = text.Split(' ', StringSplitOptions.RemoveEmptyEntries);
        foreach (var word in words)
        {
            if (tokens.Count >= maxLength - 1)
                break;

            var normalizedWord = word.ToLowerInvariant();

            // Try to find exact match
            if (_vocab.TryGetValue(normalizedWord, out var tokenId))
            {
                tokens.Add(tokenId);
            }
            else
            {
                // Try WordPiece subword tokenization
                var subwords = TokenizeWordPiece(normalizedWord, maxLength - tokens.Count - 1);
                tokens.AddRange(subwords);
            }
        }

        tokens.Add(_sepTokenId);

        // Create attention mask
        var attentionMask = new long[tokens.Count];
        Array.Fill(attentionMask, 1L);

        return new TokenizedText
        {
            InputIds = tokens.ToArray(),
            AttentionMask = attentionMask
        };
    }

    /// <summary>
    /// Tokenize a word using WordPiece algorithm.
    /// </summary>
    /// <param name="word">Word to tokenize.</param>
    /// <param name="maxSubwords">Maximum subwords to generate.</param>
    /// <returns>Sequence of token IDs.</returns>
    private IEnumerable<long> TokenizeWordPiece(string word, int maxSubwords)
    {
        var subwords = new List<long>();
        var remaining = word;
        var isFirst = true;

        while (remaining.Length > 0 && subwords.Count < maxSubwords)
        {
            var found = false;

            // Try progressively shorter substrings
            for (int len = remaining.Length; len > 0; len--)
            {
                var subword = remaining[..len];
                if (!isFirst)
                    subword = "##" + subword;

                if (_vocab!.TryGetValue(subword, out var tokenId))
                {
                    subwords.Add(tokenId);
                    remaining = remaining[len..];
                    found = true;
                    isFirst = false;
                    break;
                }
            }

            if (!found)
            {
                // Unknown character - use UNK token
                subwords.Add(_unkTokenId);
                remaining = remaining.Length > 1 ? remaining[1..] : "";
            }
        }

        return subwords;
    }

    /// <inheritdoc />
    public string Decode(long[] tokenIds)
    {
        if (_reverseVocab is null)
            throw new InvalidOperationException("Tokenizer not loaded");

        var tokens = new List<string>();

        foreach (var id in tokenIds)
        {
            if (_reverseVocab.TryGetValue((int)id, out var token))
            {
                // Skip special tokens
                if (token is "[CLS]" or "[SEP]" or "[PAD]")
                    continue;

                // Handle WordPiece continuation
                if (token.StartsWith("##"))
                    token = token[2..];

                tokens.Add(token);
            }
        }

        return string.Join(" ", tokens);
    }

    /// <inheritdoc />
    public int CountTokens(string text)
    {
        if (_vocab is null)
            return text.Split(' ', StringSplitOptions.RemoveEmptyEntries).Length;

        var encoded = Encode(text, int.MaxValue);
        return encoded.InputIds.Length;
    }
}
```

---

### 3. EmbeddingMath.cs

**Location**: `src/AIntern.Services/Embedding/EmbeddingMath.cs`
**Type**: `static class`
**Purpose**: SIMD-accelerated mathematical operations for embeddings

```csharp
namespace AIntern.Services.Embedding;

using System;
using System.Numerics;

/// <summary>
/// Shared mathematical operations for embeddings.
/// Uses SIMD acceleration when available.
/// </summary>
public static class EmbeddingMath
{
    /// <summary>
    /// Calculate cosine similarity between two vectors.
    /// </summary>
    /// <param name="a">First vector.</param>
    /// <param name="b">Second vector.</param>
    /// <returns>Cosine similarity in range [-1, 1].</returns>
    /// <exception cref="ArgumentException">Thrown when vectors have different dimensions.</exception>
    public static float CosineSimilarity(ReadOnlySpan<float> a, ReadOnlySpan<float> b)
    {
        if (a.Length != b.Length)
            throw new ArgumentException("Vectors must have the same dimension");

        if (a.Length == 0)
            return 0f;

        float dot = 0f, normA = 0f, normB = 0f;

        if (Vector.IsHardwareAccelerated && a.Length >= Vector<float>.Count)
        {
            var vectorSize = Vector<float>.Count;
            var dotVec = Vector<float>.Zero;
            var normAVec = Vector<float>.Zero;
            var normBVec = Vector<float>.Zero;

            int i = 0;
            for (; i <= a.Length - vectorSize; i += vectorSize)
            {
                var va = new Vector<float>(a.Slice(i, vectorSize));
                var vb = new Vector<float>(b.Slice(i, vectorSize));

                dotVec += va * vb;
                normAVec += va * va;
                normBVec += vb * vb;
            }

            // Sum vector elements
            for (int j = 0; j < vectorSize; j++)
            {
                dot += dotVec[j];
                normA += normAVec[j];
                normB += normBVec[j];
            }

            // Handle remaining elements
            for (; i < a.Length; i++)
            {
                dot += a[i] * b[i];
                normA += a[i] * a[i];
                normB += b[i] * b[i];
            }
        }
        else
        {
            for (int i = 0; i < a.Length; i++)
            {
                dot += a[i] * b[i];
                normA += a[i] * a[i];
                normB += b[i] * b[i];
            }
        }

        var magnitude = MathF.Sqrt(normA) * MathF.Sqrt(normB);
        return magnitude > 0 ? dot / magnitude : 0f;
    }

    /// <summary>
    /// Normalize a vector to unit length in place.
    /// </summary>
    /// <param name="vector">Vector to normalize (modified in place).</param>
    public static void Normalize(Span<float> vector)
    {
        if (vector.Length == 0)
            return;

        float sumSquares = 0f;

        if (Vector.IsHardwareAccelerated && vector.Length >= Vector<float>.Count)
        {
            var vectorSize = Vector<float>.Count;
            var sumVec = Vector<float>.Zero;
            int i = 0;

            for (; i <= vector.Length - vectorSize; i += vectorSize)
            {
                var v = new Vector<float>(vector.Slice(i, vectorSize));
                sumVec += v * v;
            }

            for (int j = 0; j < vectorSize; j++)
                sumSquares += sumVec[j];

            for (; i < vector.Length; i++)
                sumSquares += vector[i] * vector[i];
        }
        else
        {
            foreach (var val in vector)
                sumSquares += val * val;
        }

        var magnitude = MathF.Sqrt(sumSquares);
        if (magnitude <= 0)
            return;

        // Normalize in place using SIMD if possible
        if (Vector.IsHardwareAccelerated && vector.Length >= Vector<float>.Count)
        {
            var vectorSize = Vector<float>.Count;
            var magVec = new Vector<float>(magnitude);
            int i = 0;

            for (; i <= vector.Length - vectorSize; i += vectorSize)
            {
                var v = new Vector<float>(vector.Slice(i, vectorSize));
                (v / magVec).CopyTo(vector.Slice(i, vectorSize));
            }

            for (; i < vector.Length; i++)
                vector[i] /= magnitude;
        }
        else
        {
            for (int i = 0; i < vector.Length; i++)
                vector[i] /= magnitude;
        }
    }

    /// <summary>
    /// Calculate Euclidean distance between two vectors.
    /// </summary>
    /// <param name="a">First vector.</param>
    /// <param name="b">Second vector.</param>
    /// <returns>Euclidean (L2) distance.</returns>
    /// <exception cref="ArgumentException">Thrown when vectors have different dimensions.</exception>
    public static float EuclideanDistance(ReadOnlySpan<float> a, ReadOnlySpan<float> b)
    {
        if (a.Length != b.Length)
            throw new ArgumentException("Vectors must have the same dimension");

        float sumSquares = 0f;

        for (int i = 0; i < a.Length; i++)
        {
            var diff = a[i] - b[i];
            sumSquares += diff * diff;
        }

        return MathF.Sqrt(sumSquares);
    }

    /// <summary>
    /// Calculate dot product of two vectors.
    /// </summary>
    /// <param name="a">First vector.</param>
    /// <param name="b">Second vector.</param>
    /// <returns>Dot product value.</returns>
    /// <exception cref="ArgumentException">Thrown when vectors have different dimensions.</exception>
    public static float DotProduct(ReadOnlySpan<float> a, ReadOnlySpan<float> b)
    {
        if (a.Length != b.Length)
            throw new ArgumentException("Vectors must have the same dimension");

        float dot = 0f;

        if (Vector.IsHardwareAccelerated && a.Length >= Vector<float>.Count)
        {
            var vectorSize = Vector<float>.Count;
            var dotVec = Vector<float>.Zero;
            int i = 0;

            for (; i <= a.Length - vectorSize; i += vectorSize)
            {
                var va = new Vector<float>(a.Slice(i, vectorSize));
                var vb = new Vector<float>(b.Slice(i, vectorSize));
                dotVec += va * vb;
            }

            for (int j = 0; j < vectorSize; j++)
                dot += dotVec[j];

            for (; i < a.Length; i++)
                dot += a[i] * b[i];
        }
        else
        {
            for (int i = 0; i < a.Length; i++)
                dot += a[i] * b[i];
        }

        return dot;
    }
}
```

---

### 4. OnnxEmbeddingService.cs

**Location**: `src/AIntern.Services/Embedding/OnnxEmbeddingService.cs`
**Type**: `sealed class`
**Implements**: `IEmbeddingService`
**Purpose**: ONNX model loading and inference for sentence transformer embeddings

```csharp
namespace AIntern.Services.Embedding;

using System;
using System.Collections.Generic;
using System.Diagnostics;
using System.IO;
using System.Linq;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.Extensions.Logging;
using Microsoft.ML.OnnxRuntime;
using Microsoft.ML.OnnxRuntime.Tensors;
using AIntern.Core.Interfaces;
using AIntern.Core.Models;

/// <summary>
/// Embedding service implementation using ONNX Runtime.
/// Supports sentence transformer models exported to ONNX format.
/// </summary>
public sealed class OnnxEmbeddingService : IEmbeddingService
{
    private readonly ILogger<OnnxEmbeddingService> _logger;
    private readonly ITokenizerService _tokenizerService;
    private readonly SemaphoreSlim _loadLock = new(1, 1);
    private readonly SemaphoreSlim _embedLock = new(1, 1);

    private InferenceSession? _session;
    private EmbeddingModelOptions? _currentOptions;
    private string[]? _inputNames;
    private string? _outputName;
    private bool _isDisposed;

    /// <inheritdoc />
    public bool IsModelLoaded => _session is not null;

    /// <inheritdoc />
    public int EmbeddingDimension { get; private set; }

    /// <inheritdoc />
    public int MaxTokens { get; private set; } = 512;

    /// <inheritdoc />
    public string? CurrentModelName { get; private set; }

    /// <inheritdoc />
    public event EventHandler<EmbeddingModelStateChangedEventArgs>? ModelStateChanged;

    /// <summary>
    /// Initializes a new instance of the OnnxEmbeddingService.
    /// </summary>
    /// <param name="logger">Logger for diagnostic output.</param>
    /// <param name="tokenizerService">Tokenizer service for text encoding.</param>
    public OnnxEmbeddingService(
        ILogger<OnnxEmbeddingService> logger,
        ITokenizerService tokenizerService)
    {
        _logger = logger;
        _tokenizerService = tokenizerService;
    }

    /// <inheritdoc />
    public async Task LoadModelAsync(
        EmbeddingModelOptions options,
        IProgress<ModelLoadProgress>? progress = null,
        CancellationToken ct = default)
    {
        ObjectDisposedException.ThrowIf(_isDisposed, this);

        if (options.ModelType != EmbeddingModelType.Onnx &&
            options.ModelType != EmbeddingModelType.Auto)
        {
            throw new ArgumentException(
                $"OnnxEmbeddingService only supports ONNX models, not {options.ModelType}",
                nameof(options));
        }

        await _loadLock.WaitAsync(ct);
        try
        {
            if (_session is not null)
            {
                _logger.LogInformation("Unloading existing ONNX model");
                await UnloadModelInternalAsync();
            }

            _logger.LogInformation("Loading ONNX embedding model: {Path}", options.ModelPath);

            progress?.Report(new ModelLoadProgress
            {
                Stage = "Configuring session options",
                PercentComplete = 10
            });

            // Configure session options
            var sessionOptions = new SessionOptions();

            // Set thread count
            if (options.Threads > 0)
            {
                sessionOptions.IntraOpNumThreads = options.Threads;
                sessionOptions.InterOpNumThreads = options.Threads;
            }

            // Enable GPU if available and requested
            if (options.GpuLayers != 0)
            {
                try
                {
                    if (OperatingSystem.IsWindows() || OperatingSystem.IsLinux())
                    {
                        // Try CUDA first
                        sessionOptions.AppendExecutionProvider_CUDA();
                        _logger.LogInformation("CUDA execution provider enabled");
                    }
                    else if (OperatingSystem.IsMacOS())
                    {
                        // Use CoreML on macOS
                        sessionOptions.AppendExecutionProvider_CoreML();
                        _logger.LogInformation("CoreML execution provider enabled");
                    }
                }
                catch (Exception ex)
                {
                    _logger.LogWarning(ex, "GPU acceleration not available, using CPU");
                }
            }

            // Set optimization level
            sessionOptions.GraphOptimizationLevel = GraphOptimizationLevel.ORT_ENABLE_ALL;

            progress?.Report(new ModelLoadProgress
            {
                Stage = "Loading model",
                PercentComplete = 30
            });

            // Load the model
            _session = await Task.Run(
                () => new InferenceSession(options.ModelPath, sessionOptions),
                ct);

            // Discover input/output names
            _inputNames = _session.InputMetadata.Keys.ToArray();
            _outputName = _session.OutputMetadata.Keys.FirstOrDefault(k =>
                k.Contains("embedding", StringComparison.OrdinalIgnoreCase) ||
                k.Contains("output", StringComparison.OrdinalIgnoreCase) ||
                k.Contains("last_hidden", StringComparison.OrdinalIgnoreCase));

            if (_outputName is null)
                _outputName = _session.OutputMetadata.Keys.First();

            // Determine embedding dimension from output shape
            var outputMeta = _session.OutputMetadata[_outputName];
            EmbeddingDimension = (int)outputMeta.Dimensions.Last();

            progress?.Report(new ModelLoadProgress
            {
                Stage = "Loading tokenizer",
                PercentComplete = 70
            });

            // Load tokenizer for this model
            var tokenizerPath = Path.Combine(
                Path.GetDirectoryName(options.ModelPath)!,
                "tokenizer.json");

            if (File.Exists(tokenizerPath))
            {
                await _tokenizerService.LoadTokenizerAsync(tokenizerPath, ct);
            }
            else
            {
                _logger.LogWarning(
                    "Tokenizer not found at {Path}, using default",
                    tokenizerPath);
            }

            _currentOptions = options;
            MaxTokens = options.ContextSize;
            CurrentModelName = options.ModelName;

            progress?.Report(new ModelLoadProgress
            {
                Stage = "Model ready",
                PercentComplete = 100
            });

            _logger.LogInformation(
                "Loaded ONNX model: {Name}, Dimension: {Dim}, Inputs: {Inputs}",
                options.ModelName,
                EmbeddingDimension,
                string.Join(", ", _inputNames));

            ModelStateChanged?.Invoke(this, new EmbeddingModelStateChangedEventArgs
            {
                IsLoaded = true,
                ModelPath = options.ModelPath,
                ModelName = options.ModelName,
                EmbeddingDimension = EmbeddingDimension
            });
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Failed to load ONNX model: {Path}", options.ModelPath);

            ModelStateChanged?.Invoke(this, new EmbeddingModelStateChangedEventArgs
            {
                IsLoaded = false,
                Error = ex.Message
            });

            throw;
        }
        finally
        {
            _loadLock.Release();
        }
    }

    /// <inheritdoc />
    public async Task UnloadModelAsync()
    {
        await _loadLock.WaitAsync();
        try
        {
            await UnloadModelInternalAsync();
        }
        finally
        {
            _loadLock.Release();
        }
    }

    /// <summary>
    /// Internal unload method (caller must hold _loadLock).
    /// </summary>
    private Task UnloadModelInternalAsync()
    {
        _session?.Dispose();
        _session = null;

        _currentOptions = null;
        _inputNames = null;
        _outputName = null;
        EmbeddingDimension = 0;
        MaxTokens = 512;
        CurrentModelName = null;

        ModelStateChanged?.Invoke(this, new EmbeddingModelStateChangedEventArgs
        {
            IsLoaded = false
        });

        _logger.LogInformation("ONNX model unloaded");

        return Task.CompletedTask;
    }

    /// <inheritdoc />
    public async Task<float[]> EmbedAsync(string text, CancellationToken ct = default)
    {
        ObjectDisposedException.ThrowIf(_isDisposed, this);

        if (_session is null)
            throw new InvalidOperationException("No embedding model is loaded");

        if (string.IsNullOrEmpty(text))
            return new float[EmbeddingDimension];

        // Apply text prefix if configured
        var processedText = _currentOptions?.TextPrefix is not null
            ? _currentOptions.TextPrefix + text
            : text;

        await _embedLock.WaitAsync(ct);
        try
        {
            // Tokenize
            var tokens = _tokenizerService.Encode(processedText, MaxTokens);

            // Create input tensors
            var inputs = CreateInputTensors(tokens, 1);

            // Run inference
            using var results = _session.Run(inputs);

            // Extract embedding
            var outputTensor = results.First(r => r.Name == _outputName).AsTensor<float>();
            var embedding = ExtractEmbedding(outputTensor, _currentOptions?.PoolingType ?? EmbeddingPoolingType.Mean);

            // Normalize if configured
            if (_currentOptions?.NormalizeEmbeddings == true)
                NormalizeEmbedding(embedding);

            return embedding;
        }
        finally
        {
            _embedLock.Release();
        }
    }

    /// <inheritdoc />
    public async Task<IReadOnlyList<float[]>> EmbedBatchAsync(
        IEnumerable<string> texts,
        IProgress<EmbeddingProgress>? progress = null,
        CancellationToken ct = default)
    {
        ObjectDisposedException.ThrowIf(_isDisposed, this);

        if (_session is null)
            throw new InvalidOperationException("No embedding model is loaded");

        var textList = texts.ToList();
        if (textList.Count == 0)
            return Array.Empty<float[]>();

        var results = new List<float[]>(textList.Count);
        var stopwatch = Stopwatch.StartNew();
        var batchSize = _currentOptions?.BatchSize ?? 32;

        await _embedLock.WaitAsync(ct);
        try
        {
            // Process in batches
            for (int batchStart = 0; batchStart < textList.Count; batchStart += batchSize)
            {
                ct.ThrowIfCancellationRequested();

                var batchEnd = Math.Min(batchStart + batchSize, textList.Count);
                var batch = textList.GetRange(batchStart, batchEnd - batchStart);

                // Apply text prefix if configured
                if (_currentOptions?.TextPrefix is not null)
                {
                    batch = batch.Select(t => _currentOptions.TextPrefix + t).ToList();
                }

                // Tokenize batch
                var tokenizedBatch = batch
                    .Select(t => string.IsNullOrEmpty(t)
                        ? new TokenizedText { InputIds = Array.Empty<long>(), AttentionMask = Array.Empty<long>() }
                        : _tokenizerService.Encode(t, MaxTokens))
                    .ToList();

                // Pad to same length
                var maxLen = tokenizedBatch.Max(t => t.InputIds.Length);
                if (maxLen == 0)
                {
                    // All empty texts
                    results.AddRange(batch.Select(_ => new float[EmbeddingDimension]));
                    continue;
                }

                // Create batched input tensors
                var inputs = CreateBatchedInputTensors(tokenizedBatch, maxLen);

                // Run inference
                using var inferenceResults = _session.Run(inputs);

                // Extract embeddings for each item in batch
                var outputTensor = inferenceResults
                    .First(r => r.Name == _outputName)
                    .AsTensor<float>();

                for (int i = 0; i < batch.Count; i++)
                {
                    var embedding = ExtractBatchEmbedding(
                        outputTensor, i, maxLen,
                        _currentOptions?.PoolingType ?? EmbeddingPoolingType.Mean);

                    if (_currentOptions?.NormalizeEmbeddings == true)
                        NormalizeEmbedding(embedding);

                    results.Add(embedding);
                }

                // Report progress
                if (progress is not null)
                {
                    var elapsed = stopwatch.Elapsed;
                    var rate = results.Count / elapsed.TotalSeconds;
                    var remaining = TimeSpan.FromSeconds((textList.Count - results.Count) / rate);

                    progress.Report(new EmbeddingProgress
                    {
                        ProcessedCount = results.Count,
                        TotalCount = textList.Count,
                        TextsPerSecond = rate,
                        EstimatedRemaining = remaining
                    });
                }
            }
        }
        finally
        {
            _embedLock.Release();
        }

        _logger.LogDebug(
            "Generated {Count} embeddings in {Elapsed:F2}s ({Rate:F1}/s)",
            results.Count,
            stopwatch.Elapsed.TotalSeconds,
            results.Count / stopwatch.Elapsed.TotalSeconds);

        return results;
    }

    /// <summary>
    /// Create input tensors for a single sequence.
    /// </summary>
    private IReadOnlyCollection<NamedOnnxValue> CreateInputTensors(
        TokenizedText tokens,
        int batchSize)
    {
        var seqLen = tokens.InputIds.Length;
        var inputs = new List<NamedOnnxValue>();

        // Input IDs
        var inputIdsTensor = new DenseTensor<long>(
            tokens.InputIds,
            new[] { batchSize, seqLen });
        inputs.Add(NamedOnnxValue.CreateFromTensor("input_ids", inputIdsTensor));

        // Attention mask
        if (_inputNames!.Contains("attention_mask"))
        {
            var attentionTensor = new DenseTensor<long>(
                tokens.AttentionMask,
                new[] { batchSize, seqLen });
            inputs.Add(NamedOnnxValue.CreateFromTensor("attention_mask", attentionTensor));
        }

        // Token type IDs (usually all zeros for single-sequence)
        if (_inputNames.Contains("token_type_ids"))
        {
            var typeIds = new long[seqLen];
            var typeTensor = new DenseTensor<long>(typeIds, new[] { batchSize, seqLen });
            inputs.Add(NamedOnnxValue.CreateFromTensor("token_type_ids", typeTensor));
        }

        return inputs;
    }

    /// <summary>
    /// Create batched input tensors with padding.
    /// </summary>
    private IReadOnlyCollection<NamedOnnxValue> CreateBatchedInputTensors(
        List<TokenizedText> batch,
        int maxLen)
    {
        var batchSize = batch.Count;
        var inputs = new List<NamedOnnxValue>();

        // Pad and create batched arrays
        var inputIds = new long[batchSize * maxLen];
        var attentionMask = new long[batchSize * maxLen];

        for (int i = 0; i < batch.Count; i++)
        {
            var tokens = batch[i];
            var offset = i * maxLen;

            for (int j = 0; j < tokens.InputIds.Length; j++)
            {
                inputIds[offset + j] = tokens.InputIds[j];
                attentionMask[offset + j] = tokens.AttentionMask[j];
            }
            // Rest is already 0 (padding)
        }

        var inputIdsTensor = new DenseTensor<long>(inputIds, new[] { batchSize, maxLen });
        inputs.Add(NamedOnnxValue.CreateFromTensor("input_ids", inputIdsTensor));

        if (_inputNames!.Contains("attention_mask"))
        {
            var attentionTensor = new DenseTensor<long>(attentionMask, new[] { batchSize, maxLen });
            inputs.Add(NamedOnnxValue.CreateFromTensor("attention_mask", attentionTensor));
        }

        if (_inputNames.Contains("token_type_ids"))
        {
            var typeIds = new long[batchSize * maxLen];
            var typeTensor = new DenseTensor<long>(typeIds, new[] { batchSize, maxLen });
            inputs.Add(NamedOnnxValue.CreateFromTensor("token_type_ids", typeTensor));
        }

        return inputs;
    }

    /// <summary>
    /// Extract embedding from single sequence output.
    /// </summary>
    private float[] ExtractEmbedding(Tensor<float> outputTensor, EmbeddingPoolingType pooling)
    {
        // Output shape is typically [batch, seq_len, hidden_dim] or [batch, hidden_dim]
        var dimensions = outputTensor.Dimensions.ToArray();

        if (dimensions.Length == 2)
        {
            // Already pooled: [batch, hidden_dim]
            var embedding = new float[EmbeddingDimension];
            for (int i = 0; i < EmbeddingDimension; i++)
                embedding[i] = outputTensor[0, i];
            return embedding;
        }

        // Need to pool: [batch, seq_len, hidden_dim]
        var seqLen = dimensions[1];
        return ApplyPooling(outputTensor, 0, seqLen, pooling);
    }

    /// <summary>
    /// Extract embedding for a specific item in a batch.
    /// </summary>
    private float[] ExtractBatchEmbedding(
        Tensor<float> outputTensor,
        int batchIndex,
        int seqLen,
        EmbeddingPoolingType pooling)
    {
        var dimensions = outputTensor.Dimensions.ToArray();

        if (dimensions.Length == 2)
        {
            // Already pooled
            var embedding = new float[EmbeddingDimension];
            for (int i = 0; i < EmbeddingDimension; i++)
                embedding[i] = outputTensor[batchIndex, i];
            return embedding;
        }

        return ApplyPooling(outputTensor, batchIndex, seqLen, pooling);
    }

    /// <summary>
    /// Apply pooling strategy to reduce [seq_len, hidden_dim] to [hidden_dim].
    /// </summary>
    private float[] ApplyPooling(
        Tensor<float> tensor,
        int batchIndex,
        int seqLen,
        EmbeddingPoolingType pooling)
    {
        var embedding = new float[EmbeddingDimension];

        switch (pooling)
        {
            case EmbeddingPoolingType.Mean:
                // Mean of all token embeddings
                for (int d = 0; d < EmbeddingDimension; d++)
                {
                    float sum = 0;
                    for (int s = 0; s < seqLen; s++)
                        sum += tensor[batchIndex, s, d];
                    embedding[d] = sum / seqLen;
                }
                break;

            case EmbeddingPoolingType.Cls:
                // First token ([CLS])
                for (int d = 0; d < EmbeddingDimension; d++)
                    embedding[d] = tensor[batchIndex, 0, d];
                break;

            case EmbeddingPoolingType.Last:
                // Last token
                for (int d = 0; d < EmbeddingDimension; d++)
                    embedding[d] = tensor[batchIndex, seqLen - 1, d];
                break;

            case EmbeddingPoolingType.Max:
                // Max pooling
                for (int d = 0; d < EmbeddingDimension; d++)
                {
                    float max = float.MinValue;
                    for (int s = 0; s < seqLen; s++)
                        max = Math.Max(max, tensor[batchIndex, s, d]);
                    embedding[d] = max;
                }
                break;
        }

        return embedding;
    }

    /// <inheritdoc />
    public float CosineSimilarity(ReadOnlySpan<float> embedding1, ReadOnlySpan<float> embedding2)
    {
        // Delegate to shared implementation
        return EmbeddingMath.CosineSimilarity(embedding1, embedding2);
    }

    /// <inheritdoc />
    public void NormalizeEmbedding(Span<float> embedding)
    {
        EmbeddingMath.Normalize(embedding);
    }

    /// <inheritdoc />
    public async ValueTask DisposeAsync()
    {
        if (_isDisposed)
            return;

        _isDisposed = true;

        await UnloadModelAsync();

        _loadLock.Dispose();
        _embedLock.Dispose();
    }
}
```

---

### 5. Project File Modification

**Location**: `src/AIntern.Services/AIntern.Services.csproj`
**Change**: Add ONNX Runtime package references

```xml
<ItemGroup>
  <!-- Existing packages... -->

  <!-- ONNX Runtime for ONNX embedding model support -->
  <PackageReference Include="Microsoft.ML.OnnxRuntime" Version="1.17.0" />
  <PackageReference Include="Microsoft.ML.OnnxRuntime.Managed" Version="1.17.0" />

  <!-- GPU support (optional, platform-specific) -->
  <PackageReference Include="Microsoft.ML.OnnxRuntime.Gpu" Version="1.17.0"
      Condition="'$(RuntimeIdentifier)' == 'win-x64' OR '$(RuntimeIdentifier)' == 'linux-x64'" />
</ItemGroup>
```

**Package Details**:

| Package | Version | Purpose |
|---------|---------|---------|
| `Microsoft.ML.OnnxRuntime` | 1.17.0 | Core ONNX Runtime library |
| `Microsoft.ML.OnnxRuntime.Managed` | 1.17.0 | Managed tensor API (DenseTensor) |
| `Microsoft.ML.OnnxRuntime.Gpu` | 1.17.0 | CUDA GPU support (conditional) |

---

## File Summary

### Files to Create

| File | Location | Purpose |
|------|----------|---------|
| `ITokenizerService.cs` | `src/AIntern.Core/Interfaces/` | Tokenizer interface and TokenizedText model |
| `HuggingFaceTokenizerService.cs` | `src/AIntern.Services/Embedding/` | HuggingFace tokenizer.json parser |
| `EmbeddingMath.cs` | `src/AIntern.Services/Embedding/` | SIMD-optimized vector operations |
| `OnnxEmbeddingService.cs` | `src/AIntern.Services/Embedding/` | ONNX model loading and inference |

### Files to Modify

| File | Location | Change |
|------|----------|--------|
| `AIntern.Services.csproj` | `src/AIntern.Services/` | Add ONNX Runtime package references |

### Directory Structure

```
src/AIntern.Core/
└── Interfaces/
    └── ITokenizerService.cs                (NEW - v0.7.1c)

src/AIntern.Services/
├── Embedding/
│   ├── LlamaEmbeddingService.cs            (from v0.7.1b)
│   ├── EmbeddingServiceFactory.cs          (from v0.7.1b)
│   ├── EmbeddingServiceExtensions.cs       (from v0.7.1b)
│   ├── OnnxEmbeddingService.cs             (NEW - v0.7.1c)
│   ├── EmbeddingMath.cs                    (NEW - v0.7.1c)
│   └── HuggingFaceTokenizerService.cs      (NEW - v0.7.1c)
└── AIntern.Services.csproj                 (MODIFIED - v0.7.1c)
```

---

## Usage Examples

### Basic ONNX Model Loading

```csharp
// Inject via DI
public class SemanticSearchService
{
    private readonly OnnxEmbeddingService _embedding;

    public SemanticSearchService(OnnxEmbeddingService embedding)
    {
        _embedding = embedding;
    }

    public async Task InitializeAsync(CancellationToken ct)
    {
        var options = new EmbeddingModelOptions
        {
            ModelPath = "/models/all-MiniLM-L6-v2.onnx",
            ModelName = "MiniLM-L6-v2",
            ModelType = EmbeddingModelType.Onnx,
            GpuLayers = -1,  // Auto-detect GPU
            ContextSize = 512,
            NormalizeEmbeddings = true,
            PoolingType = EmbeddingPoolingType.Mean
        };

        var progress = new Progress<ModelLoadProgress>(p =>
            Console.WriteLine($"{p.Stage}: {p.PercentComplete}%"));

        await _embedding.LoadModelAsync(options, progress, ct);
    }
}
```

### Using Different Pooling Strategies

```csharp
// For BERT-style classification models
var options = new EmbeddingModelOptions
{
    ModelPath = "/models/bert-base.onnx",
    PoolingType = EmbeddingPoolingType.Cls  // Use [CLS] token
};

// For general semantic similarity
var options = new EmbeddingModelOptions
{
    ModelPath = "/models/sentence-transformer.onnx",
    PoolingType = EmbeddingPoolingType.Mean  // Average all tokens
};

// For decoder-only models
var options = new EmbeddingModelOptions
{
    ModelPath = "/models/gpt-embed.onnx",
    PoolingType = EmbeddingPoolingType.Last  // Use last token
};
```

### Batch Processing with Padding

```csharp
public async Task IndexDocumentsAsync(
    IReadOnlyList<Document> documents,
    CancellationToken ct)
{
    var texts = documents.Select(d => d.Content).ToList();

    var progress = new Progress<EmbeddingProgress>(p =>
    {
        Console.WriteLine(
            $"Embedding {p.ProcessedCount}/{p.TotalCount} " +
            $"({p.PercentComplete:F1}%) - {p.TextsPerSecond:F1}/s");
    });

    // Batch processing handles variable-length texts automatically
    var embeddings = await _embedding.EmbedBatchAsync(texts, progress, ct);

    // Store embeddings
    for (int i = 0; i < documents.Count; i++)
    {
        await _vectorStore.AddAsync(
            documents[i].Id,
            embeddings[i],
            ct);
    }
}
```

### Using EmbeddingMath for Similarity Search

```csharp
public IEnumerable<SearchResult> FindSimilar(
    float[] queryEmbedding,
    IEnumerable<(string Id, float[] Embedding)> candidates,
    int topK = 10)
{
    return candidates
        .Select(c => new SearchResult
        {
            Id = c.Id,
            Score = EmbeddingMath.CosineSimilarity(queryEmbedding, c.Embedding)
        })
        .OrderByDescending(r => r.Score)
        .Take(topK);
}

public float[] GetNormalizedEmbedding(float[] embedding)
{
    var normalized = (float[])embedding.Clone();
    EmbeddingMath.Normalize(normalized);
    return normalized;
}
```

### Tokenizer Usage

```csharp
// Direct tokenizer usage
public void AnalyzeTokens(string text)
{
    var tokenized = _tokenizerService.Encode(text, maxLength: 512);

    Console.WriteLine($"Token count: {tokenized.InputIds.Length}");
    Console.WriteLine($"Tokens: {string.Join(", ", tokenized.InputIds)}");

    var decoded = _tokenizerService.Decode(tokenized.InputIds);
    Console.WriteLine($"Decoded: {decoded}");
}

// Quick token counting
public bool FitsInContext(string text, int maxTokens = 512)
{
    return _tokenizerService.CountTokens(text) <= maxTokens;
}
```

---

## Verification Steps

### 1. Build Verification

```bash
# Verify Services project builds with ONNX Runtime
dotnet build src/AIntern.Services

# Verify no warnings
dotnet build src/AIntern.Services --warnaserror
```

### 2. Package Restoration

```bash
# Restore NuGet packages
dotnet restore src/AIntern.Services

# Verify ONNX Runtime packages are installed
dotnet list src/AIntern.Services package
```

### 3. Unit Test Suggestions

```csharp
public class EmbeddingMathTests
{
    [Fact]
    public void CosineSimilarity_IdenticalVectors_ReturnsOne()
    {
        var embedding = new float[] { 0.1f, 0.2f, 0.3f, 0.4f };

        var similarity = EmbeddingMath.CosineSimilarity(embedding, embedding);

        Assert.Equal(1.0f, similarity, precision: 5);
    }

    [Fact]
    public void CosineSimilarity_OrthogonalVectors_ReturnsZero()
    {
        var embedding1 = new float[] { 1f, 0f, 0f, 0f };
        var embedding2 = new float[] { 0f, 1f, 0f, 0f };

        var similarity = EmbeddingMath.CosineSimilarity(embedding1, embedding2);

        Assert.Equal(0f, similarity, precision: 5);
    }

    [Fact]
    public void CosineSimilarity_OppositeVectors_ReturnsNegativeOne()
    {
        var embedding1 = new float[] { 1f, 0f, 0f, 0f };
        var embedding2 = new float[] { -1f, 0f, 0f, 0f };

        var similarity = EmbeddingMath.CosineSimilarity(embedding1, embedding2);

        Assert.Equal(-1f, similarity, precision: 5);
    }

    [Fact]
    public void Normalize_ProducesUnitVector()
    {
        var embedding = new float[] { 3f, 4f, 0f, 0f }; // Length = 5

        EmbeddingMath.Normalize(embedding);

        // Should be [0.6, 0.8, 0, 0]
        Assert.Equal(0.6f, embedding[0], precision: 5);
        Assert.Equal(0.8f, embedding[1], precision: 5);

        // Verify unit length
        var length = MathF.Sqrt(embedding.Sum(x => x * x));
        Assert.Equal(1f, length, precision: 5);
    }

    [Fact]
    public void DotProduct_CalculatesCorrectly()
    {
        var a = new float[] { 1f, 2f, 3f };
        var b = new float[] { 4f, 5f, 6f };

        var result = EmbeddingMath.DotProduct(a, b);

        // 1*4 + 2*5 + 3*6 = 4 + 10 + 18 = 32
        Assert.Equal(32f, result, precision: 5);
    }

    [Fact]
    public void EuclideanDistance_CalculatesCorrectly()
    {
        var a = new float[] { 0f, 0f };
        var b = new float[] { 3f, 4f };

        var distance = EmbeddingMath.EuclideanDistance(a, b);

        Assert.Equal(5f, distance, precision: 5);
    }
}

public class HuggingFaceTokenizerServiceTests
{
    [Fact]
    public void Encode_AddsSpecialTokens()
    {
        var logger = NullLogger<HuggingFaceTokenizerService>.Instance;
        var service = new HuggingFaceTokenizerService(logger);

        // Note: Would need mock tokenizer.json for full test
        // This tests the structure
        var result = service.Encode("test", 512);

        // Should have [CLS] at start and [SEP] at end
        Assert.Equal(101, result.InputIds[0]); // [CLS]
        Assert.Equal(102, result.InputIds[^1]); // [SEP]
    }

    [Fact]
    public void Encode_CreatesMatchingAttentionMask()
    {
        var logger = NullLogger<HuggingFaceTokenizerService>.Instance;
        var service = new HuggingFaceTokenizerService(logger);

        var result = service.Encode("hello world", 512);

        // Attention mask should match InputIds length
        Assert.Equal(result.InputIds.Length, result.AttentionMask.Length);

        // All attention values should be 1 (no padding)
        Assert.All(result.AttentionMask, m => Assert.Equal(1, m));
    }
}

public class OnnxEmbeddingServiceTests
{
    [Fact]
    public void IsModelLoaded_WhenNoModel_ReturnsFalse()
    {
        var logger = NullLogger<OnnxEmbeddingService>.Instance;
        var tokenizer = new HuggingFaceTokenizerService(
            NullLogger<HuggingFaceTokenizerService>.Instance);
        var service = new OnnxEmbeddingService(logger, tokenizer);

        Assert.False(service.IsModelLoaded);
    }

    [Fact]
    public async Task EmbedAsync_WhenNoModel_ThrowsInvalidOperationException()
    {
        var logger = NullLogger<OnnxEmbeddingService>.Instance;
        var tokenizer = new HuggingFaceTokenizerService(
            NullLogger<HuggingFaceTokenizerService>.Instance);
        var service = new OnnxEmbeddingService(logger, tokenizer);

        await Assert.ThrowsAsync<InvalidOperationException>(() =>
            service.EmbedAsync("test"));
    }

    [Fact]
    public async Task LoadModelAsync_WithInvalidModelType_ThrowsArgumentException()
    {
        var logger = NullLogger<OnnxEmbeddingService>.Instance;
        var tokenizer = new HuggingFaceTokenizerService(
            NullLogger<HuggingFaceTokenizerService>.Instance);
        var service = new OnnxEmbeddingService(logger, tokenizer);

        var options = new EmbeddingModelOptions
        {
            ModelPath = "/test/model.gguf",
            ModelType = EmbeddingModelType.Gguf  // Wrong type for ONNX service
        };

        await Assert.ThrowsAsync<ArgumentException>(() =>
            service.LoadModelAsync(options));
    }
}
```

---

## Integration Notes

### Connection to v0.7.1a

This version implements the `IEmbeddingService` interface defined in v0.7.1a:
- All properties from interface are implemented (IsModelLoaded, EmbeddingDimension, MaxTokens, CurrentModelName)
- All methods from interface are implemented (LoadModelAsync, UnloadModelAsync, EmbedAsync, EmbedBatchAsync, CosineSimilarity, NormalizeEmbedding)
- ModelStateChanged event is raised appropriately

### Connection to v0.7.1b

The ONNX service works alongside the LLamaSharp service:
- Factory already includes ONNX case (implemented in v0.7.1b)
- `AddEmbeddingServices()` already registers OnnxEmbeddingService
- Auto-detection routes .onnx files to OnnxEmbeddingService

### Shared Components

`EmbeddingMath` can be used by both services:
- `LlamaEmbeddingService` can delegate CosineSimilarity/Normalize to EmbeddingMath
- Consistent SIMD-accelerated operations across all embedding types

---

## Acceptance Criteria

- [ ] OnnxEmbeddingService loads ONNX models
- [ ] Tokenizer service parses HuggingFace tokenizer.json
- [ ] Batch embedding with padding works correctly
- [ ] All pooling types implemented (Mean, CLS, Last, Max)
- [ ] GPU acceleration via CUDA/CoreML when available
- [ ] EmbeddingMath provides SIMD-optimized operations
- [ ] Proper resource cleanup on dispose
- [ ] Thread-safe operation with semaphore locking
- [ ] Build succeeds with no warnings

---

## Changelog Entry

```markdown
## v0.7.1c - ONNX Embedding Implementation

### Added
- `OnnxEmbeddingService` implementing `IEmbeddingService` for ONNX models
  - Model loading with session options configuration
  - GPU acceleration via CUDA (Windows/Linux) and CoreML (macOS)
  - Single and batch embedding generation
  - Multiple pooling strategies (Mean, CLS, Last, Max)
  - Dynamic input/output name discovery
  - Thread-safe operation with semaphore locking
  - Proper resource cleanup on dispose
- `ITokenizerService` interface for text tokenization
  - LoadTokenizerAsync for loading tokenizer configurations
  - Encode/Decode for text <-> token conversion
  - CountTokens for token estimation
- `TokenizedText` model for tokenization results
  - InputIds, AttentionMask, TokenTypeIds arrays
- `HuggingFaceTokenizerService` for HuggingFace tokenizer.json files
  - Vocabulary parsing from JSON
  - Special token discovery ([CLS], [SEP], [PAD], [UNK])
  - WordPiece subword tokenization
  - Text reconstruction from token IDs
- `EmbeddingMath` static class for SIMD-accelerated operations
  - CosineSimilarity with hardware acceleration
  - Normalize for unit length normalization
  - EuclideanDistance for L2 distance
  - DotProduct for vector multiplication

### Dependencies
- Added Microsoft.ML.OnnxRuntime v1.17.0 for ONNX model inference
- Added Microsoft.ML.OnnxRuntime.Managed v1.17.0 for tensor API
- Added Microsoft.ML.OnnxRuntime.Gpu v1.17.0 (conditional, win-x64/linux-x64)

### Files Created
- `src/AIntern.Core/Interfaces/ITokenizerService.cs`
- `src/AIntern.Services/Embedding/OnnxEmbeddingService.cs`
- `src/AIntern.Services/Embedding/EmbeddingMath.cs`
- `src/AIntern.Services/Embedding/HuggingFaceTokenizerService.cs`

### Files Modified
- `src/AIntern.Services/AIntern.Services.csproj` - Added ONNX Runtime packages
```
