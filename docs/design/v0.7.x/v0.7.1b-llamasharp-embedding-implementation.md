# Design Specification: AIntern v0.7.1b "LLamaSharp Embedding Implementation"

## Overview

**Version**: v0.7.1b
**Parent**: v0.7.1 Embedding Foundation
**Focus**: Implementing embedding generation using LLamaSharp for GGUF-format embedding models

### Purpose

This sub-version provides the concrete implementation of the embedding service using LLamaSharp:
1. `LlamaEmbeddingService` - Full implementation of `IEmbeddingService` for GGUF models
2. `EmbeddingServiceFactory` - Factory pattern for creating appropriate service instances
3. `EmbeddingServiceExtensions` - Dependency injection registration helpers
4. Package configuration for LLamaSharp in the Services project

### Dependencies

**From v0.7.1a (Embedding Service Interface)**:
- `IEmbeddingService` interface contract
- `EmbeddingModelOptions` configuration class
- `EmbeddingModelType` and `EmbeddingPoolingType` enums
- `ModelLoadProgress` and `EmbeddingProgress` models
- `EmbeddingModelStateChangedEventArgs` event arguments
- `IEmbeddingModelRegistry` interface

**External Dependencies**:
- LLamaSharp (v0.18.0) - GGUF model loading and inference
- LLamaSharp.Backend.Cpu - CPU backend for cross-platform compatibility
- System.Numerics - SIMD-accelerated vector operations
- Microsoft.Extensions.Logging - Structured logging
- Microsoft.Extensions.DependencyInjection - DI container abstractions

---

## Architecture

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                  v0.7.1b LLamaSharp Embedding Implementation                  │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │                        Service Implementation Layer                       │ │
│  │  src/AIntern.Services/Embedding/                                         │ │
│  ├─────────────────────────────────────────────────────────────────────────┤ │
│  │                                                                          │ │
│  │  ┌────────────────────────────────────────────────────────────────────┐ │ │
│  │  │                     LlamaEmbeddingService                           │ │ │
│  │  │  Implements: IEmbeddingService                                      │ │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐   │ │ │
│  │  │  │  Private Fields:                                             │   │ │ │
│  │  │  │  ├── _logger: ILogger<LlamaEmbeddingService>                │   │ │ │
│  │  │  │  ├── _loadLock: SemaphoreSlim (thread-safe loading)         │   │ │ │
│  │  │  │  ├── _embedLock: SemaphoreSlim (thread-safe embedding)      │   │ │ │
│  │  │  │  ├── _model: LLamaWeights?                                  │   │ │ │
│  │  │  │  ├── _embedder: LLamaEmbedder?                              │   │ │ │
│  │  │  │  ├── _currentOptions: EmbeddingModelOptions?                │   │ │ │
│  │  │  │  └── _isDisposed: bool                                      │   │ │ │
│  │  │  └─────────────────────────────────────────────────────────────┘   │ │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐   │ │ │
│  │  │  │  Model Lifecycle:                                            │   │ │ │
│  │  │  │  ├── LoadModelAsync() → LLamaWeights + LLamaEmbedder         │   │ │ │
│  │  │  │  ├── UnloadModelAsync() → Dispose + GC.Collect               │   │ │ │
│  │  │  │  └── UnloadModelInternalAsync() → Internal cleanup           │   │ │ │
│  │  │  └─────────────────────────────────────────────────────────────┘   │ │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐   │ │ │
│  │  │  │  Embedding Generation:                                       │   │ │ │
│  │  │  │  ├── EmbedAsync() → Single text embedding                    │   │ │ │
│  │  │  │  └── EmbedBatchAsync() → Batch with progress reporting       │   │ │ │
│  │  │  └─────────────────────────────────────────────────────────────┘   │ │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐   │ │ │
│  │  │  │  Vector Operations (SIMD-accelerated):                       │   │ │ │
│  │  │  │  ├── CosineSimilarity() → Hardware-accelerated dot product   │   │ │ │
│  │  │  │  └── NormalizeEmbedding() → Unit length normalization        │   │ │ │
│  │  │  └─────────────────────────────────────────────────────────────┘   │ │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐   │ │ │
│  │  │  │  GPU Detection:                                              │   │ │ │
│  │  │  │  └── DetectOptimalGpuLayers() → Metal/CUDA/CPU detection     │   │ │ │
│  │  │  └─────────────────────────────────────────────────────────────┘   │ │ │
│  │  └────────────────────────────────────────────────────────────────────┘ │ │
│  │                                                                          │ │
│  │  ┌────────────────────────────────────────────────────────────────────┐ │ │
│  │  │                    EmbeddingServiceFactory                          │ │ │
│  │  │  Implements: IEmbeddingServiceFactory                               │ │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐   │ │ │
│  │  │  │  Factory Methods:                                            │   │ │ │
│  │  │  │  ├── CreateService(options) → IEmbeddingService              │   │ │ │
│  │  │  │  └── DetectModelType(path) → EmbeddingModelType (static)     │   │ │ │
│  │  │  └─────────────────────────────────────────────────────────────┘   │ │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐   │ │ │
│  │  │  │  Type Detection:                                             │   │ │ │
│  │  │  │  ├── .gguf → Gguf (LlamaEmbeddingService)                   │   │ │ │
│  │  │  │  ├── .onnx → Onnx (OnnxEmbeddingService)                    │   │ │ │
│  │  │  │  └── .bin → SentenceTransformers (not yet supported)        │   │ │ │
│  │  │  └─────────────────────────────────────────────────────────────┘   │ │ │
│  │  └────────────────────────────────────────────────────────────────────┘ │ │
│  │                                                                          │ │
│  │  ┌────────────────────────────────────────────────────────────────────┐ │ │
│  │  │                 EmbeddingServiceExtensions                          │ │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐   │ │ │
│  │  │  │  Extension Method:                                           │   │ │ │
│  │  │  │  └── AddEmbeddingServices(IServiceCollection)                │   │ │ │
│  │  │  │      ├── Registers LlamaEmbeddingService (Singleton)        │   │ │ │
│  │  │  │      ├── Registers OnnxEmbeddingService (Singleton)         │   │ │ │
│  │  │  │      ├── Registers IEmbeddingServiceFactory (Singleton)     │   │ │ │
│  │  │  │      └── Registers IEmbeddingModelRegistry (Singleton)      │   │ │ │
│  │  │  └─────────────────────────────────────────────────────────────┘   │ │ │
│  │  └────────────────────────────────────────────────────────────────────┘ │ │
│  │                                                                          │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │                        LLamaSharp Integration                            │ │
│  │  NuGet Packages                                                          │ │
│  ├─────────────────────────────────────────────────────────────────────────┤ │
│  │                                                                          │ │
│  │  ┌────────────────────────────────────────────────────────────────────┐ │ │
│  │  │  Package Dependencies                                               │ │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐   │ │ │
│  │  │  │  LLamaSharp (v0.18.0)                                        │   │ │ │
│  │  │  │  ├── LLamaWeights - Model weight loading                     │   │ │ │
│  │  │  │  ├── LLamaEmbedder - Embedding generation                    │   │ │ │
│  │  │  │  ├── ModelParams - Model configuration                       │   │ │ │
│  │  │  │  └── NativeLibraryConfig - Backend detection                 │   │ │ │
│  │  │  └─────────────────────────────────────────────────────────────┘   │ │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐   │ │ │
│  │  │  │  LLamaSharp.Backend.Cpu (v0.18.0)                            │   │ │ │
│  │  │  │  └── CPU fallback for all platforms                          │   │ │ │
│  │  │  └─────────────────────────────────────────────────────────────┘   │ │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐   │ │ │
│  │  │  │  Platform-Specific Backends (runtime detection)              │   │ │ │
│  │  │  │  ├── Metal (macOS) - Native GPU acceleration                │   │ │ │
│  │  │  │  └── CUDA (Windows/Linux) - NVIDIA GPU acceleration         │   │ │ │
│  │  │  └─────────────────────────────────────────────────────────────┘   │ │ │
│  │  └────────────────────────────────────────────────────────────────────┘ │ │
│  │                                                                          │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
└──────────────────────────────────────────────────────────────────────────────┘
```

---

## Thread Safety Model

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                        Thread Safety Architecture                             │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  Semaphore-Based Locking                                                 │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │                                                                    │  │ │
│  │  │  _loadLock: SemaphoreSlim(1, 1)                                   │  │ │
│  │  │  ├── Protects: LoadModelAsync, UnloadModelAsync                   │  │ │
│  │  │  ├── Ensures: Only one load/unload operation at a time           │  │ │
│  │  │  └── Scope: Model lifecycle operations                            │  │ │
│  │  │                                                                    │  │ │
│  │  │  _embedLock: SemaphoreSlim(1, 1)                                  │  │ │
│  │  │  ├── Protects: EmbedAsync, EmbedBatchAsync                        │  │ │
│  │  │  ├── Ensures: One embedding operation at a time                   │  │ │
│  │  │  └── Scope: Inference operations                                   │  │ │
│  │  │                                                                    │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  Concurrent Access Flow                                                  │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │                                                                    │  │ │
│  │  │  Request 1: EmbedAsync("hello")                                   │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────┐                                                   │  │ │
│  │  │  │ _embedLock  │ ◄──── Acquire                                    │  │ │
│  │  │  │  .WaitAsync │                                                   │  │ │
│  │  │  └──────┬──────┘                                                   │  │ │
│  │  │         │                                                          │  │ │
│  │  │         ▼                                                          │  │ │
│  │  │  ┌─────────────┐       Request 2: EmbedAsync("world")             │  │ │
│  │  │  │  Generate   │              │                                    │  │ │
│  │  │  │  Embedding  │              ▼                                    │  │ │
│  │  │  │   (work)    │       ┌─────────────┐                            │  │ │
│  │  │  └──────┬──────┘       │  _embedLock │ ◄──── Waiting              │  │ │
│  │  │         │              │  .WaitAsync │                             │  │ │
│  │  │         ▼              └──────┬──────┘                             │  │ │
│  │  │  ┌─────────────┐              │                                    │  │ │
│  │  │  │ _embedLock  │ ◄──── Release│                                   │  │ │
│  │  │  │  .Release   │              │                                    │  │ │
│  │  │  └─────────────┘              ▼                                    │  │ │
│  │  │         │              ┌─────────────┐                             │  │ │
│  │  │         ▼              │  Acquired   │ ◄──── Now allowed           │  │ │
│  │  │    Return result       │  Generate   │                             │  │ │
│  │  │                        │  Embedding  │                             │  │ │
│  │  │                        └─────────────┘                             │  │ │
│  │  │                                                                    │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  Why Two Locks?                                                          │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │                                                                    │  │ │
│  │  │  Scenario: UnloadModelAsync called during EmbedBatchAsync         │  │ │
│  │  │                                                                    │  │ │
│  │  │  Thread 1 (Batch Embedding):    Thread 2 (Unload Request):        │  │ │
│  │  │  ─────────────────────────────  ─────────────────────────────     │  │ │
│  │  │  await _embedLock.WaitAsync()   await _loadLock.WaitAsync()       │  │ │
│  │  │  // Holds _embedLock             // Holds _loadLock                │  │ │
│  │  │  for (each text):               UnloadModelInternalAsync()         │  │ │
│  │  │    GetEmbeddings(text)          // Waits for _embedLock?          │  │ │
│  │  │  _embedLock.Release()           // No! Separate concerns.          │  │ │
│  │  │                                                                    │  │ │
│  │  │  Result: Batch completes, then model unloads safely.              │  │ │
│  │  │                                                                    │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
└──────────────────────────────────────────────────────────────────────────────┘
```

---

## GPU Detection Flow

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                      DetectOptimalGpuLayers() Flow                            │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  Detection Algorithm                                                     │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │                                                                    │  │ │
│  │  │                        Start                                       │  │ │
│  │  │                          │                                         │  │ │
│  │  │                          ▼                                         │  │ │
│  │  │                 ┌─────────────────┐                                │  │ │
│  │  │                 │ Is macOS?       │                                │  │ │
│  │  │                 │ (Metal backend) │                                │  │ │
│  │  │                 └────────┬────────┘                                │  │ │
│  │  │                    Yes   │   No                                    │  │ │
│  │  │              ┌───────────┴───────────┐                            │  │ │
│  │  │              ▼                       ▼                             │  │ │
│  │  │         ┌─────────┐          ┌─────────────────┐                  │  │ │
│  │  │         │ Return  │          │ Check CUDA      │                  │  │ │
│  │  │         │   999   │          │ Availability    │                  │  │ │
│  │  │         │ (all    │          │ (NVIDIA GPU)    │                  │  │ │
│  │  │         │ layers) │          └────────┬────────┘                  │  │ │
│  │  │         └─────────┘              Yes  │  No                       │  │ │
│  │  │                            ┌──────────┴──────────┐                 │  │ │
│  │  │                            ▼                     ▼                 │  │ │
│  │  │                       ┌─────────┐           ┌─────────┐           │  │ │
│  │  │                       │ Return  │           │ Return  │           │  │ │
│  │  │                       │   999   │           │    0    │           │  │ │
│  │  │                       │ (all    │           │ (CPU    │           │  │ │
│  │  │                       │ layers) │           │  only)  │           │  │ │
│  │  │                       └─────────┘           └─────────┘           │  │ │
│  │  │                                                                    │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  Platform Behavior                                                       │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │                                                                    │  │ │
│  │  │  Platform     │ GPU Backend │ Detection Method    │ Result        │  │ │
│  │  │  ─────────────┼─────────────┼─────────────────────┼───────────────│  │ │
│  │  │  macOS        │ Metal       │ OperatingSystem     │ 999 (all GPU) │  │ │
│  │  │               │             │ .IsMacOS()          │               │  │ │
│  │  │  ─────────────┼─────────────┼─────────────────────┼───────────────│  │ │
│  │  │  Windows +    │ CUDA        │ NativeLibraryConfig │ 999 (all GPU) │  │ │
│  │  │  NVIDIA GPU   │             │ .All.WithCuda       │               │  │ │
│  │  │  ─────────────┼─────────────┼─────────────────────┼───────────────│  │ │
│  │  │  Linux +      │ CUDA        │ NativeLibraryConfig │ 999 (all GPU) │  │ │
│  │  │  NVIDIA GPU   │             │ .All.WithCuda       │               │  │ │
│  │  │  ─────────────┼─────────────┼─────────────────────┼───────────────│  │ │
│  │  │  Windows/     │ CPU only    │ CUDA check fails    │ 0 (CPU only)  │  │ │
│  │  │  Linux (no    │             │                     │               │  │ │
│  │  │  NVIDIA)      │             │                     │               │  │ │
│  │  │                                                                    │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  Why 999?                                                                │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │                                                                    │  │ │
│  │  │  Embedding models typically have fewer layers than LLMs:          │  │ │
│  │  │  ├── BERT-based: 6-12 layers                                      │  │ │
│  │  │  ├── Nomic: 12 layers                                             │  │ │
│  │  │  └── Typical embedding models: <40 layers                         │  │ │
│  │  │                                                                    │  │ │
│  │  │  Setting GpuLayers = 999 means:                                   │  │ │
│  │  │  "Load ALL available layers to GPU"                               │  │ │
│  │  │                                                                    │  │ │
│  │  │  LLamaSharp will cap at actual layer count, so 999 is safe.       │  │ │
│  │  │                                                                    │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
└──────────────────────────────────────────────────────────────────────────────┘
```

---

## Model Loading Flow

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                          LoadModelAsync() Flow                                │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  Loading Sequence                                                        │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │                                                                    │  │ │
│  │  │  LoadModelAsync(options, progress, ct)                            │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │ 1. ObjectDisposedException.ThrowIf(_isDisposed, this)       │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │ 2. Validate ModelType (must be Gguf or Auto)                │  │  │ │
│  │  │  │    Throw ArgumentException if invalid                        │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │ 3. await _loadLock.WaitAsync(ct)                            │  │  │ │
│  │  │  │    Acquire exclusive load access                             │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │ 4. If _embedder is not null:                                │  │  │ │
│  │  │  │    await UnloadModelInternalAsync()                          │  │  │ │
│  │  │  │    (Unload existing model before loading new)                │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │ 5. Report Progress: "Initializing model parameters" (5%)    │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │ 6. Configure ModelParams:                                    │  │  │ │
│  │  │  │    ├── ContextSize = options.ContextSize                    │  │  │ │
│  │  │  │    ├── GpuLayerCount = options.GpuLayers (-1 → auto-detect) │  │  │ │
│  │  │  │    ├── Threads = options.Threads (0 → ProcessorCount)       │  │  │ │
│  │  │  │    ├── BatchSize = options.BatchSize                        │  │  │ │
│  │  │  │    ├── EmbeddingMode = true                                 │  │  │ │
│  │  │  │    └── UseMemorymap = options.UseMemoryMapping              │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │ 7. Report Progress: "Loading model weights" (20%)           │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │ 8. _model = await Task.Run(() =>                            │  │  │ │
│  │  │  │        LLamaWeights.LoadFromFile(modelParams), ct)          │  │  │ │
│  │  │  │    (Run on thread pool to avoid blocking)                    │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │ 9. Report Progress: "Creating embedder" (80%)               │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │ 10. _embedder = new LLamaEmbedder(_model, modelParams)      │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │ 11. Store configuration:                                     │  │  │ │
│  │  │  │     ├── _currentOptions = options                           │  │  │ │
│  │  │  │     ├── EmbeddingDimension = _model.EmbeddingSize           │  │  │ │
│  │  │  │     ├── MaxTokens = options.ContextSize                     │  │  │ │
│  │  │  │     └── CurrentModelName = options.ModelName                │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │ 12. Report Progress: "Model ready" (100%)                   │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │ 13. Raise ModelStateChanged event (IsLoaded = true)         │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │ finally: _loadLock.Release()                                │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │                                                                    │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
└──────────────────────────────────────────────────────────────────────────────┘
```

---

## SIMD Vector Operations

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                     SIMD-Accelerated Vector Operations                        │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  CosineSimilarity Algorithm                                              │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │                                                                    │  │ │
│  │  │  Input: embedding1[N], embedding2[N] where N = EmbeddingDimension  │  │ │
│  │  │                                                                    │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │  Check: Vector.IsHardwareAccelerated?                       │  │  │ │
│  │  │  │         AND length >= Vector<float>.Count?                  │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │        │                                                          │  │ │
│  │  │   Yes  │  No                                                      │  │ │
│  │  │        ▼                                                          │  │ │
│  │  │  ┌─────────────────┐    ┌─────────────────────────────────────┐  │  │ │
│  │  │  │ SIMD Path       │    │ Scalar Fallback                     │  │  │ │
│  │  │  ├─────────────────┤    ├─────────────────────────────────────┤  │  │ │
│  │  │  │ vectorSize =    │    │ for (i = 0; i < length; i++)        │  │  │ │
│  │  │  │ Vector<float>   │    │ {                                   │  │  │ │
│  │  │  │ .Count          │    │   dot += e1[i] * e2[i];             │  │  │ │
│  │  │  │                 │    │   norm1 += e1[i] * e1[i];           │  │  │ │
│  │  │  │ // Process      │    │   norm2 += e2[i] * e2[i];           │  │  │ │
│  │  │  │ // vectorSize   │    │ }                                   │  │  │ │
│  │  │  │ // floats at    │    └─────────────────────────────────────┘  │  │ │
│  │  │  │ // once         │                                              │  │ │
│  │  │  │                 │                                              │  │ │
│  │  │  │ for (; i <=     │                                              │  │ │
│  │  │  │   len - vecSize;│                                              │  │ │
│  │  │  │   i += vecSize) │                                              │  │ │
│  │  │  │ {               │                                              │  │ │
│  │  │  │   v1 = new Vec  │                                              │  │ │
│  │  │  │     (e1[i..]);  │                                              │  │ │
│  │  │  │   v2 = new Vec  │                                              │  │ │
│  │  │  │     (e2[i..]);  │                                              │  │ │
│  │  │  │                 │                                              │  │ │
│  │  │  │   dotVec +=     │                                              │  │ │
│  │  │  │     v1 * v2;    │                                              │  │ │
│  │  │  │   norm1Vec +=   │                                              │  │ │
│  │  │  │     v1 * v1;    │                                              │  │ │
│  │  │  │   norm2Vec +=   │                                              │  │ │
│  │  │  │     v2 * v2;    │                                              │  │ │
│  │  │  │ }               │                                              │  │ │
│  │  │  │                 │                                              │  │ │
│  │  │  │ // Horizontal   │                                              │  │ │
│  │  │  │ // sum vector   │                                              │  │ │
│  │  │  │ // elements     │                                              │  │ │
│  │  │  │ for (j = 0;     │                                              │  │ │
│  │  │  │   j < vecSize;  │                                              │  │ │
│  │  │  │   j++)          │                                              │  │ │
│  │  │  │ {               │                                              │  │ │
│  │  │  │   dot +=        │                                              │  │ │
│  │  │  │     dotVec[j];  │                                              │  │ │
│  │  │  │   ...           │                                              │  │ │
│  │  │  │ }               │                                              │  │ │
│  │  │  │                 │                                              │  │ │
│  │  │  │ // Handle       │                                              │  │ │
│  │  │  │ // remaining    │                                              │  │ │
│  │  │  │ // elements     │                                              │  │ │
│  │  │  └─────────────────┘                                              │  │ │
│  │  │        │                                                          │  │ │
│  │  │        ▼                                                          │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │  magnitude = sqrt(norm1) * sqrt(norm2)                      │  │  │ │
│  │  │  │  return (magnitude > 0) ? dot / magnitude : 0               │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │                                                                    │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  NormalizeEmbedding Algorithm                                            │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │                                                                    │  │ │
│  │  │  Input: embedding[N] (mutable Span<float>)                        │  │ │
│  │  │                                                                    │  │ │
│  │  │  Step 1: Calculate sum of squares (SIMD-accelerated)              │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │  sumSquares = 0                                             │  │  │ │
│  │  │  │  if (SIMD available):                                       │  │  │ │
│  │  │  │      for each vector chunk:                                 │  │  │ │
│  │  │  │          v = new Vector<float>(embedding[i..])              │  │  │ │
│  │  │  │          sumVec += v * v                                    │  │  │ │
│  │  │  │      horizontal sum sumVec into sumSquares                  │  │  │ │
│  │  │  │      handle remaining elements with scalar loop             │  │  │ │
│  │  │  │  else:                                                      │  │  │ │
│  │  │  │      foreach val in embedding:                              │  │  │ │
│  │  │  │          sumSquares += val * val                            │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │                                                                    │  │ │
│  │  │  Step 2: Calculate magnitude                                      │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │  magnitude = MathF.Sqrt(sumSquares)                         │  │  │ │
│  │  │  │  if (magnitude <= 0) return; // No-op for zero vectors      │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │                                                                    │  │ │
│  │  │  Step 3: Normalize in place (scalar loop - small overhead)       │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │  for (i = 0; i < embedding.Length; i++)                     │  │  │ │
│  │  │  │      embedding[i] /= magnitude;                             │  │  │ │
│  │  │  │                                                             │  │  │ │
│  │  │  │  // Result: ||embedding|| = 1                               │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │                                                                    │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  SIMD Vector Width by Platform                                           │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │                                                                    │  │ │
│  │  │  Instruction Set │ Vector<float>.Count │ Floats per operation    │  │ │
│  │  │  ────────────────┼─────────────────────┼─────────────────────────│  │ │
│  │  │  AVX2            │         8           │ 8 floats (256-bit)      │  │ │
│  │  │  AVX-512         │        16           │ 16 floats (512-bit)     │  │ │
│  │  │  SSE             │         4           │ 4 floats (128-bit)      │  │ │
│  │  │  ARM NEON        │         4           │ 4 floats (128-bit)      │  │ │
│  │  │  Apple Silicon   │         4           │ 4 floats (128-bit)      │  │ │
│  │  │                                                                    │  │ │
│  │  │  Performance Impact:                                               │  │ │
│  │  │  ├── 768-dim embedding: 96-192 vector ops vs 768 scalar ops      │  │ │
│  │  │  ├── Speedup: 4-8x depending on hardware                         │  │ │
│  │  │  └── Critical for batch similarity calculations                   │  │ │
│  │  │                                                                    │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
└──────────────────────────────────────────────────────────────────────────────┘
```

---

## Factory Pattern

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                       EmbeddingServiceFactory Pattern                         │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  Factory Creation Flow                                                   │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │                                                                    │  │ │
│  │  │  CreateService(options)                                            │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │  Is ModelType == Auto?                                      │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │  Yes  │  No                                                        │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │  DetectModelType(options.ModelPath)                         │  │  │ │
│  │  │  │  ├── Extension ".gguf" → Gguf                               │  │  │ │
│  │  │  │  ├── Extension ".onnx" → Onnx                               │  │  │ │
│  │  │  │  ├── Extension ".bin"  → SentenceTransformers               │  │  │ │
│  │  │  │  └── Otherwise         → ArgumentException                  │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │       │                                                            │  │ │
│  │  │       ▼                                                            │  │ │
│  │  │  ┌─────────────────────────────────────────────────────────────┐  │  │ │
│  │  │  │  Switch on modelType:                                       │  │  │ │
│  │  │  │                                                             │  │  │ │
│  │  │  │  case Gguf:                                                 │  │  │ │
│  │  │  │      return _serviceProvider                                │  │  │ │
│  │  │  │          .GetRequiredService<LlamaEmbeddingService>()       │  │  │ │
│  │  │  │                                                             │  │  │ │
│  │  │  │  case Onnx:                                                 │  │  │ │
│  │  │  │      return _serviceProvider                                │  │  │ │
│  │  │  │          .GetRequiredService<OnnxEmbeddingService>()        │  │  │ │
│  │  │  │                                                             │  │  │ │
│  │  │  │  case SentenceTransformers:                                 │  │  │ │
│  │  │  │      throw NotSupportedException(...)                       │  │  │ │
│  │  │  │                                                             │  │  │ │
│  │  │  │  default:                                                   │  │  │ │
│  │  │  │      throw ArgumentException(...)                           │  │  │ │
│  │  │  └─────────────────────────────────────────────────────────────┘  │  │ │
│  │  │                                                                    │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  Factory Usage Example                                                   │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │                                                                    │  │ │
│  │  │  // Inject factory via DI                                         │  │ │
│  │  │  public class EmbeddingManager                                    │  │ │
│  │  │  {                                                                 │  │ │
│  │  │      private readonly IEmbeddingServiceFactory _factory;          │  │ │
│  │  │      private IEmbeddingService? _activeService;                   │  │ │
│  │  │                                                                    │  │ │
│  │  │      public async Task LoadModelAsync(string modelPath, ct)       │  │ │
│  │  │      {                                                             │  │ │
│  │  │          var options = new EmbeddingModelOptions                  │  │ │
│  │  │          {                                                         │  │ │
│  │  │              ModelPath = modelPath,                               │  │ │
│  │  │              ModelType = EmbeddingModelType.Auto // Let factory   │  │ │
│  │  │          };                                       // decide       │  │ │
│  │  │                                                                    │  │ │
│  │  │          // Dispose previous service if any                       │  │ │
│  │  │          if (_activeService is not null)                          │  │ │
│  │  │              await _activeService.DisposeAsync();                 │  │ │
│  │  │                                                                    │  │ │
│  │  │          // Factory creates appropriate service type              │  │ │
│  │  │          _activeService = _factory.CreateService(options);        │  │ │
│  │  │          await _activeService.LoadModelAsync(options, null, ct);  │  │ │
│  │  │      }                                                             │  │ │
│  │  │  }                                                                 │  │ │
│  │  │                                                                    │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
└──────────────────────────────────────────────────────────────────────────────┘
```

---

## File Specifications

### 1. LlamaEmbeddingService.cs

**Location**: `src/AIntern.Services/Embedding/LlamaEmbeddingService.cs`
**Type**: `sealed class`
**Implements**: `IEmbeddingService`
**Purpose**: GGUF embedding model loading and inference using LLamaSharp

```csharp
namespace AIntern.Services.Embedding;

using System;
using System.Collections.Generic;
using System.Diagnostics;
using System.Linq;
using System.Numerics;
using System.Threading;
using System.Threading.Tasks;
using LLama;
using LLama.Common;
using LLama.Native;
using Microsoft.Extensions.Logging;
using AIntern.Core.Interfaces;
using AIntern.Core.Models;

/// <summary>
/// Embedding service implementation using LLamaSharp for GGUF models.
/// Thread-safe with semaphore-based locking for model loading and inference.
/// </summary>
public sealed class LlamaEmbeddingService : IEmbeddingService
{
    private readonly ILogger<LlamaEmbeddingService> _logger;
    private readonly SemaphoreSlim _loadLock = new(1, 1);
    private readonly SemaphoreSlim _embedLock = new(1, 1);

    private LLamaWeights? _model;
    private LLamaEmbedder? _embedder;
    private EmbeddingModelOptions? _currentOptions;
    private bool _isDisposed;

    /// <inheritdoc />
    public bool IsModelLoaded => _embedder is not null;

    /// <inheritdoc />
    public int EmbeddingDimension { get; private set; }

    /// <inheritdoc />
    public int MaxTokens { get; private set; } = 512;

    /// <inheritdoc />
    public string? CurrentModelName { get; private set; }

    /// <inheritdoc />
    public event EventHandler<EmbeddingModelStateChangedEventArgs>? ModelStateChanged;

    /// <summary>
    /// Initializes a new instance of the LlamaEmbeddingService.
    /// </summary>
    /// <param name="logger">Logger for diagnostic output.</param>
    public LlamaEmbeddingService(ILogger<LlamaEmbeddingService> logger)
    {
        _logger = logger;
    }

    /// <inheritdoc />
    public async Task LoadModelAsync(
        EmbeddingModelOptions options,
        IProgress<ModelLoadProgress>? progress = null,
        CancellationToken ct = default)
    {
        ObjectDisposedException.ThrowIf(_isDisposed, this);

        if (options.ModelType != EmbeddingModelType.Gguf &&
            options.ModelType != EmbeddingModelType.Auto)
        {
            throw new ArgumentException(
                $"LlamaEmbeddingService only supports GGUF models, not {options.ModelType}",
                nameof(options));
        }

        await _loadLock.WaitAsync(ct);
        try
        {
            // Unload existing model if any
            if (_embedder is not null)
            {
                _logger.LogInformation("Unloading existing embedding model");
                await UnloadModelInternalAsync();
            }

            _logger.LogInformation("Loading GGUF embedding model: {Path}", options.ModelPath);

            progress?.Report(new ModelLoadProgress
            {
                Stage = "Initializing model parameters",
                PercentComplete = 5
            });

            // Configure model parameters
            var modelParams = new ModelParams(options.ModelPath)
            {
                ContextSize = (uint)options.ContextSize,
                GpuLayerCount = options.GpuLayers == -1
                    ? DetectOptimalGpuLayers()
                    : options.GpuLayers,
                Threads = options.Threads == 0
                    ? (uint)Environment.ProcessorCount
                    : (uint)options.Threads,
                BatchSize = (uint)options.BatchSize,
                EmbeddingMode = true,
                UseMemorymap = options.UseMemoryMapping
            };

            progress?.Report(new ModelLoadProgress
            {
                Stage = "Loading model weights",
                PercentComplete = 20
            });

            // Load model weights on thread pool
            _model = await Task.Run(
                () => LLamaWeights.LoadFromFile(modelParams),
                ct);

            progress?.Report(new ModelLoadProgress
            {
                Stage = "Creating embedder",
                PercentComplete = 80
            });

            // Create embedder
            _embedder = new LLamaEmbedder(_model, modelParams);

            // Store configuration
            _currentOptions = options;
            EmbeddingDimension = _model.EmbeddingSize;
            MaxTokens = options.ContextSize;
            CurrentModelName = options.ModelName;

            progress?.Report(new ModelLoadProgress
            {
                Stage = "Model ready",
                PercentComplete = 100
            });

            _logger.LogInformation(
                "Loaded embedding model: {Name}, Dimension: {Dim}, GPU Layers: {Gpu}",
                options.ModelName,
                EmbeddingDimension,
                modelParams.GpuLayerCount);

            ModelStateChanged?.Invoke(this, new EmbeddingModelStateChangedEventArgs
            {
                IsLoaded = true,
                ModelPath = options.ModelPath,
                ModelName = options.ModelName,
                EmbeddingDimension = EmbeddingDimension
            });
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Failed to load embedding model: {Path}", options.ModelPath);

            ModelStateChanged?.Invoke(this, new EmbeddingModelStateChangedEventArgs
            {
                IsLoaded = false,
                Error = ex.Message
            });

            throw;
        }
        finally
        {
            _loadLock.Release();
        }
    }

    /// <inheritdoc />
    public async Task UnloadModelAsync()
    {
        await _loadLock.WaitAsync();
        try
        {
            await UnloadModelInternalAsync();
        }
        finally
        {
            _loadLock.Release();
        }
    }

    /// <summary>
    /// Internal unload method (caller must hold _loadLock).
    /// </summary>
    private Task UnloadModelInternalAsync()
    {
        _embedder?.Dispose();
        _embedder = null;

        _model?.Dispose();
        _model = null;

        _currentOptions = null;
        EmbeddingDimension = 0;
        MaxTokens = 512;
        CurrentModelName = null;

        // Force garbage collection to release GPU memory
        GC.Collect();
        GC.WaitForPendingFinalizers();

        ModelStateChanged?.Invoke(this, new EmbeddingModelStateChangedEventArgs
        {
            IsLoaded = false
        });

        _logger.LogInformation("Embedding model unloaded");

        return Task.CompletedTask;
    }

    /// <inheritdoc />
    public async Task<float[]> EmbedAsync(string text, CancellationToken ct = default)
    {
        ObjectDisposedException.ThrowIf(_isDisposed, this);

        if (_embedder is null)
            throw new InvalidOperationException("No embedding model is loaded");

        if (string.IsNullOrEmpty(text))
            return new float[EmbeddingDimension];

        // Apply text prefix if configured
        var processedText = _currentOptions?.TextPrefix is not null
            ? _currentOptions.TextPrefix + text
            : text;

        await _embedLock.WaitAsync(ct);
        try
        {
            var embeddings = await Task.Run(
                () => _embedder.GetEmbeddings(processedText),
                ct);

            var embedding = embeddings.First();

            // Normalize if configured
            if (_currentOptions?.NormalizeEmbeddings == true)
                NormalizeEmbedding(embedding);

            return embedding;
        }
        finally
        {
            _embedLock.Release();
        }
    }

    /// <inheritdoc />
    public async Task<IReadOnlyList<float[]>> EmbedBatchAsync(
        IEnumerable<string> texts,
        IProgress<EmbeddingProgress>? progress = null,
        CancellationToken ct = default)
    {
        ObjectDisposedException.ThrowIf(_isDisposed, this);

        if (_embedder is null)
            throw new InvalidOperationException("No embedding model is loaded");

        var textList = texts.ToList();
        if (textList.Count == 0)
            return Array.Empty<float[]>();

        var results = new List<float[]>(textList.Count);
        var stopwatch = Stopwatch.StartNew();

        await _embedLock.WaitAsync(ct);
        try
        {
            var batchSize = _currentOptions?.BatchSize ?? 32;

            for (int i = 0; i < textList.Count; i++)
            {
                ct.ThrowIfCancellationRequested();

                var text = textList[i];

                // Apply text prefix if configured
                if (_currentOptions?.TextPrefix is not null)
                    text = _currentOptions.TextPrefix + text;

                // Handle empty text
                if (string.IsNullOrEmpty(text))
                {
                    results.Add(new float[EmbeddingDimension]);
                    continue;
                }

                var embeddings = await Task.Run(
                    () => _embedder.GetEmbeddings(text),
                    ct);

                var embedding = embeddings.First();

                // Normalize if configured
                if (_currentOptions?.NormalizeEmbeddings == true)
                    NormalizeEmbedding(embedding);

                results.Add(embedding);

                // Report progress every 10 items or at completion
                if (progress is not null && ((i + 1) % 10 == 0 || i == textList.Count - 1))
                {
                    var elapsed = stopwatch.Elapsed;
                    var rate = (i + 1) / elapsed.TotalSeconds;
                    var remaining = TimeSpan.FromSeconds((textList.Count - i - 1) / rate);

                    progress.Report(new EmbeddingProgress
                    {
                        ProcessedCount = i + 1,
                        TotalCount = textList.Count,
                        TextsPerSecond = rate,
                        EstimatedRemaining = remaining
                    });
                }
            }
        }
        finally
        {
            _embedLock.Release();
        }

        _logger.LogDebug(
            "Generated {Count} embeddings in {Elapsed:F2}s ({Rate:F1}/s)",
            results.Count,
            stopwatch.Elapsed.TotalSeconds,
            results.Count / stopwatch.Elapsed.TotalSeconds);

        return results;
    }

    /// <inheritdoc />
    public float CosineSimilarity(ReadOnlySpan<float> embedding1, ReadOnlySpan<float> embedding2)
    {
        if (embedding1.Length != embedding2.Length)
            throw new ArgumentException("Embeddings must have the same dimension");

        if (embedding1.Length == 0)
            return 0f;

        // Use SIMD for efficient computation
        float dot = 0f, norm1 = 0f, norm2 = 0f;

        // Try to use hardware acceleration
        if (Vector.IsHardwareAccelerated && embedding1.Length >= Vector<float>.Count)
        {
            var vectorSize = Vector<float>.Count;
            var i = 0;

            var dotVec = Vector<float>.Zero;
            var norm1Vec = Vector<float>.Zero;
            var norm2Vec = Vector<float>.Zero;

            for (; i <= embedding1.Length - vectorSize; i += vectorSize)
            {
                var v1 = new Vector<float>(embedding1.Slice(i, vectorSize));
                var v2 = new Vector<float>(embedding2.Slice(i, vectorSize));

                dotVec += v1 * v2;
                norm1Vec += v1 * v1;
                norm2Vec += v2 * v2;
            }

            // Sum vector elements
            for (int j = 0; j < vectorSize; j++)
            {
                dot += dotVec[j];
                norm1 += norm1Vec[j];
                norm2 += norm2Vec[j];
            }

            // Handle remaining elements
            for (; i < embedding1.Length; i++)
            {
                dot += embedding1[i] * embedding2[i];
                norm1 += embedding1[i] * embedding1[i];
                norm2 += embedding2[i] * embedding2[i];
            }
        }
        else
        {
            // Scalar fallback
            for (int i = 0; i < embedding1.Length; i++)
            {
                dot += embedding1[i] * embedding2[i];
                norm1 += embedding1[i] * embedding1[i];
                norm2 += embedding2[i] * embedding2[i];
            }
        }

        var magnitude = MathF.Sqrt(norm1) * MathF.Sqrt(norm2);
        return magnitude > 0 ? dot / magnitude : 0f;
    }

    /// <inheritdoc />
    public void NormalizeEmbedding(Span<float> embedding)
    {
        if (embedding.Length == 0)
            return;

        float sumSquares = 0f;

        // Use SIMD if available
        if (Vector.IsHardwareAccelerated && embedding.Length >= Vector<float>.Count)
        {
            var vectorSize = Vector<float>.Count;
            var sumVec = Vector<float>.Zero;
            var i = 0;

            for (; i <= embedding.Length - vectorSize; i += vectorSize)
            {
                var v = new Vector<float>(embedding.Slice(i, vectorSize));
                sumVec += v * v;
            }

            for (int j = 0; j < vectorSize; j++)
                sumSquares += sumVec[j];

            for (; i < embedding.Length; i++)
                sumSquares += embedding[i] * embedding[i];
        }
        else
        {
            foreach (var val in embedding)
                sumSquares += val * val;
        }

        var magnitude = MathF.Sqrt(sumSquares);
        if (magnitude <= 0)
            return;

        // Normalize in place
        for (int i = 0; i < embedding.Length; i++)
            embedding[i] /= magnitude;
    }

    /// <summary>
    /// Detect optimal GPU layer count based on available hardware.
    /// Returns 999 for GPU-enabled platforms (Metal/CUDA), 0 for CPU-only.
    /// </summary>
    private static int DetectOptimalGpuLayers()
    {
        // For embedding models, typically load all layers to GPU if available
        if (OperatingSystem.IsMacOS())
        {
            // Metal backend - use all layers
            return 999;
        }

        // Check for CUDA availability
        try
        {
            var cudaAvailable = NativeLibraryConfig.All.WithCuda;
            if (cudaAvailable)
                return 999;
        }
        catch
        {
            // CUDA not available
        }

        // CPU only
        return 0;
    }

    /// <inheritdoc />
    public async ValueTask DisposeAsync()
    {
        if (_isDisposed)
            return;

        _isDisposed = true;

        await UnloadModelAsync();

        _loadLock.Dispose();
        _embedLock.Dispose();
    }
}
```

---

### 2. EmbeddingServiceFactory.cs

**Location**: `src/AIntern.Services/Embedding/EmbeddingServiceFactory.cs`
**Purpose**: Factory for creating appropriate embedding service instances based on model type

```csharp
namespace AIntern.Services.Embedding;

using System;
using System.IO;
using Microsoft.Extensions.DependencyInjection;
using Microsoft.Extensions.Logging;
using AIntern.Core.Interfaces;
using AIntern.Core.Models;

/// <summary>
/// Factory for creating appropriate embedding service instances based on model type.
/// </summary>
public sealed class EmbeddingServiceFactory : IEmbeddingServiceFactory
{
    private readonly IServiceProvider _serviceProvider;
    private readonly ILogger<EmbeddingServiceFactory> _logger;

    /// <summary>
    /// Initializes a new instance of the EmbeddingServiceFactory.
    /// </summary>
    /// <param name="serviceProvider">Service provider for resolving service instances.</param>
    /// <param name="logger">Logger for diagnostic output.</param>
    public EmbeddingServiceFactory(
        IServiceProvider serviceProvider,
        ILogger<EmbeddingServiceFactory> logger)
    {
        _serviceProvider = serviceProvider;
        _logger = logger;
    }

    /// <summary>
    /// Create an embedding service for the specified model options.
    /// </summary>
    /// <param name="options">Model configuration options.</param>
    /// <returns>An embedding service appropriate for the model type.</returns>
    /// <exception cref="NotSupportedException">
    /// Thrown when SentenceTransformers model type is specified.
    /// </exception>
    /// <exception cref="ArgumentException">
    /// Thrown when model type cannot be determined or is unknown.
    /// </exception>
    public IEmbeddingService CreateService(EmbeddingModelOptions options)
    {
        var modelType = options.ModelType;

        // Auto-detect model type from file extension
        if (modelType == EmbeddingModelType.Auto)
        {
            modelType = DetectModelType(options.ModelPath);
            _logger.LogDebug(
                "Auto-detected model type {Type} for {Path}",
                modelType, options.ModelPath);
        }

        return modelType switch
        {
            EmbeddingModelType.Gguf => _serviceProvider
                .GetRequiredService<LlamaEmbeddingService>(),

            EmbeddingModelType.Onnx => _serviceProvider
                .GetRequiredService<OnnxEmbeddingService>(),

            EmbeddingModelType.SentenceTransformers => throw new NotSupportedException(
                "SentenceTransformers models are not yet supported"),

            _ => throw new ArgumentException($"Unknown model type: {modelType}")
        };
    }

    /// <summary>
    /// Detect model type from file extension.
    /// </summary>
    /// <param name="modelPath">Path to the model file.</param>
    /// <returns>Detected model type.</returns>
    /// <exception cref="ArgumentException">
    /// Thrown when model type cannot be determined from extension.
    /// </exception>
    public static EmbeddingModelType DetectModelType(string modelPath)
    {
        var extension = Path.GetExtension(modelPath).ToLowerInvariant();

        return extension switch
        {
            ".gguf" => EmbeddingModelType.Gguf,
            ".onnx" => EmbeddingModelType.Onnx,
            ".bin" => EmbeddingModelType.SentenceTransformers,
            _ => throw new ArgumentException(
                $"Cannot determine model type from extension: {extension}")
        };
    }
}

/// <summary>
/// Interface for embedding service factory.
/// </summary>
public interface IEmbeddingServiceFactory
{
    /// <summary>
    /// Create an embedding service for the specified model options.
    /// </summary>
    /// <param name="options">Model configuration options.</param>
    /// <returns>An embedding service appropriate for the model type.</returns>
    IEmbeddingService CreateService(EmbeddingModelOptions options);
}
```

---

### 3. EmbeddingServiceExtensions.cs

**Location**: `src/AIntern.Services/Embedding/EmbeddingServiceExtensions.cs`
**Purpose**: Dependency injection registration helper methods

```csharp
namespace AIntern.Services.Embedding;

using Microsoft.Extensions.DependencyInjection;
using AIntern.Core.Interfaces;

/// <summary>
/// Extension methods for registering embedding services with the DI container.
/// </summary>
public static class EmbeddingServiceExtensions
{
    /// <summary>
    /// Add embedding services to the service collection.
    /// Registers all embedding service implementations, factory, and model registry.
    /// </summary>
    /// <param name="services">The service collection to add services to.</param>
    /// <returns>The service collection for chaining.</returns>
    /// <remarks>
    /// Registers the following services as singletons:
    /// - LlamaEmbeddingService (GGUF model support)
    /// - OnnxEmbeddingService (ONNX model support)
    /// - IEmbeddingServiceFactory (factory for creating services)
    /// - IEmbeddingModelRegistry (model discovery and management)
    /// </remarks>
    public static IServiceCollection AddEmbeddingServices(this IServiceCollection services)
    {
        // Register individual service implementations
        services.AddSingleton<LlamaEmbeddingService>();
        services.AddSingleton<OnnxEmbeddingService>();

        // Register factory
        services.AddSingleton<IEmbeddingServiceFactory, EmbeddingServiceFactory>();

        // Register model registry
        services.AddSingleton<IEmbeddingModelRegistry, EmbeddingModelRegistry>();

        return services;
    }
}
```

---

### 4. Project File Modification

**Location**: `src/AIntern.Services/AIntern.Services.csproj`
**Change**: Add LLamaSharp package references

```xml
<ItemGroup>
  <!-- Existing packages... -->

  <!-- LLamaSharp for GGUF embedding model support -->
  <PackageReference Include="LLamaSharp" Version="0.18.0" />
  <PackageReference Include="LLamaSharp.Backend.Cpu" Version="0.18.0" Condition="'$(RuntimeIdentifier)' == ''" />
</ItemGroup>
```

**Package Details**:

| Package | Version | Purpose |
|---------|---------|---------|
| `LLamaSharp` | 0.18.0 | Core LLamaSharp library for GGUF model loading |
| `LLamaSharp.Backend.Cpu` | 0.18.0 | CPU backend for cross-platform compatibility |

**Backend Selection**:
- CPU backend is included by default for development
- Metal backend (macOS) is auto-selected at runtime
- CUDA backend requires separate installation for NVIDIA GPU support

---

## File Summary

### Files to Create

| File | Location | Purpose |
|------|----------|---------|
| `LlamaEmbeddingService.cs` | `src/AIntern.Services/Embedding/` | GGUF model loading and embedding generation |
| `EmbeddingServiceFactory.cs` | `src/AIntern.Services/Embedding/` | Factory for creating embedding services |
| `EmbeddingServiceExtensions.cs` | `src/AIntern.Services/Embedding/` | DI registration extension methods |

### Files to Modify

| File | Location | Change |
|------|----------|--------|
| `AIntern.Services.csproj` | `src/AIntern.Services/` | Add LLamaSharp package references |

### Directory Structure

```
src/AIntern.Services/
├── Embedding/
│   ├── LlamaEmbeddingService.cs       (NEW - v0.7.1b)
│   ├── EmbeddingServiceFactory.cs     (NEW - v0.7.1b)
│   └── EmbeddingServiceExtensions.cs  (NEW - v0.7.1b)
└── AIntern.Services.csproj            (MODIFIED - v0.7.1b)
```

---

## Usage Examples

### Basic Model Loading and Embedding

```csharp
// Inject via DI
public class CodeSearchService
{
    private readonly LlamaEmbeddingService _embedding;

    public CodeSearchService(LlamaEmbeddingService embedding)
    {
        _embedding = embedding;
    }

    public async Task InitializeAsync(CancellationToken ct)
    {
        var options = new EmbeddingModelOptions
        {
            ModelPath = "/models/nomic-embed-text-v1.5.Q8_0.gguf",
            ModelName = "Nomic Embed Text",
            ModelType = EmbeddingModelType.Gguf,
            GpuLayers = -1,  // Auto-detect
            ContextSize = 8192,
            NormalizeEmbeddings = true,
            TextPrefix = "search_document: "
        };

        var progress = new Progress<ModelLoadProgress>(p =>
            Console.WriteLine($"{p.Stage}: {p.PercentComplete}%"));

        await _embedding.LoadModelAsync(options, progress, ct);
    }

    public async Task<float[]> GetEmbeddingAsync(string text, CancellationToken ct)
    {
        return await _embedding.EmbedAsync(text, ct);
    }
}
```

### Factory-Based Model Selection

```csharp
public class FlexibleEmbeddingService
{
    private readonly IEmbeddingServiceFactory _factory;
    private IEmbeddingService? _activeService;

    public FlexibleEmbeddingService(IEmbeddingServiceFactory factory)
    {
        _factory = factory;
    }

    public async Task LoadAsync(string modelPath, CancellationToken ct)
    {
        var options = new EmbeddingModelOptions
        {
            ModelPath = modelPath,
            ModelType = EmbeddingModelType.Auto  // Factory auto-detects
        };

        // Dispose previous if switching models
        if (_activeService is not null)
            await _activeService.DisposeAsync();

        // Factory creates correct service type
        _activeService = _factory.CreateService(options);
        await _activeService.LoadModelAsync(options, null, ct);
    }
}
```

### Batch Processing with Progress

```csharp
public async Task IndexCodebaseAsync(
    IReadOnlyList<CodeFile> files,
    CancellationToken ct)
{
    var texts = files.Select(f => f.Content).ToList();

    var progress = new Progress<EmbeddingProgress>(p =>
    {
        Console.WriteLine(
            $"Embedding {p.ProcessedCount}/{p.TotalCount} " +
            $"({p.PercentComplete:F1}%) - {p.TextsPerSecond:F1}/s");
    });

    var embeddings = await _embedding.EmbedBatchAsync(texts, progress, ct);

    // Store embeddings with file metadata
    for (int i = 0; i < files.Count; i++)
    {
        await _vectorStore.AddAsync(
            files[i].Path,
            embeddings[i],
            ct);
    }
}
```

### Similarity Search

```csharp
public async Task<IEnumerable<SearchResult>> SearchAsync(
    string query,
    IEnumerable<(string Text, float[] Embedding)> documents,
    int topK = 10,
    CancellationToken ct = default)
{
    // Get query embedding
    var queryEmbedding = await _embedding.EmbedAsync(query, ct);

    // Calculate similarities using SIMD-accelerated method
    var results = documents
        .Select(doc => new SearchResult
        {
            Text = doc.Text,
            Score = _embedding.CosineSimilarity(queryEmbedding, doc.Embedding)
        })
        .OrderByDescending(r => r.Score)
        .Take(topK);

    return results;
}
```

### DI Container Registration

```csharp
// In App.axaml.cs or Startup.cs
public static void ConfigureServices(IServiceCollection services)
{
    // Register all embedding services
    services.AddEmbeddingServices();

    // This registers:
    // - LlamaEmbeddingService (singleton)
    // - OnnxEmbeddingService (singleton)
    // - IEmbeddingServiceFactory (singleton)
    // - IEmbeddingModelRegistry (singleton)
}
```

---

## Verification Steps

### 1. Build Verification

```bash
# Verify Services project builds with LLamaSharp
dotnet build src/AIntern.Services

# Verify no warnings
dotnet build src/AIntern.Services --warnaserror
```

### 2. Package Restoration

```bash
# Restore NuGet packages
dotnet restore src/AIntern.Services

# Verify LLamaSharp packages are installed
dotnet list src/AIntern.Services package
```

### 3. Unit Test Suggestions

```csharp
public class LlamaEmbeddingServiceTests
{
    [Fact]
    public void CosineSimilarity_IdenticalVectors_ReturnsOne()
    {
        var logger = NullLogger<LlamaEmbeddingService>.Instance;
        var service = new LlamaEmbeddingService(logger);

        var embedding = new float[] { 0.1f, 0.2f, 0.3f, 0.4f };

        var similarity = service.CosineSimilarity(embedding, embedding);

        Assert.Equal(1.0f, similarity, precision: 5);
    }

    [Fact]
    public void CosineSimilarity_OrthogonalVectors_ReturnsZero()
    {
        var logger = NullLogger<LlamaEmbeddingService>.Instance;
        var service = new LlamaEmbeddingService(logger);

        var embedding1 = new float[] { 1f, 0f, 0f, 0f };
        var embedding2 = new float[] { 0f, 1f, 0f, 0f };

        var similarity = service.CosineSimilarity(embedding1, embedding2);

        Assert.Equal(0f, similarity, precision: 5);
    }

    [Fact]
    public void CosineSimilarity_DifferentLengths_ThrowsArgumentException()
    {
        var logger = NullLogger<LlamaEmbeddingService>.Instance;
        var service = new LlamaEmbeddingService(logger);

        var embedding1 = new float[] { 1f, 2f, 3f };
        var embedding2 = new float[] { 1f, 2f };

        Assert.Throws<ArgumentException>(() =>
            service.CosineSimilarity(embedding1, embedding2));
    }

    [Fact]
    public void NormalizeEmbedding_ProducesUnitVector()
    {
        var logger = NullLogger<LlamaEmbeddingService>.Instance;
        var service = new LlamaEmbeddingService(logger);

        var embedding = new float[] { 3f, 4f, 0f, 0f }; // Length = 5

        service.NormalizeEmbedding(embedding);

        // Should be [0.6, 0.8, 0, 0]
        Assert.Equal(0.6f, embedding[0], precision: 5);
        Assert.Equal(0.8f, embedding[1], precision: 5);

        // Verify unit length
        var length = MathF.Sqrt(embedding.Sum(x => x * x));
        Assert.Equal(1f, length, precision: 5);
    }

    [Fact]
    public void NormalizeEmbedding_ZeroVector_RemainsZero()
    {
        var logger = NullLogger<LlamaEmbeddingService>.Instance;
        var service = new LlamaEmbeddingService(logger);

        var embedding = new float[] { 0f, 0f, 0f, 0f };

        service.NormalizeEmbedding(embedding);

        Assert.All(embedding, val => Assert.Equal(0f, val));
    }

    [Fact]
    public void IsModelLoaded_WhenNoModel_ReturnsFalse()
    {
        var logger = NullLogger<LlamaEmbeddingService>.Instance;
        var service = new LlamaEmbeddingService(logger);

        Assert.False(service.IsModelLoaded);
    }

    [Fact]
    public async Task EmbedAsync_WhenNoModel_ThrowsInvalidOperationException()
    {
        var logger = NullLogger<LlamaEmbeddingService>.Instance;
        var service = new LlamaEmbeddingService(logger);

        await Assert.ThrowsAsync<InvalidOperationException>(() =>
            service.EmbedAsync("test"));
    }

    [Fact]
    public async Task LoadModelAsync_WithInvalidModelType_ThrowsArgumentException()
    {
        var logger = NullLogger<LlamaEmbeddingService>.Instance;
        var service = new LlamaEmbeddingService(logger);

        var options = new EmbeddingModelOptions
        {
            ModelPath = "/test/model.onnx",
            ModelType = EmbeddingModelType.Onnx  // Wrong type for Llama service
        };

        await Assert.ThrowsAsync<ArgumentException>(() =>
            service.LoadModelAsync(options));
    }
}

public class EmbeddingServiceFactoryTests
{
    [Theory]
    [InlineData(".gguf", EmbeddingModelType.Gguf)]
    [InlineData(".onnx", EmbeddingModelType.Onnx)]
    [InlineData(".bin", EmbeddingModelType.SentenceTransformers)]
    public void DetectModelType_ReturnsCorrectType(
        string extension,
        EmbeddingModelType expected)
    {
        var result = EmbeddingServiceFactory.DetectModelType($"/models/test{extension}");

        Assert.Equal(expected, result);
    }

    [Theory]
    [InlineData(".txt")]
    [InlineData(".json")]
    [InlineData("")]
    public void DetectModelType_UnknownExtension_ThrowsArgumentException(string extension)
    {
        Assert.Throws<ArgumentException>(() =>
            EmbeddingServiceFactory.DetectModelType($"/models/test{extension}"));
    }
}
```

---

## Integration Notes

### Connection to v0.7.1a

This version implements the `IEmbeddingService` interface defined in v0.7.1a:
- All properties from interface are implemented (IsModelLoaded, EmbeddingDimension, MaxTokens, CurrentModelName)
- All methods from interface are implemented (LoadModelAsync, UnloadModelAsync, EmbedAsync, EmbedBatchAsync, CosineSimilarity, NormalizeEmbedding)
- ModelStateChanged event is raised appropriately

### Forward Compatibility with v0.7.1c

The factory is designed to support the `OnnxEmbeddingService` that will be implemented in v0.7.1c:
- Factory switch statement already includes the Onnx case
- `AddEmbeddingServices()` registers both service types
- Auto-detection will route .onnx files to the correct service

### Memory Management

GPU memory is explicitly released during unload:
```csharp
_embedder?.Dispose();
_model?.Dispose();
GC.Collect();
GC.WaitForPendingFinalizers();
```

This ensures Metal/CUDA memory is freed when switching models.

---

## Acceptance Criteria

- [ ] LlamaEmbeddingService loads GGUF embedding models
- [ ] Single text embedding generation works
- [ ] Batch embedding with progress reporting works
- [ ] Cosine similarity calculation uses SIMD when available
- [ ] Embedding normalization implemented
- [ ] GPU layer auto-detection for macOS Metal
- [ ] Model unloading frees resources properly
- [ ] Factory creates appropriate service based on model type
- [ ] DI extension method registers all services
- [ ] Thread-safe operation with semaphore locking
- [ ] Build succeeds with no warnings

---

## Changelog Entry

```markdown
## v0.7.1b - LLamaSharp Embedding Implementation

### Added
- `LlamaEmbeddingService` implementing `IEmbeddingService` for GGUF models
  - Model loading with progress reporting
  - Single and batch embedding generation
  - SIMD-accelerated cosine similarity calculation
  - SIMD-accelerated embedding normalization
  - Thread-safe operation with semaphore locking
  - GPU layer auto-detection (Metal on macOS, CUDA on Windows/Linux)
  - Proper resource cleanup on unload and dispose
- `EmbeddingServiceFactory` for creating service instances
  - Auto-detection of model type from file extension
  - `IEmbeddingServiceFactory` interface
- `EmbeddingServiceExtensions` for DI registration
  - `AddEmbeddingServices()` extension method
  - Registers all embedding services as singletons

### Dependencies
- Added LLamaSharp v0.18.0 for GGUF model support
- Added LLamaSharp.Backend.Cpu v0.18.0 for cross-platform compatibility

### Files Created
- `src/AIntern.Services/Embedding/LlamaEmbeddingService.cs`
- `src/AIntern.Services/Embedding/EmbeddingServiceFactory.cs`
- `src/AIntern.Services/Embedding/EmbeddingServiceExtensions.cs`

### Files Modified
- `src/AIntern.Services/AIntern.Services.csproj` - Added LLamaSharp packages
```
