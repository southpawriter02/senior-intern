# Design Specification: AIntern v0.7.3c "Indexing Service Implementation"

## Overview

**Version**: v0.7.3c
**Parent**: v0.7.3 Indexing Pipeline
**Focus**: Core implementation of the IndexingService that orchestrates the complete indexing workflow

### Purpose

This sub-version implements the main `IndexingService` class that:
1. Orchestrates the complete indexing pipeline (scan → chunk → embed → store)
2. Manages job lifecycle (queue, start, pause, resume, cancel)
3. Tracks progress and raises events for UI integration
4. Coordinates with dependent services (`IVectorStore`, `IEmbeddingService`, `IChunkingService`)
5. Supports both synchronous and queued (background) indexing operations

### Dependencies

**From v0.7.1 (Embedding Foundation)**:
- `IEmbeddingService` for generating embeddings (v0.7.1a)
- `IChunkingService` for text chunking (v0.7.1e)
- `TextChunk` model (v0.7.1d)
- `ChunkingOptions` model (v0.7.1d)

**From v0.7.2 (Vector Storage)**:
- `IVectorStore` for storing embeddings (v0.7.2a)
- `VectorIndex` model (v0.7.2b)
- `VectorIndexSettings` model (v0.7.2b)
- `IndexedFile` model (v0.7.2h)
- `ChunkWithEmbedding` model (v0.7.2c)

**From v0.7.3a (Indexing Service Interface)**:
- `IIndexingService` interface
- `IIndexingJobQueue` interface
- `IGitignoreParser` interface
- All event args classes

**From v0.7.3b (Indexing Models & Options)**:
- `IndexingOptions`, `IndexingProgress`, `IndexingResult`
- `IndexingJob`, `IndexSyncResult`, `IndexingError`
- All indexing enums

---

## Architecture

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                 v0.7.3c Indexing Service Implementation                       │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  src/SeniorIntern.Services/Indexing/                                         │
│  ┌──────────────────────────────────────────────────────────────────────────┐│
│  │                                                                          ││
│  │  IndexingService : IIndexingService, IAsyncDisposable                    ││
│  │  ┌────────────────────────────────────────────────────────────────────┐  ││
│  │  │  Dependencies (Injected)                                           │  ││
│  │  │  ├── IEmbeddingService _embeddingService                           │  ││
│  │  │  ├── IChunkingService _chunkingService                             │  ││
│  │  │  ├── IVectorStore _vectorStore                                     │  ││
│  │  │  ├── IIndexingJobQueue _jobQueue                                   │  ││
│  │  │  ├── IGitignoreParser _gitignoreParser                             │  ││
│  │  │  ├── IOptions<IndexingServiceOptions> _options                     │  ││
│  │  │  └── ILogger<IndexingService> _logger                              │  ││
│  │  └────────────────────────────────────────────────────────────────────┘  ││
│  │  ┌────────────────────────────────────────────────────────────────────┐  ││
│  │  │  State Management                                                  │  ││
│  │  │  ├── _activeJobs: ConcurrentDictionary<string, IndexingJob>       │  ││
│  │  │  ├── _jobHistory: ConcurrentDictionary<string, IndexingJob>       │  ││
│  │  │  ├── _indexingLock: SemaphoreSlim(1, 1)                           │  ││
│  │  │  ├── _currentJobCts: CancellationTokenSource?                     │  ││
│  │  │  ├── _status: IndexingStatus                                       │  ││
│  │  │  ├── _currentJob: IndexingJob?                                     │  ││
│  │  │  └── _isPaused: bool                                               │  ││
│  │  └────────────────────────────────────────────────────────────────────┘  ││
│  │  ┌────────────────────────────────────────────────────────────────────┐  ││
│  │  │  IIndexingService Implementation                                   │  ││
│  │  │  ├── IndexWorkspaceAsync()    [Synchronous full index]            │  ││
│  │  │  ├── QueueWorkspaceIndexing() [Background queue]                  │  ││
│  │  │  ├── UpdateIndexAsync()       [Incremental update]                │  ││
│  │  │  ├── IndexFilesAsync()        [Specific files]                    │  ││
│  │  │  ├── ReindexFileAsync()       [Single file refresh]               │  ││
│  │  │  ├── RemoveFilesAsync()       [Remove from index]                 │  ││
│  │  │  ├── SyncIndexAsync()         [Full filesystem sync]              │  ││
│  │  │  ├── CancelIndexing()         [Cancel current]                    │  ││
│  │  │  ├── PauseIndexing()          [Pause current]                     │  ││
│  │  │  ├── ResumeIndexing()         [Resume paused]                     │  ││
│  │  │  ├── GetJob()                 [Query job by ID]                   │  ││
│  │  │  ├── GetActiveJobs()          [List active jobs]                  │  ││
│  │  │  └── CancelJob()              [Cancel specific job]               │  ││
│  │  └────────────────────────────────────────────────────────────────────┘  ││
│  │  ┌────────────────────────────────────────────────────────────────────┐  ││
│  │  │  Events                                                            │  ││
│  │  │  ├── StateChanged             [Status transitions]                │  ││
│  │  │  ├── FileIndexed              [File completion]                   │  ││
│  │  │  ├── FileError                [File failure]                      │  ││
│  │  │  ├── JobStarted               [Job begins]                        │  ││
│  │  │  ├── JobCompleted             [Job ends]                          │  ││
│  │  │  └── ProgressUpdated          [Progress reports]                  │  ││
│  │  └────────────────────────────────────────────────────────────────────┘  ││
│  │                                                                          ││
│  └──────────────────────────────────────────────────────────────────────────┘│
│                                                                              │
│  IndexingService.Processing.cs (Partial Class)                               │
│  ┌──────────────────────────────────────────────────────────────────────────┐│
│  │  Pipeline Methods                                                        ││
│  │  ├── ScanFilesAsync()           [Phase 1: Discover files]               ││
│  │  ├── GetOrCreateIndexAsync()    [Phase 2: Index management]             ││
│  │  ├── FilterChangedFilesAsync()  [Phase 3: Change detection]             ││
│  │  ├── ProcessFilesAsync()        [Phase 4: Parallel processing]          ││
│  │  ├── ProcessSingleFileAsync()   [Individual file pipeline]              ││
│  │  ├── FinalizeIndexAsync()       [Phase 5: Cleanup & stats]              ││
│  │  └── ComputeFileHashAsync()     [Change detection helper]               ││
│  └──────────────────────────────────────────────────────────────────────────┘│
│                                                                              │
│  IndexingServiceOptions                                                      │
│  ┌──────────────────────────────────────────────────────────────────────────┐│
│  │  ├── MaxJobHistory: int (100)                                            ││
│  │  ├── ProgressReportIntervalMs: int (500)                                 ││
│  │  ├── DefaultParallelism: int (4)                                         ││
│  │  ├── EnableBackgroundProcessing: bool (true)                             ││
│  │  └── MaxConcurrentJobs: int (1)                                          ││
│  └──────────────────────────────────────────────────────────────────────────┘│
│                                                                              │
└──────────────────────────────────────────────────────────────────────────────┘
```

---

## Indexing Pipeline Flow

```
┌──────────────────────────────────────────────────────────────────────────────┐
│                        Indexing Pipeline Phases                               │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  IndexWorkspaceAsync(workspacePath, options)                                 │
│  │                                                                           │
│  ▼                                                                           │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  Phase 1: SCANNING                                                      │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │  ScanFilesAsync()                                                 │  │ │
│  │  │  ├── Enumerate files matching IncludePatterns                     │  │ │
│  │  │  ├── Apply ExcludePatterns                                        │  │ │
│  │  │  ├── Apply RespectGitignore via IGitignoreParser                  │  │ │
│  │  │  ├── Filter by size (MinFileSizeBytes, MaxFileSizeKb)            │  │ │
│  │  │  └── Return list of absolute file paths                          │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  │  Status: IndexingStatus.Scanning                                        │ │
│  │  Phase: IndexingPhase.Scanning                                          │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│  │                                                                           │
│  ▼                                                                           │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  Phase 2: INDEX MANAGEMENT                                              │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │  GetOrCreateIndexAsync()                                          │  │ │
│  │  │  ├── Check for existing index via GetIndexForWorkspaceAsync()    │  │ │
│  │  │  ├── If exists: return existing index                            │  │ │
│  │  │  ├── If not: verify embedding model loaded                       │  │ │
│  │  │  └── Create new index via CreateIndexAsync()                     │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│  │                                                                           │
│  ▼                                                                           │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  Phase 3: CHANGE DETECTION (if !ForceReindex)                          │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │  FilterChangedFilesAsync()                                        │  │ │
│  │  │  ├── For each file: get relative path                            │  │ │
│  │  │  ├── Check if file exists in index via GetIndexedFileAsync()     │  │ │
│  │  │  ├── If new file: add to changed list                            │  │ │
│  │  │  ├── If exists: compare LastWriteTimeUtc with IndexedAt          │  │ │
│  │  │  └── Return filtered list of files needing indexing              │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  │  Phase: IndexingPhase.Analyzing                                         │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│  │                                                                           │
│  ▼                                                                           │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  Phase 4: PROCESSING                                                    │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │  ProcessFilesAsync() - Parallel file processing                  │  │ │
│  │  │  ├── Create SemaphoreSlim(ParallelProcessing)                    │  │ │
│  │  │  ├── Start progress timer (ProgressReportIntervalMs)             │  │ │
│  │  │  ├── For each file (throttled by semaphore):                     │  │ │
│  │  │  │   ├── Check cancellation token                                │  │ │
│  │  │  │   ├── Wait if _isPaused                                       │  │ │
│  │  │  │   ├── ProcessSingleFileAsync()                                │  │ │
│  │  │  │   ├── Update counters (Interlocked)                           │  │ │
│  │  │  │   ├── Fire FileIndexed or FileError event                    │  │ │
│  │  │  │   └── Check MaxErrors threshold                               │  │ │
│  │  │  └── Await Task.WhenAll(tasks)                                   │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  │  Status: IndexingStatus.Indexing                                        │ │
│  │  Phase: IndexingPhase.Embedding (majority of time)                      │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│  │                                                                           │
│  │  ┌───────────────────────────────────────────────────────────────────┐   │
│  │  │  ProcessSingleFileAsync() - Per-file pipeline                    │   │
│  │  │  ├── Read file content via FileContentReader                     │   │
│  │  │  ├── Compute file hash via ComputeFileHashAsync()               │   │
│  │  │  ├── Chunk content via IChunkingService.ChunkDocument()         │   │
│  │  │  ├── Generate embeddings via IEmbeddingService.EmbedBatchAsync()│   │
│  │  │  ├── Create IndexedFile record                                   │   │
│  │  │  ├── Upsert file via IVectorStore.UpsertIndexedFileAsync()      │   │
│  │  │  ├── Create ChunkWithEmbedding list                             │   │
│  │  │  └── Store chunks via IVectorStore.AddChunksAsync()             │   │
│  │  └───────────────────────────────────────────────────────────────────┘   │
│  │                                                                           │
│  ▼                                                                           │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │  Phase 5: FINALIZATION                                                  │ │
│  │  ┌───────────────────────────────────────────────────────────────────┐  │ │
│  │  │  FinalizeIndexAsync()                                             │  │ │
│  │  │  ├── Update index statistics (ChunkCount, FileCount)            │  │ │
│  │  │  ├── Update TotalFileSizeBytes                                   │  │ │
│  │  │  ├── Set UpdatedAt and LastFullIndexAt timestamps               │  │ │
│  │  │  └── Persist via IVectorStore.UpdateIndexAsync()                │  │ │
│  │  └───────────────────────────────────────────────────────────────────┘  │ │
│  │  Phase: IndexingPhase.Finalizing → IndexingPhase.Complete               │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
│  │                                                                           │
│  ▼                                                                           │
│  Return IndexingResult                                                       │
│                                                                              │
└──────────────────────────────────────────────────────────────────────────────┘
```

---

## Files to Create

| File | Purpose |
|------|---------|
| `src/SeniorIntern.Services/Indexing/IndexingService.cs` | Main service implementation |
| `src/SeniorIntern.Services/Indexing/IndexingService.Processing.cs` | Pipeline processing methods (partial class) |
| `src/SeniorIntern.Services/Indexing/IndexingServiceOptions.cs` | Application configuration options |
| `src/SeniorIntern.Services/Indexing/IndexingExtensions.cs` | Extension methods for type conversion |

---

## Detailed Specifications

### 1. IndexingService.cs

**Location**: `src/SeniorIntern.Services/Indexing/IndexingService.cs`

```csharp
using System;
using System.Collections.Concurrent;
using System.Collections.Generic;
using System.Diagnostics;
using System.IO;
using System.Linq;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.Extensions.Logging;
using Microsoft.Extensions.Options;
using SeniorIntern.Core.Interfaces;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Services.Indexing;

/// <summary>
/// Service for indexing workspace files into the vector store.
/// </summary>
/// <remarks>
/// <para>
/// IndexingService is the main orchestrator for the indexing pipeline. It coordinates:
/// <list type="bullet">
///   <item>File scanning with pattern matching and gitignore support</item>
///   <item>Change detection for incremental updates</item>
///   <item>Parallel file processing (chunking, embedding, storage)</item>
///   <item>Job lifecycle management (queue, pause, cancel)</item>
///   <item>Progress reporting and event notifications</item>
/// </list>
/// </para>
/// <para>
/// Thread safety: The service serializes indexing operations using an internal
/// semaphore. Only one indexing operation runs at a time to avoid resource
/// contention on GPU and database.
/// </para>
/// </remarks>
public sealed partial class IndexingService : IIndexingService, IAsyncDisposable
{
    #region Dependencies

    private readonly IEmbeddingService _embeddingService;
    private readonly IChunkingService _chunkingService;
    private readonly IVectorStore _vectorStore;
    private readonly IIndexingJobQueue _jobQueue;
    private readonly IGitignoreParser _gitignoreParser;
    private readonly IndexingServiceOptions _options;
    private readonly ILogger<IndexingService> _logger;

    #endregion

    #region State

    private readonly ConcurrentDictionary<string, IndexingJob> _activeJobs = new();
    private readonly ConcurrentDictionary<string, IndexingJob> _jobHistory = new();
    private readonly SemaphoreSlim _indexingLock = new(1, 1);
    private CancellationTokenSource? _currentJobCts;
    private IndexingStatus _status = IndexingStatus.Idle;
    private IndexingJob? _currentJob;
    private bool _isPaused;
    private bool _isDisposed;

    #endregion

    #region Properties

    /// <inheritdoc />
    public IndexingStatus Status => _status;

    /// <inheritdoc />
    public bool IsIndexing => _status is IndexingStatus.Indexing
        or IndexingStatus.Scanning
        or IndexingStatus.Cancelling;

    /// <inheritdoc />
    public IndexingJob? CurrentJob => _currentJob;

    #endregion

    #region Events

    /// <inheritdoc />
    public event EventHandler<IndexingStateChangedEventArgs>? StateChanged;

    /// <inheritdoc />
    public event EventHandler<FileIndexedEventArgs>? FileIndexed;

    /// <inheritdoc />
    public event EventHandler<FileIndexingErrorEventArgs>? FileError;

    /// <inheritdoc />
    public event EventHandler<IndexingJobEventArgs>? JobStarted;

    /// <inheritdoc />
    public event EventHandler<IndexingJobCompletedEventArgs>? JobCompleted;

    /// <inheritdoc />
    public event EventHandler<IndexingProgressEventArgs>? ProgressUpdated;

    #endregion

    #region Constructor

    /// <summary>
    /// Initializes a new instance of the <see cref="IndexingService"/> class.
    /// </summary>
    public IndexingService(
        IEmbeddingService embeddingService,
        IChunkingService chunkingService,
        IVectorStore vectorStore,
        IIndexingJobQueue jobQueue,
        IGitignoreParser gitignoreParser,
        IOptions<IndexingServiceOptions> options,
        ILogger<IndexingService> logger)
    {
        _embeddingService = embeddingService ?? throw new ArgumentNullException(nameof(embeddingService));
        _chunkingService = chunkingService ?? throw new ArgumentNullException(nameof(chunkingService));
        _vectorStore = vectorStore ?? throw new ArgumentNullException(nameof(vectorStore));
        _jobQueue = jobQueue ?? throw new ArgumentNullException(nameof(jobQueue));
        _gitignoreParser = gitignoreParser ?? throw new ArgumentNullException(nameof(gitignoreParser));
        _options = options?.Value ?? throw new ArgumentNullException(nameof(options));
        _logger = logger ?? throw new ArgumentNullException(nameof(logger));

        _logger.LogDebug("IndexingService initialized with parallelism {Parallelism}",
            _options.DefaultParallelism);
    }

    #endregion

    #region Workspace Indexing

    /// <inheritdoc />
    public async Task<IndexingResult> IndexWorkspaceAsync(
        string workspacePath,
        IndexingOptions options,
        IProgress<IndexingProgress>? progress = null,
        CancellationToken ct = default)
    {
        ArgumentException.ThrowIfNullOrWhiteSpace(workspacePath);
        ArgumentNullException.ThrowIfNull(options);

        if (!Directory.Exists(workspacePath))
        {
            throw new ArgumentException($"Workspace does not exist: {workspacePath}",
                nameof(workspacePath));
        }

        // Try to acquire the indexing lock
        if (!await _indexingLock.WaitAsync(0, ct))
        {
            throw new InvalidOperationException("Another indexing operation is in progress");
        }

        // Create job for tracking
        var job = new IndexingJob
        {
            Type = IndexingJobType.FullIndex,
            WorkspacePath = workspacePath,
            Options = options,
            Status = IndexingJobStatus.Running,
            StartedAt = DateTime.UtcNow
        };

        _currentJob = job;
        _currentJobCts = CancellationTokenSource.CreateLinkedTokenSource(ct);
        var stopwatch = Stopwatch.StartNew();
        var errors = new ConcurrentBag<IndexingError>();

        try
        {
            _logger.LogInformation("Starting workspace indexing: {Path}", workspacePath);
            SetStatus(IndexingStatus.Scanning);
            JobStarted?.Invoke(this, new IndexingJobEventArgs { Job = job });

            // Phase 1: Scan files
            var scanProgress = CreateProgressReporter(progress, job);
            progress?.Report(IndexingProgress.Scanning());

            var files = await ScanFilesAsync(
                workspacePath, options, scanProgress, _currentJobCts.Token);

            _logger.LogInformation("Found {Count} files to index", files.Count);

            // Phase 2: Create or get index
            var index = await GetOrCreateIndexAsync(
                workspacePath, options, _currentJobCts.Token);
            job = job with { IndexId = index.Id };

            // Phase 3: Filter to changed files if not forcing
            var filesToIndex = files;
            if (!options.ForceReindex)
            {
                SetStatus(IndexingStatus.Scanning);
                progress?.Report(new IndexingProgress
                {
                    Phase = IndexingPhase.Analyzing,
                    TotalFiles = files.Count,
                    Message = "Detecting changed files..."
                });

                filesToIndex = await FilterChangedFilesAsync(
                    index.Id, files, _currentJobCts.Token);

                _logger.LogInformation(
                    "After filtering: {Changed} files to index, {Skipped} unchanged",
                    filesToIndex.Count, files.Count - filesToIndex.Count);
            }

            // Phase 4: Process files
            SetStatus(IndexingStatus.Indexing);
            var result = await ProcessFilesAsync(
                index.Id,
                workspacePath,
                filesToIndex,
                options,
                errors,
                progress,
                _currentJobCts.Token);

            // Phase 5: Finalize
            progress?.Report(new IndexingProgress
            {
                Phase = IndexingPhase.Finalizing,
                Message = "Finalizing index..."
            });

            await FinalizeIndexAsync(index.Id, result, _currentJobCts.Token);

            stopwatch.Stop();

            var finalResult = new IndexingResult
            {
                Success = true,
                IndexId = index.Id,
                FilesIndexed = result.FilesIndexed,
                FilesSkipped = files.Count - filesToIndex.Count,
                FilesErrored = errors.Count,
                ChunksCreated = result.ChunksCreated,
                BytesProcessed = result.BytesProcessed,
                Duration = stopwatch.Elapsed,
                StartedAt = job.StartedAt ?? DateTime.UtcNow,
                CompletedAt = DateTime.UtcNow,
                Errors = errors.ToList()
            };

            job.Result = finalResult;
            job.Status = IndexingJobStatus.Completed;
            job.CompletedAt = DateTime.UtcNow;

            progress?.Report(IndexingProgress.Complete(
                finalResult.FilesIndexed,
                finalResult.ChunksCreated,
                stopwatch.Elapsed));

            JobCompleted?.Invoke(this, new IndexingJobCompletedEventArgs
            {
                Job = job,
                Result = finalResult
            });

            _logger.LogInformation(
                "Indexing complete: {Files} files, {Chunks} chunks in {Duration}",
                finalResult.FilesIndexed, finalResult.ChunksCreated, stopwatch.Elapsed);

            return finalResult;
        }
        catch (OperationCanceledException)
        {
            stopwatch.Stop();
            job.Status = IndexingJobStatus.Cancelled;
            job.CompletedAt = DateTime.UtcNow;

            var result = IndexingResult.Cancelled(
                job.Progress?.ProcessedFiles ?? 0,
                job.Progress?.ProcessedChunks ?? 0,
                stopwatch.Elapsed);

            job.Result = result;
            JobCompleted?.Invoke(this, new IndexingJobCompletedEventArgs
            {
                Job = job,
                Result = result
            });

            _logger.LogInformation("Indexing cancelled after {Duration}", stopwatch.Elapsed);
            return result;
        }
        catch (Exception ex)
        {
            stopwatch.Stop();
            job.Status = IndexingJobStatus.Failed;
            job.CompletedAt = DateTime.UtcNow;
            job.ErrorMessage = ex.Message;

            var result = IndexingResult.Failed(ex.Message, stopwatch.Elapsed, errors.ToList());
            job.Result = result;

            JobCompleted?.Invoke(this, new IndexingJobCompletedEventArgs
            {
                Job = job,
                Result = result
            });

            _logger.LogError(ex, "Indexing failed after {Duration}", stopwatch.Elapsed);
            return result;
        }
        finally
        {
            _currentJob = null;
            _currentJobCts?.Dispose();
            _currentJobCts = null;
            SetStatus(IndexingStatus.Idle);
            _indexingLock.Release();

            // Add to history
            _jobHistory.TryAdd(job.Id, job);
            TrimJobHistory();
        }
    }

    /// <inheritdoc />
    public IndexingJob QueueWorkspaceIndexing(string workspacePath, IndexingOptions options)
    {
        ArgumentException.ThrowIfNullOrWhiteSpace(workspacePath);
        ArgumentNullException.ThrowIfNull(options);

        var job = new IndexingJob
        {
            Type = IndexingJobType.FullIndex,
            WorkspacePath = workspacePath,
            Options = options,
            Priority = options.Priority
        };

        _jobQueue.Enqueue(job);
        _activeJobs.TryAdd(job.Id, job);

        _logger.LogInformation("Queued indexing job {JobId} for {Path}", job.Id, workspacePath);
        return job;
    }

    #endregion

    #region Incremental Updates

    /// <inheritdoc />
    public async Task<IndexingResult> UpdateIndexAsync(
        string indexId,
        IProgress<IndexingProgress>? progress = null,
        CancellationToken ct = default)
    {
        ArgumentException.ThrowIfNullOrWhiteSpace(indexId);

        var index = await _vectorStore.GetIndexAsync(indexId, ct)
            ?? throw new ArgumentException($"Index not found: {indexId}", nameof(indexId));

        var options = index.Settings.ToIndexingOptions();
        options = options with { ForceReindex = false };

        return await IndexWorkspaceAsync(index.WorkspacePath, options, progress, ct);
    }

    /// <inheritdoc />
    public async Task<IndexingResult> IndexFilesAsync(
        string indexId,
        IEnumerable<string> filePaths,
        IProgress<IndexingProgress>? progress = null,
        CancellationToken ct = default)
    {
        ArgumentException.ThrowIfNullOrWhiteSpace(indexId);
        ArgumentNullException.ThrowIfNull(filePaths);

        var index = await _vectorStore.GetIndexAsync(indexId, ct)
            ?? throw new ArgumentException($"Index not found: {indexId}", nameof(indexId));

        var options = index.Settings.ToIndexingOptions();
        var files = filePaths.ToList();
        var errors = new ConcurrentBag<IndexingError>();
        var stopwatch = Stopwatch.StartNew();

        SetStatus(IndexingStatus.Indexing);

        try
        {
            var result = await ProcessFilesAsync(
                indexId,
                index.WorkspacePath,
                files,
                options,
                errors,
                progress,
                ct);

            stopwatch.Stop();

            return new IndexingResult
            {
                Success = true,
                IndexId = indexId,
                FilesIndexed = result.FilesIndexed,
                ChunksCreated = result.ChunksCreated,
                Duration = stopwatch.Elapsed,
                Errors = errors.ToList()
            };
        }
        finally
        {
            SetStatus(IndexingStatus.Idle);
        }
    }

    /// <inheritdoc />
    public async Task<int> ReindexFileAsync(
        string indexId,
        string filePath,
        CancellationToken ct = default)
    {
        ArgumentException.ThrowIfNullOrWhiteSpace(indexId);
        ArgumentException.ThrowIfNullOrWhiteSpace(filePath);

        // Remove old chunks
        await _vectorStore.RemoveChunksForFileAsync(indexId, filePath, ct);

        // Re-index
        var index = await _vectorStore.GetIndexAsync(indexId, ct)
            ?? throw new ArgumentException($"Index not found: {indexId}", nameof(indexId));

        var options = index.Settings.ToIndexingOptions();
        var fullPath = Path.Combine(index.WorkspacePath, filePath);

        return await ProcessSingleFileAsync(indexId, index.WorkspacePath, fullPath, options, ct);
    }

    #endregion

    #region File Management

    /// <inheritdoc />
    public async Task<int> RemoveFilesAsync(
        string indexId,
        IEnumerable<string> filePaths,
        CancellationToken ct = default)
    {
        ArgumentException.ThrowIfNullOrWhiteSpace(indexId);
        ArgumentNullException.ThrowIfNull(filePaths);

        return await _vectorStore.RemoveChunksForFilesAsync(indexId, filePaths, ct);
    }

    /// <inheritdoc />
    public async Task<IndexSyncResult> SyncIndexAsync(
        string indexId,
        IProgress<IndexingProgress>? progress = null,
        CancellationToken ct = default)
    {
        ArgumentException.ThrowIfNullOrWhiteSpace(indexId);

        var index = await _vectorStore.GetIndexAsync(indexId, ct)
            ?? throw new ArgumentException($"Index not found: {indexId}", nameof(indexId));

        var options = index.Settings.ToIndexingOptions();
        var stopwatch = Stopwatch.StartNew();
        var errors = new ConcurrentBag<IndexingError>();

        // Scan current files
        var currentFiles = await ScanFilesAsync(
            index.WorkspacePath, options, null, ct);

        // Compute hashes
        var fileHashes = new Dictionary<string, string>();
        foreach (var file in currentFiles)
        {
            var relativePath = Path.GetRelativePath(index.WorkspacePath, file);
            var hash = await ComputeFileHashAsync(file, ct);
            fileHashes[relativePath] = hash;
        }

        // Detect changes
        var changes = await _vectorStore.DetectFileChangesAsync(indexId, fileHashes, ct);

        var chunksAdded = 0;
        var chunksRemoved = 0;

        // Remove deleted files
        if (changes.DeletedFiles.Count > 0)
        {
            chunksRemoved = await _vectorStore.RemoveChunksForFilesAsync(
                indexId, changes.DeletedFiles, ct);
        }

        // Process added and modified files
        var filesToProcess = changes.AddedFiles.Concat(changes.ModifiedFiles).ToList();
        if (filesToProcess.Count > 0)
        {
            // Remove chunks for modified files first
            foreach (var file in changes.ModifiedFiles)
            {
                chunksRemoved += await _vectorStore.RemoveChunksForFileAsync(indexId, file, ct);
            }

            var result = await ProcessFilesAsync(
                indexId,
                index.WorkspacePath,
                filesToProcess.Select(f => Path.Combine(index.WorkspacePath, f)).ToList(),
                options,
                errors,
                progress,
                ct);

            chunksAdded = result.ChunksCreated;
        }

        stopwatch.Stop();

        return new IndexSyncResult
        {
            Success = true,
            FilesAdded = changes.AddedFiles.Count,
            FilesUpdated = changes.ModifiedFiles.Count,
            FilesRemoved = changes.DeletedFiles.Count,
            FilesUnchanged = changes.UnchangedFiles.Count,
            ChunksDelta = chunksAdded - chunksRemoved,
            Duration = stopwatch.Elapsed,
            Errors = errors.ToList()
        };
    }

    #endregion

    #region Job Control

    /// <inheritdoc />
    public void CancelIndexing()
    {
        if (_currentJobCts != null && !_currentJobCts.IsCancellationRequested)
        {
            _logger.LogInformation("Cancelling current indexing operation");
            SetStatus(IndexingStatus.Cancelling);
            _currentJobCts.Cancel();
        }
    }

    /// <inheritdoc />
    public void PauseIndexing()
    {
        if (IsIndexing && !_isPaused)
        {
            _isPaused = true;
            SetStatus(IndexingStatus.Paused);
            _logger.LogInformation("Indexing paused");
        }
    }

    /// <inheritdoc />
    public void ResumeIndexing()
    {
        if (_isPaused)
        {
            _isPaused = false;
            SetStatus(IndexingStatus.Indexing);
            _logger.LogInformation("Indexing resumed");
        }
    }

    /// <inheritdoc />
    public IndexingJob? GetJob(string jobId)
    {
        if (_activeJobs.TryGetValue(jobId, out var job))
            return job;
        if (_jobHistory.TryGetValue(jobId, out job))
            return job;
        return null;
    }

    /// <inheritdoc />
    public IReadOnlyList<IndexingJob> GetActiveJobs()
    {
        return _activeJobs.Values.ToList();
    }

    /// <inheritdoc />
    public bool CancelJob(string jobId)
    {
        if (_currentJob?.Id == jobId)
        {
            CancelIndexing();
            return true;
        }

        return _jobQueue.Remove(jobId);
    }

    #endregion

    #region Private Helpers

    private void SetStatus(IndexingStatus status)
    {
        if (_status != status)
        {
            var oldStatus = _status;
            _status = status;
            StateChanged?.Invoke(this, new IndexingStateChangedEventArgs
            {
                OldStatus = oldStatus,
                NewStatus = status
            });
        }
    }

    private void TrimJobHistory()
    {
        while (_jobHistory.Count > _options.MaxJobHistory)
        {
            var oldest = _jobHistory.Values
                .OrderBy(j => j.CompletedAt ?? j.CreatedAt)
                .FirstOrDefault();

            if (oldest != null)
            {
                _jobHistory.TryRemove(oldest.Id, out _);
            }
        }
    }

    private IProgress<IndexingProgress> CreateProgressReporter(
        IProgress<IndexingProgress>? userProgress,
        IndexingJob job)
    {
        return new Progress<IndexingProgress>(p =>
        {
            job.Progress = p;
            userProgress?.Report(p);
            ProgressUpdated?.Invoke(this, new IndexingProgressEventArgs { Progress = p });
        });
    }

    #endregion

    #region IAsyncDisposable

    /// <inheritdoc />
    public async ValueTask DisposeAsync()
    {
        if (_isDisposed) return;
        _isDisposed = true;

        CancelIndexing();
        _currentJobCts?.Dispose();
        _indexingLock.Dispose();

        await Task.CompletedTask;
    }

    #endregion
}
```

### 2. IndexingService.Processing.cs

**Location**: `src/SeniorIntern.Services/Indexing/IndexingService.Processing.cs`

```csharp
using System;
using System.Collections.Concurrent;
using System.Collections.Generic;
using System.Diagnostics;
using System.IO;
using System.Linq;
using System.Security.Cryptography;
using System.Threading;
using System.Threading.Tasks;
using SeniorIntern.Core.Models;
using SeniorIntern.Services.VectorStore;

namespace SeniorIntern.Services.Indexing;

/// <summary>
/// Processing methods for IndexingService (partial class).
/// </summary>
public sealed partial class IndexingService
{
    /// <summary>
    /// Scan workspace for files matching the configured patterns.
    /// </summary>
    private async Task<List<string>> ScanFilesAsync(
        string workspacePath,
        IndexingOptions options,
        IProgress<IndexingProgress>? progress,
        CancellationToken ct)
    {
        var files = new List<string>();

        // Load gitignore patterns if enabled
        IGitignoreMatcher? gitignoreMatcher = null;
        if (options.RespectGitignore)
        {
            gitignoreMatcher = _gitignoreParser.LoadFromDirectory(workspacePath);
            _logger.LogDebug("Loaded {Count} gitignore patterns", gitignoreMatcher.PatternCount);
        }

        // Create file enumeration options
        var enumOptions = new EnumerationOptions
        {
            RecurseSubdirectories = options.MaxDepth != 0,
            MaxRecursionDepth = options.MaxDepth > 0 ? options.MaxDepth : int.MaxValue,
            IgnoreInaccessible = true,
            AttributesToSkip = options.IndexHiddenFiles
                ? FileAttributes.System
                : FileAttributes.Hidden | FileAttributes.System
        };

        var scannedCount = 0;
        await Task.Run(() =>
        {
            foreach (var file in Directory.EnumerateFiles(workspacePath, "*", enumOptions))
            {
                ct.ThrowIfCancellationRequested();

                var relativePath = Path.GetRelativePath(workspacePath, file);

                // Check gitignore
                if (gitignoreMatcher?.IsIgnored(relativePath) == true)
                    continue;

                // Check include patterns
                if (!MatchesAnyPattern(relativePath, options.IncludePatterns))
                    continue;

                // Check exclude patterns
                if (MatchesAnyPattern(relativePath, options.ExcludePatterns))
                    continue;

                // Check file size
                var fileInfo = new FileInfo(file);
                if (fileInfo.Length < options.MinFileSizeBytes)
                    continue;
                if (fileInfo.Length > options.MaxFileSizeKb * 1024)
                    continue;

                files.Add(file);
                scannedCount++;

                if (scannedCount % 100 == 0)
                {
                    progress?.Report(new IndexingProgress
                    {
                        Phase = IndexingPhase.Scanning,
                        ProcessedFiles = scannedCount,
                        Message = $"Found {scannedCount} files..."
                    });
                }
            }
        }, ct);

        return files;
    }

    /// <summary>
    /// Get an existing index or create a new one.
    /// </summary>
    private async Task<VectorIndex> GetOrCreateIndexAsync(
        string workspacePath,
        IndexingOptions options,
        CancellationToken ct)
    {
        var existing = await _vectorStore.GetIndexForWorkspaceAsync(workspacePath, ct);
        if (existing != null)
        {
            return existing;
        }

        if (!_embeddingService.IsModelLoaded)
        {
            throw new InvalidOperationException("No embedding model loaded");
        }

        return await _vectorStore.CreateIndexAsync(
            workspacePath,
            options.IndexName,
            _embeddingService.EmbeddingDimension,
            options.ToVectorIndexSettings(),
            ct);
    }

    /// <summary>
    /// Filter files to only those that have changed since last indexing.
    /// </summary>
    private async Task<List<string>> FilterChangedFilesAsync(
        string indexId,
        List<string> files,
        CancellationToken ct)
    {
        var changedFiles = new List<string>();
        var index = await _vectorStore.GetIndexAsync(indexId, ct);
        if (index == null) return files;

        foreach (var file in files)
        {
            ct.ThrowIfCancellationRequested();

            var relativePath = Path.GetRelativePath(index.WorkspacePath, file);
            var indexed = await _vectorStore.GetIndexedFileAsync(indexId, relativePath, ct);

            if (indexed == null)
            {
                // New file
                changedFiles.Add(file);
            }
            else
            {
                // Check if modified
                var fileInfo = new FileInfo(file);
                if (fileInfo.LastWriteTimeUtc > indexed.IndexedAt)
                {
                    changedFiles.Add(file);
                }
            }
        }

        return changedFiles;
    }

    /// <summary>
    /// Process files in parallel through the indexing pipeline.
    /// </summary>
    private async Task<ProcessingResult> ProcessFilesAsync(
        string indexId,
        string workspacePath,
        List<string> files,
        IndexingOptions options,
        ConcurrentBag<IndexingError> errors,
        IProgress<IndexingProgress>? progress,
        CancellationToken ct)
    {
        var processedFiles = 0;
        var totalChunks = 0;
        long bytesProcessed = 0;
        var totalFiles = files.Count;

        using var semaphore = new SemaphoreSlim(options.ParallelProcessing);
        var tasks = new List<Task<int>>();

        // Progress reporting timer
        using var progressTimer = new System.Timers.Timer(_options.ProgressReportIntervalMs);
        progressTimer.Elapsed += (s, e) =>
        {
            if (_isPaused) return;

            progress?.Report(new IndexingProgress
            {
                Phase = IndexingPhase.Embedding,
                TotalFiles = totalFiles,
                ProcessedFiles = processedFiles,
                FailedFiles = errors.Count,
                TotalChunks = totalChunks,
                ProcessedChunks = totalChunks,
                PercentComplete = totalFiles > 0
                    ? (double)processedFiles / totalFiles * 100
                    : 0,
                BytesProcessed = bytesProcessed
            });
        };
        progressTimer.Start();

        try
        {
            foreach (var file in files)
            {
                ct.ThrowIfCancellationRequested();

                // Wait if paused
                while (_isPaused && !ct.IsCancellationRequested)
                {
                    await Task.Delay(100, ct);
                }

                await semaphore.WaitAsync(ct);

                var task = Task.Run(async () =>
                {
                    try
                    {
                        var chunks = await ProcessSingleFileAsync(
                            indexId, workspacePath, file, options, ct);

                        var fileInfo = new FileInfo(file);
                        Interlocked.Add(ref bytesProcessed, fileInfo.Length);
                        Interlocked.Increment(ref processedFiles);
                        Interlocked.Add(ref totalChunks, chunks);

                        var relativePath = Path.GetRelativePath(workspacePath, file);
                        FileIndexed?.Invoke(this, new FileIndexedEventArgs
                        {
                            FilePath = relativePath,
                            ChunkCount = chunks,
                            Success = true
                        });

                        return chunks;
                    }
                    catch (Exception ex)
                    {
                        var relativePath = Path.GetRelativePath(workspacePath, file);
                        var error = IndexingError.FromException(relativePath, ex);
                        errors.Add(error);

                        FileError?.Invoke(this, new FileIndexingErrorEventArgs
                        {
                            FilePath = relativePath,
                            Error = error
                        });

                        _logger.LogWarning(ex, "Error processing file: {File}", relativePath);

                        if (!options.ContinueOnError)
                            throw;

                        if (options.MaxErrors > 0 && errors.Count >= options.MaxErrors)
                            throw new InvalidOperationException(
                                $"Maximum error count ({options.MaxErrors}) exceeded");

                        return 0;
                    }
                    finally
                    {
                        semaphore.Release();
                    }
                }, ct);

                tasks.Add(task);
            }

            await Task.WhenAll(tasks);
        }
        finally
        {
            progressTimer.Stop();
        }

        return new ProcessingResult
        {
            FilesIndexed = processedFiles,
            ChunksCreated = totalChunks,
            BytesProcessed = bytesProcessed
        };
    }

    /// <summary>
    /// Process a single file through the complete pipeline.
    /// </summary>
    private async Task<int> ProcessSingleFileAsync(
        string indexId,
        string workspacePath,
        string filePath,
        IndexingOptions options,
        CancellationToken ct)
    {
        var relativePath = Path.GetRelativePath(workspacePath, filePath);

        // Read file with encoding detection
        var fileContent = await FileContentReader.ReadAsync(filePath, ct);
        var fileInfo = new FileInfo(filePath);
        var fileHash = FileHasher.ComputeHash(fileContent.Content);

        // Chunk the content
        var chunks = _chunkingService.ChunkDocument(
            fileContent.Content,
            relativePath,
            options.ChunkingOptions);

        if (chunks.Count == 0)
        {
            // Still track the file even if no chunks
            await _vectorStore.UpsertIndexedFileAsync(indexId, new IndexedFile
            {
                IndexId = indexId,
                FilePath = relativePath,
                FileHash = fileHash,
                FileSize = fileInfo.Length,
                LastModified = fileInfo.LastWriteTimeUtc,
                ChunkCount = 0,
                Language = _chunkingService.DetectLanguage(relativePath),
                Encoding = fileContent.Encoding,
                LineCount = fileContent.LineCount,
                Status = FileIndexStatus.Indexed
            }, ct);

            return 0;
        }

        // Generate embeddings in batches
        var chunkContents = chunks.Select(c => c.Content).ToList();
        var embeddings = await _embeddingService.EmbedBatchAsync(chunkContents, null, ct);

        // Create indexed file record
        var indexedFile = new IndexedFile
        {
            IndexId = indexId,
            FilePath = relativePath,
            FileHash = fileHash,
            FileSize = fileInfo.Length,
            LastModified = fileInfo.LastWriteTimeUtc,
            ChunkCount = chunks.Count,
            Language = chunks.FirstOrDefault()?.Language
                ?? _chunkingService.DetectLanguage(relativePath),
            Encoding = fileContent.Encoding,
            LineCount = fileContent.LineCount,
            Status = FileIndexStatus.Indexed
        };

        await _vectorStore.UpsertIndexedFileAsync(indexId, indexedFile, ct);

        // Prepare chunks with embeddings
        var chunksWithEmbeddings = chunks.Zip(embeddings, (chunk, embedding) =>
            new ChunkWithEmbedding
            {
                Chunk = chunk,
                Embedding = embedding,
                FileId = indexedFile.Id
            }).ToList();

        // Store in vector database
        await _vectorStore.AddChunksAsync(indexId, chunksWithEmbeddings, ct);

        return chunks.Count;
    }

    /// <summary>
    /// Update index metadata after processing.
    /// </summary>
    private async Task FinalizeIndexAsync(
        string indexId,
        ProcessingResult result,
        CancellationToken ct)
    {
        var index = await _vectorStore.GetIndexAsync(indexId, ct);
        if (index != null)
        {
            index.ChunkCount = result.ChunksCreated;
            index.FileCount = result.FilesIndexed;
            index.TotalFileSizeBytes = result.BytesProcessed;
            index.UpdatedAt = DateTime.UtcNow;
            index.LastFullIndexAt = DateTime.UtcNow;

            await _vectorStore.UpdateIndexAsync(index, ct);
        }
    }

    /// <summary>
    /// Compute SHA256 hash of a file.
    /// </summary>
    private static async Task<string> ComputeFileHashAsync(string filePath, CancellationToken ct)
    {
        await using var stream = File.OpenRead(filePath);
        var hashBytes = await SHA256.HashDataAsync(stream, ct);
        return Convert.ToHexString(hashBytes).ToLowerInvariant();
    }

    /// <summary>
    /// Check if a path matches any of the glob patterns.
    /// </summary>
    private static bool MatchesAnyPattern(string path, IReadOnlyList<string> patterns)
    {
        foreach (var pattern in patterns)
        {
            if (MatchGlobPattern(path, pattern))
                return true;
        }
        return false;
    }

    /// <summary>
    /// Simple glob pattern matching supporting * and **.
    /// </summary>
    private static bool MatchGlobPattern(string path, string pattern)
    {
        // Normalize separators
        path = path.Replace('\\', '/');
        pattern = pattern.Replace('\\', '/');

        // Handle ** recursive matching
        if (pattern.StartsWith("**/"))
        {
            var rest = pattern[3..];
            // Match at any directory level
            var segments = path.Split('/');
            for (int i = 0; i < segments.Length; i++)
            {
                var subPath = string.Join('/', segments.Skip(i));
                if (MatchGlobPattern(subPath, rest))
                    return true;
            }
            return false;
        }

        // Handle trailing /**
        if (pattern.EndsWith("/**"))
        {
            var prefix = pattern[..^3];
            return path.StartsWith(prefix + "/") || path == prefix;
        }

        // Convert glob to regex for simple patterns
        var regexPattern = "^" + System.Text.RegularExpressions.Regex.Escape(pattern)
            .Replace("\\*", "[^/]*")
            .Replace("\\?", "[^/]") + "$";

        return System.Text.RegularExpressions.Regex.IsMatch(path, regexPattern);
    }

    /// <summary>
    /// Internal result class for processing.
    /// </summary>
    private sealed class ProcessingResult
    {
        public int FilesIndexed { get; init; }
        public int ChunksCreated { get; init; }
        public long BytesProcessed { get; init; }
    }
}
```

### 3. IndexingServiceOptions.cs

**Location**: `src/SeniorIntern.Services/Indexing/IndexingServiceOptions.cs`

```csharp
namespace SeniorIntern.Services.Indexing;

/// <summary>
/// Configuration options for the IndexingService.
/// </summary>
/// <remarks>
/// Configure via appsettings.json under "Indexing" section or via code:
/// <code>
/// services.Configure&lt;IndexingServiceOptions&gt;(options =>
/// {
///     options.MaxJobHistory = 50;
///     options.DefaultParallelism = 8;
/// });
/// </code>
/// </remarks>
public sealed class IndexingServiceOptions
{
    /// <summary>
    /// Configuration section name.
    /// </summary>
    public const string SectionName = "Indexing";

    /// <summary>
    /// Maximum number of completed jobs to retain in history.
    /// </summary>
    /// <remarks>
    /// Older jobs are removed when this limit is exceeded.
    /// </remarks>
    public int MaxJobHistory { get; set; } = 100;

    /// <summary>
    /// Interval in milliseconds for progress reporting.
    /// </summary>
    /// <remarks>
    /// Lower values give more responsive UI but slightly more overhead.
    /// </remarks>
    public int ProgressReportIntervalMs { get; set; } = 500;

    /// <summary>
    /// Default parallelism for file processing.
    /// </summary>
    /// <remarks>
    /// Used when IndexingOptions.ParallelProcessing is not specified.
    /// </remarks>
    public int DefaultParallelism { get; set; } = 4;

    /// <summary>
    /// Whether to enable background job processing.
    /// </summary>
    /// <remarks>
    /// When false, jobs must be processed synchronously.
    /// </remarks>
    public bool EnableBackgroundProcessing { get; set; } = true;

    /// <summary>
    /// Maximum number of concurrent indexing jobs.
    /// </summary>
    /// <remarks>
    /// Currently only 1 is supported to avoid GPU contention.
    /// Reserved for future multi-GPU support.
    /// </remarks>
    public int MaxConcurrentJobs { get; set; } = 1;
}
```

### 4. IndexingExtensions.cs

**Location**: `src/SeniorIntern.Services/Indexing/IndexingExtensions.cs`

```csharp
using SeniorIntern.Core.Models;

namespace SeniorIntern.Services.Indexing;

/// <summary>
/// Extension methods for indexing type conversions.
/// </summary>
public static class IndexingExtensions
{
    /// <summary>
    /// Convert IndexingOptions to VectorIndexSettings for storage.
    /// </summary>
    public static VectorIndexSettings ToVectorIndexSettings(this IndexingOptions options)
    {
        return new VectorIndexSettings
        {
            ChunkingOptions = options.ChunkingOptions,
            IncludePatterns = options.IncludePatterns,
            ExcludePatterns = options.ExcludePatterns,
            MaxFileSizeKb = options.MaxFileSizeKb,
            MinFileSizeBytes = options.MinFileSizeBytes,
            IndexHiddenFiles = options.IndexHiddenFiles,
            RespectGitignore = options.RespectGitignore,
            FollowSymlinks = options.FollowSymlinks,
            MaxDepth = options.MaxDepth,
            AutoReindex = options.EnableFileWatching,
            AutoReindexDelayMs = options.FileWatchDebounceMs,
            ParallelProcessing = options.ParallelProcessing
        };
    }

    /// <summary>
    /// Convert VectorIndexSettings to IndexingOptions for re-indexing.
    /// </summary>
    public static IndexingOptions ToIndexingOptions(this VectorIndexSettings settings)
    {
        return new IndexingOptions
        {
            ChunkingOptions = settings.ChunkingOptions,
            IncludePatterns = settings.IncludePatterns,
            ExcludePatterns = settings.ExcludePatterns,
            MaxFileSizeKb = settings.MaxFileSizeKb,
            MinFileSizeBytes = settings.MinFileSizeBytes,
            IndexHiddenFiles = settings.IndexHiddenFiles,
            RespectGitignore = settings.RespectGitignore,
            FollowSymlinks = settings.FollowSymlinks,
            MaxDepth = settings.MaxDepth,
            EnableFileWatching = settings.AutoReindex,
            FileWatchDebounceMs = settings.AutoReindexDelayMs,
            ParallelProcessing = settings.ParallelProcessing
        };
    }
}
```

---

## Unit Testing Requirements

| Class | Test Count | Focus Areas |
|-------|------------|-------------|
| `IndexingService` | 35-45 | Workspace indexing, incremental updates, job control, events, error handling |
| `IndexingService.Processing` | 20-25 | Scanning, filtering, parallel processing, single file pipeline |
| `IndexingServiceOptions` | 5-8 | Default values, configuration binding |
| `IndexingExtensions` | 8-10 | Bidirectional conversions, property mapping |
| Glob Pattern Matching | 15-20 | Pattern variations, edge cases |

**Total: ~83-108 tests**

---

## Acceptance Criteria

### Functional Requirements
- [ ] `IndexWorkspaceAsync` processes files through complete pipeline
- [ ] `QueueWorkspaceIndexing` adds jobs to background queue
- [ ] `UpdateIndexAsync` only reprocesses changed files
- [ ] `SyncIndexAsync` detects added, modified, and deleted files
- [ ] `CancelIndexing`/`PauseIndexing`/`ResumeIndexing` work correctly
- [ ] All events are raised at appropriate times
- [ ] Progress is reported at configured intervals
- [ ] Gitignore patterns are respected
- [ ] Include/exclude glob patterns work correctly
- [ ] File size limits are enforced

### Quality Requirements
- [ ] Thread-safe operation with concurrent dictionary and semaphore
- [ ] Proper cancellation token propagation
- [ ] Comprehensive error handling with ContinueOnError support
- [ ] Resource cleanup in finally blocks and DisposeAsync
- [ ] All public members have XML documentation

---

## Future Considerations

Items explicitly deferred to later sub-versions:
- **v0.7.3d**: `FileProcessor` for detailed file processing pipeline
- **v0.7.3e**: `GitignoreParser` implementation
- **v0.7.3f**: `FileScanner` for advanced scanning
- **v0.7.3g**: `IndexingJobQueue` implementation
- **v0.7.3h**: `FileWatcherService` implementation
