# Design Specification: AIntern v0.7.2 "Vector Storage"

## Overview

This document provides a comprehensive design specification for v0.7.2 of The Senior Intern project. This version integrates SQLite-vec for local vector storage, implements index management, and provides CRUD operations for embeddings. This is the foundation for storing and retrieving code embeddings that power the RAG-based semantic search.

### Objectives
- Integrate SQLite-vec as the vector storage backend
- Implement the `IVectorStore` interface for vector operations
- Create database schema for vector indexes, chunks, and metadata
- Build efficient vector similarity search with filtering capabilities
- Implement index lifecycle management (create, update, delete)
- Add file tracking for incremental indexing support
- Provide statistics and health monitoring for indexes

### Prerequisites
- v0.7.1 (Embedding Foundation) completed
- SQLite-vec extension available for the target platform
- Understanding of vector similarity search concepts

### Why SQLite-vec?

| Consideration | SQLite-vec | Alternatives (FAISS, Qdrant) |
|--------------|------------|------------------------------|
| **Local-first** | Single file, no server | Requires separate process or embedded binary |
| **Cross-platform** | Windows, macOS, Linux | Platform-specific builds |
| **Integration** | Joins with metadata tables | Separate storage systems |
| **Dependencies** | SQLite extension only | Large native libraries |
| **Index Types** | IVF, linear scan | HNSW, IVF, PQ |
| **Maintenance** | Zero-config | Index optimization required |

---

## Sub-version Breakdown

| Version | Focus | Files to Create | Files to Modify |
|---------|-------|-----------------|-----------------|
| v0.7.2a | Vector Store Interface | 4 | 0 |
| v0.7.2b | Vector Index Models | 4 | 0 |
| v0.7.2c | Search Models & Options | 3 | 0 |
| v0.7.2d | SQLite-vec Database Schema | 2 | 0 |
| v0.7.2e | SQLite-vec Store Implementation | 3 | 1 |
| v0.7.2f | Index Management Operations | 2 | 0 |
| v0.7.2g | Vector Search Implementation | 2 | 0 |
| v0.7.2h | File Tracking & Change Detection | 3 | 0 |
| v0.7.2i | Statistics & Health Monitoring | 2 | 0 |
| v0.7.2j | Unit Testing & Integration | 8 | 0 |

**Totals: 33 files to create, 1 file to modify**

---

## v0.7.2a: Vector Store Interface

### Objective
Define the core interface for vector storage operations, establishing the contract for storing and retrieving embeddings with metadata.

### File: `src/SeniorIntern.Core/Interfaces/IVectorStore.cs`

```csharp
using System;
using System.Collections.Generic;
using System.Threading;
using System.Threading.Tasks;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Core.Interfaces;

/// <summary>
/// Interface for vector storage and retrieval operations.
/// Provides CRUD operations for vector indexes and similarity search capabilities.
/// </summary>
public interface IVectorStore : IAsyncDisposable
{
    /// <summary>
    /// Whether the vector store is initialized and ready for operations.
    /// </summary>
    bool IsInitialized { get; }

    /// <summary>
    /// Initialize the vector store (create database, load extensions, etc.).
    /// Must be called before any other operations.
    /// </summary>
    /// <param name="ct">Cancellation token.</param>
    Task InitializeAsync(CancellationToken ct = default);

    #region Index Management

    /// <summary>
    /// Create a new vector index for a workspace.
    /// </summary>
    /// <param name="workspacePath">Absolute path to the workspace directory.</param>
    /// <param name="name">Human-readable name for the index.</param>
    /// <param name="embeddingDimension">Dimension of embedding vectors to store.</param>
    /// <param name="settings">Optional index settings.</param>
    /// <param name="ct">Cancellation token.</param>
    /// <returns>The created vector index.</returns>
    Task<VectorIndex> CreateIndexAsync(
        string workspacePath,
        string name,
        int embeddingDimension,
        VectorIndexSettings? settings = null,
        CancellationToken ct = default);

    /// <summary>
    /// Get an existing index by its ID.
    /// </summary>
    /// <param name="indexId">The unique index identifier.</param>
    /// <param name="ct">Cancellation token.</param>
    /// <returns>The index if found, null otherwise.</returns>
    Task<VectorIndex?> GetIndexAsync(string indexId, CancellationToken ct = default);

    /// <summary>
    /// Get the index associated with a workspace path.
    /// </summary>
    /// <param name="workspacePath">Absolute path to the workspace directory.</param>
    /// <param name="ct">Cancellation token.</param>
    /// <returns>The index if found, null otherwise.</returns>
    Task<VectorIndex?> GetIndexForWorkspaceAsync(
        string workspacePath,
        CancellationToken ct = default);

    /// <summary>
    /// List all vector indexes.
    /// </summary>
    /// <param name="ct">Cancellation token.</param>
    /// <returns>List of all indexes.</returns>
    Task<IReadOnlyList<VectorIndex>> ListIndexesAsync(CancellationToken ct = default);

    /// <summary>
    /// Update index metadata and settings.
    /// </summary>
    /// <param name="index">The index with updated values.</param>
    /// <param name="ct">Cancellation token.</param>
    Task UpdateIndexAsync(VectorIndex index, CancellationToken ct = default);

    /// <summary>
    /// Delete an index and all associated data.
    /// </summary>
    /// <param name="indexId">The index to delete.</param>
    /// <param name="ct">Cancellation token.</param>
    Task DeleteIndexAsync(string indexId, CancellationToken ct = default);

    #endregion

    #region Chunk Operations

    /// <summary>
    /// Add chunks with their embeddings to an index.
    /// </summary>
    /// <param name="indexId">Target index ID.</param>
    /// <param name="chunks">Chunks with their embedding vectors.</param>
    /// <param name="ct">Cancellation token.</param>
    Task AddChunksAsync(
        string indexId,
        IEnumerable<ChunkWithEmbedding> chunks,
        CancellationToken ct = default);

    /// <summary>
    /// Add chunks in a batch with progress reporting.
    /// </summary>
    /// <param name="indexId">Target index ID.</param>
    /// <param name="chunks">Chunks with their embedding vectors.</param>
    /// <param name="progress">Optional progress reporter.</param>
    /// <param name="ct">Cancellation token.</param>
    Task AddChunksBatchAsync(
        string indexId,
        IEnumerable<ChunkWithEmbedding> chunks,
        IProgress<ChunkStorageProgress>? progress = null,
        CancellationToken ct = default);

    /// <summary>
    /// Get a specific chunk by ID.
    /// </summary>
    /// <param name="chunkId">The chunk identifier.</param>
    /// <param name="includeEmbedding">Whether to include the embedding vector.</param>
    /// <param name="ct">Cancellation token.</param>
    /// <returns>The chunk if found, null otherwise.</returns>
    Task<StoredChunk?> GetChunkAsync(
        string chunkId,
        bool includeEmbedding = false,
        CancellationToken ct = default);

    /// <summary>
    /// Remove all chunks for a specific file (for re-indexing).
    /// </summary>
    /// <param name="indexId">The index containing the chunks.</param>
    /// <param name="filePath">Relative path to the file.</param>
    /// <param name="ct">Cancellation token.</param>
    /// <returns>Number of chunks removed.</returns>
    Task<int> RemoveChunksForFileAsync(
        string indexId,
        string filePath,
        CancellationToken ct = default);

    /// <summary>
    /// Remove all chunks for multiple files.
    /// </summary>
    /// <param name="indexId">The index containing the chunks.</param>
    /// <param name="filePaths">Relative paths to the files.</param>
    /// <param name="ct">Cancellation token.</param>
    /// <returns>Total number of chunks removed.</returns>
    Task<int> RemoveChunksForFilesAsync(
        string indexId,
        IEnumerable<string> filePaths,
        CancellationToken ct = default);

    /// <summary>
    /// Clear all chunks from an index without deleting the index itself.
    /// </summary>
    /// <param name="indexId">The index to clear.</param>
    /// <param name="ct">Cancellation token.</param>
    Task ClearIndexAsync(string indexId, CancellationToken ct = default);

    #endregion

    #region Search Operations

    /// <summary>
    /// Search for similar chunks using vector similarity.
    /// </summary>
    /// <param name="indexId">The index to search.</param>
    /// <param name="queryEmbedding">The query embedding vector.</param>
    /// <param name="options">Search options and filters.</param>
    /// <param name="ct">Cancellation token.</param>
    /// <returns>Ranked list of matching chunks.</returns>
    Task<IReadOnlyList<ChunkSearchResult>> SearchAsync(
        string indexId,
        float[] queryEmbedding,
        VectorSearchOptions options,
        CancellationToken ct = default);

    /// <summary>
    /// Search across multiple indexes.
    /// </summary>
    /// <param name="indexIds">Indexes to search.</param>
    /// <param name="queryEmbedding">The query embedding vector.</param>
    /// <param name="options">Search options and filters.</param>
    /// <param name="ct">Cancellation token.</param>
    /// <returns>Ranked list of matching chunks from all indexes.</returns>
    Task<IReadOnlyList<ChunkSearchResult>> SearchMultipleAsync(
        IEnumerable<string> indexIds,
        float[] queryEmbedding,
        VectorSearchOptions options,
        CancellationToken ct = default);

    #endregion

    #region File Tracking

    /// <summary>
    /// Get the indexing status of a specific file.
    /// </summary>
    /// <param name="indexId">The index to check.</param>
    /// <param name="filePath">Relative path to the file.</param>
    /// <param name="ct">Cancellation token.</param>
    /// <returns>File status if indexed, null otherwise.</returns>
    Task<IndexedFile?> GetIndexedFileAsync(
        string indexId,
        string filePath,
        CancellationToken ct = default);

    /// <summary>
    /// Get all indexed files for an index.
    /// </summary>
    /// <param name="indexId">The index to query.</param>
    /// <param name="ct">Cancellation token.</param>
    /// <returns>List of all indexed files.</returns>
    Task<IReadOnlyList<IndexedFile>> GetIndexedFilesAsync(
        string indexId,
        CancellationToken ct = default);

    /// <summary>
    /// Update or insert file indexing status.
    /// </summary>
    /// <param name="indexId">The index containing the file.</param>
    /// <param name="file">File indexing information.</param>
    /// <param name="ct">Cancellation token.</param>
    Task UpsertIndexedFileAsync(
        string indexId,
        IndexedFile file,
        CancellationToken ct = default);

    /// <summary>
    /// Remove a file tracking record.
    /// </summary>
    /// <param name="indexId">The index containing the file.</param>
    /// <param name="filePath">Relative path to the file.</param>
    /// <param name="ct">Cancellation token.</param>
    Task RemoveIndexedFileAsync(
        string indexId,
        string filePath,
        CancellationToken ct = default);

    /// <summary>
    /// Find files that need re-indexing based on hash comparison.
    /// </summary>
    /// <param name="indexId">The index to check.</param>
    /// <param name="fileHashes">Current file paths and their content hashes.</param>
    /// <param name="ct">Cancellation token.</param>
    /// <returns>Files that are new, modified, or deleted.</returns>
    Task<FileChangeSet> DetectFileChangesAsync(
        string indexId,
        IReadOnlyDictionary<string, string> fileHashes,
        CancellationToken ct = default);

    #endregion

    #region Statistics

    /// <summary>
    /// Get statistics for a specific index.
    /// </summary>
    /// <param name="indexId">The index to analyze.</param>
    /// <param name="ct">Cancellation token.</param>
    /// <returns>Index statistics.</returns>
    Task<IndexStatistics> GetStatisticsAsync(
        string indexId,
        CancellationToken ct = default);

    /// <summary>
    /// Get overall vector store statistics.
    /// </summary>
    /// <param name="ct">Cancellation token.</param>
    /// <returns>Aggregate statistics for all indexes.</returns>
    Task<VectorStoreStatistics> GetOverallStatisticsAsync(CancellationToken ct = default);

    #endregion

    /// <summary>
    /// Event raised when store state changes (initialization, errors, etc.).
    /// </summary>
    event EventHandler<VectorStoreStateChangedEventArgs>? StateChanged;
}
```

### File: `src/SeniorIntern.Core/Models/ChunkWithEmbedding.cs`

```csharp
using System;

namespace SeniorIntern.Core.Models;

/// <summary>
/// A text chunk paired with its embedding vector for storage.
/// </summary>
public sealed class ChunkWithEmbedding
{
    /// <summary>
    /// The text chunk with metadata.
    /// </summary>
    public required TextChunk Chunk { get; init; }

    /// <summary>
    /// The embedding vector for the chunk content.
    /// </summary>
    public required float[] Embedding { get; init; }

    /// <summary>
    /// ID of the indexed file this chunk belongs to.
    /// </summary>
    public required string FileId { get; init; }

    /// <summary>
    /// Validates that the embedding has the expected dimension.
    /// </summary>
    /// <param name="expectedDimension">Expected embedding dimension.</param>
    /// <returns>True if valid, false otherwise.</returns>
    public bool ValidateEmbeddingDimension(int expectedDimension)
    {
        return Embedding.Length == expectedDimension;
    }
}
```

### File: `src/SeniorIntern.Core/Models/StoredChunk.cs`

```csharp
using System;

namespace SeniorIntern.Core.Models;

/// <summary>
/// A chunk as stored in the vector database, with optional embedding.
/// </summary>
public sealed class StoredChunk
{
    /// <summary>
    /// Unique identifier for this chunk.
    /// </summary>
    public required string Id { get; init; }

    /// <summary>
    /// ID of the index containing this chunk.
    /// </summary>
    public required string IndexId { get; init; }

    /// <summary>
    /// ID of the indexed file this chunk belongs to.
    /// </summary>
    public required string FileId { get; init; }

    /// <summary>
    /// The actual text content.
    /// </summary>
    public required string Content { get; init; }

    /// <summary>
    /// Relative file path within the workspace.
    /// </summary>
    public required string FilePath { get; init; }

    /// <summary>
    /// Starting line number (1-based).
    /// </summary>
    public int StartLine { get; init; }

    /// <summary>
    /// Ending line number (1-based).
    /// </summary>
    public int EndLine { get; init; }

    /// <summary>
    /// Character offset from start of file.
    /// </summary>
    public int StartOffset { get; init; }

    /// <summary>
    /// End character offset.
    /// </summary>
    public int EndOffset { get; init; }

    /// <summary>
    /// Type of chunk content.
    /// </summary>
    public ChunkType ChunkType { get; init; }

    /// <summary>
    /// Programming language (if applicable).
    /// </summary>
    public string? Language { get; init; }

    /// <summary>
    /// Symbol name if chunk represents a specific symbol.
    /// </summary>
    public string? SymbolName { get; init; }

    /// <summary>
    /// Type of symbol (class, method, etc.).
    /// </summary>
    public SymbolType? SymbolType { get; init; }

    /// <summary>
    /// Parent symbol name (e.g., class containing a method).
    /// </summary>
    public string? ParentSymbol { get; init; }

    /// <summary>
    /// Approximate token count of the content.
    /// </summary>
    public int TokenCount { get; init; }

    /// <summary>
    /// When this chunk was indexed.
    /// </summary>
    public DateTime IndexedAt { get; init; }

    /// <summary>
    /// The embedding vector (only populated if requested).
    /// </summary>
    public float[]? Embedding { get; init; }

    /// <summary>
    /// Convert to TextChunk model.
    /// </summary>
    public TextChunk ToTextChunk()
    {
        return new TextChunk
        {
            Id = Guid.Parse(Id),
            Content = Content,
            FilePath = FilePath,
            StartLine = StartLine,
            EndLine = EndLine,
            StartOffset = StartOffset,
            EndOffset = EndOffset,
            Type = ChunkType,
            Language = Language,
            SymbolName = SymbolName,
            SymbolType = SymbolType,
            ParentSymbol = ParentSymbol,
            TokenCount = TokenCount
        };
    }
}
```

### File: `src/SeniorIntern.Core/Models/ChunkStorageProgress.cs`

```csharp
namespace SeniorIntern.Core.Models;

/// <summary>
/// Progress information for chunk storage operations.
/// </summary>
public sealed class ChunkStorageProgress
{
    /// <summary>
    /// Number of chunks stored so far.
    /// </summary>
    public int StoredCount { get; init; }

    /// <summary>
    /// Total number of chunks to store.
    /// </summary>
    public int TotalCount { get; init; }

    /// <summary>
    /// Percentage complete (0-100).
    /// </summary>
    public double PercentComplete => TotalCount > 0
        ? (double)StoredCount / TotalCount * 100
        : 0;

    /// <summary>
    /// Current operation phase.
    /// </summary>
    public ChunkStoragePhase Phase { get; init; }

    /// <summary>
    /// Optional message describing current operation.
    /// </summary>
    public string? Message { get; init; }
}

/// <summary>
/// Phases of chunk storage operations.
/// </summary>
public enum ChunkStoragePhase
{
    /// <summary>
    /// Preparing data for storage.
    /// </summary>
    Preparing,

    /// <summary>
    /// Inserting vectors into database.
    /// </summary>
    InsertingVectors,

    /// <summary>
    /// Inserting chunk metadata.
    /// </summary>
    InsertingMetadata,

    /// <summary>
    /// Committing transaction.
    /// </summary>
    Committing,

    /// <summary>
    /// Operation complete.
    /// </summary>
    Complete
}
```

---

## v0.7.2b: Vector Index Models

### Objective
Define the models for vector indexes, including configuration, status tracking, and settings.

### File: `src/SeniorIntern.Core/Models/VectorIndex.cs`

```csharp
using System;

namespace SeniorIntern.Core.Models;

/// <summary>
/// Represents a vector index for a workspace.
/// </summary>
public sealed class VectorIndex
{
    /// <summary>
    /// Unique identifier for this index.
    /// </summary>
    public string Id { get; init; } = Guid.NewGuid().ToString();

    /// <summary>
    /// Human-readable name for the index.
    /// </summary>
    public required string Name { get; init; }

    /// <summary>
    /// Absolute path to the indexed workspace.
    /// </summary>
    public required string WorkspacePath { get; init; }

    /// <summary>
    /// When the index was created.
    /// </summary>
    public DateTime CreatedAt { get; init; } = DateTime.UtcNow;

    /// <summary>
    /// When the index was last updated.
    /// </summary>
    public DateTime UpdatedAt { get; set; } = DateTime.UtcNow;

    /// <summary>
    /// Name/path of the embedding model used.
    /// </summary>
    public required string EmbeddingModel { get; init; }

    /// <summary>
    /// Dimension of stored embedding vectors.
    /// </summary>
    public required int EmbeddingDimension { get; init; }

    /// <summary>
    /// Number of chunks stored in the index.
    /// </summary>
    public int ChunkCount { get; set; }

    /// <summary>
    /// Number of files indexed.
    /// </summary>
    public int FileCount { get; set; }

    /// <summary>
    /// Total size of indexed files in bytes.
    /// </summary>
    public long TotalFileSizeBytes { get; set; }

    /// <summary>
    /// Current status of the index.
    /// </summary>
    public IndexStatus Status { get; set; } = IndexStatus.Active;

    /// <summary>
    /// Index configuration settings.
    /// </summary>
    public VectorIndexSettings Settings { get; init; } = new();

    /// <summary>
    /// Version number for schema migrations.
    /// </summary>
    public int SchemaVersion { get; init; } = 1;

    /// <summary>
    /// Optional description or notes.
    /// </summary>
    public string? Description { get; set; }

    /// <summary>
    /// Last error message if status is Error.
    /// </summary>
    public string? LastError { get; set; }

    /// <summary>
    /// When the last full index operation completed.
    /// </summary>
    public DateTime? LastFullIndexAt { get; set; }

    /// <summary>
    /// When the last incremental update completed.
    /// </summary>
    public DateTime? LastIncrementalUpdateAt { get; set; }
}

/// <summary>
/// Status of a vector index.
/// </summary>
public enum IndexStatus
{
    /// <summary>
    /// Index is active and ready for queries.
    /// </summary>
    Active,

    /// <summary>
    /// Index is being created or rebuilt.
    /// </summary>
    Building,

    /// <summary>
    /// Index is being updated incrementally.
    /// </summary>
    Updating,

    /// <summary>
    /// Indexing is paused.
    /// </summary>
    Paused,

    /// <summary>
    /// Index encountered an error.
    /// </summary>
    Error,

    /// <summary>
    /// Index is marked for deletion.
    /// </summary>
    Deleting,

    /// <summary>
    /// Index needs rebuilding (e.g., schema migration).
    /// </summary>
    NeedsRebuild
}
```

### File: `src/SeniorIntern.Core/Models/VectorIndexSettings.cs`

```csharp
using System;
using System.Collections.Generic;

namespace SeniorIntern.Core.Models;

/// <summary>
/// Configuration settings for a vector index.
/// </summary>
public sealed class VectorIndexSettings
{
    /// <summary>
    /// Chunking configuration for text processing.
    /// </summary>
    public ChunkingOptions ChunkingOptions { get; init; } = new();

    /// <summary>
    /// Glob patterns for files to include.
    /// Default includes common source code file types.
    /// </summary>
    public IReadOnlyList<string> IncludePatterns { get; init; } = new[]
    {
        // .NET
        "**/*.cs", "**/*.fs", "**/*.vb", "**/*.csproj", "**/*.fsproj",
        // Web
        "**/*.ts", "**/*.tsx", "**/*.js", "**/*.jsx", "**/*.vue", "**/*.svelte",
        "**/*.html", "**/*.css", "**/*.scss", "**/*.less",
        // Python
        "**/*.py", "**/*.pyi", "**/*.pyx",
        // JVM
        "**/*.java", "**/*.kt", "**/*.scala", "**/*.groovy",
        // Systems
        "**/*.go", "**/*.rs", "**/*.cpp", "**/*.c", "**/*.h", "**/*.hpp",
        // Other
        "**/*.swift", "**/*.rb", "**/*.php", "**/*.lua",
        // Config/Docs
        "**/*.md", "**/*.json", "**/*.yaml", "**/*.yml", "**/*.xml", "**/*.toml"
    };

    /// <summary>
    /// Glob patterns for files/directories to exclude.
    /// </summary>
    public IReadOnlyList<string> ExcludePatterns { get; init; } = new[]
    {
        // Build outputs
        "**/node_modules/**", "**/bin/**", "**/obj/**", "**/dist/**", "**/build/**",
        "**/target/**", "**/out/**", "**/.next/**", "**/.nuxt/**",
        // Package managers
        "**/packages/**", "**/vendor/**", "**/.npm/**", "**/.yarn/**",
        // Version control
        "**/.git/**", "**/.svn/**", "**/.hg/**",
        // IDE
        "**/.vs/**", "**/.idea/**", "**/.vscode/**",
        // Minified/generated
        "**/*.min.js", "**/*.min.css", "**/*.map", "**/*.d.ts",
        // Large files
        "**/*.lock", "**/package-lock.json", "**/yarn.lock",
        // Temporary
        "**/tmp/**", "**/temp/**", "**/.cache/**"
    };

    /// <summary>
    /// Maximum file size to index in kilobytes.
    /// Files larger than this are skipped.
    /// </summary>
    public int MaxFileSizeKb { get; init; } = 1024; // 1MB

    /// <summary>
    /// Minimum file size to index in bytes.
    /// Very small files are often not useful.
    /// </summary>
    public int MinFileSizeBytes { get; init; } = 10;

    /// <summary>
    /// Whether to index hidden files (starting with .).
    /// </summary>
    public bool IndexHiddenFiles { get; init; } = false;

    /// <summary>
    /// Whether to respect .gitignore rules.
    /// </summary>
    public bool RespectGitignore { get; init; } = true;

    /// <summary>
    /// Whether to follow symbolic links.
    /// </summary>
    public bool FollowSymlinks { get; init; } = false;

    /// <summary>
    /// Maximum directory depth to traverse (-1 for unlimited).
    /// </summary>
    public int MaxDepth { get; init; } = -1;

    /// <summary>
    /// Whether to automatically re-index when files change.
    /// </summary>
    public bool AutoReindex { get; init; } = true;

    /// <summary>
    /// Delay in milliseconds before re-indexing after file changes.
    /// Helps batch rapid file changes.
    /// </summary>
    public int AutoReindexDelayMs { get; init; } = 5000;

    /// <summary>
    /// Number of parallel file processing threads.
    /// </summary>
    public int ParallelProcessing { get; init; } = 4;

    /// <summary>
    /// Custom file extension to language mappings.
    /// </summary>
    public IReadOnlyDictionary<string, string>? CustomLanguageMappings { get; init; }
}
```

### File: `src/SeniorIntern.Core/Models/IndexedFile.cs`

```csharp
using System;

namespace SeniorIntern.Core.Models;

/// <summary>
/// Tracks a file that has been indexed.
/// </summary>
public sealed class IndexedFile
{
    /// <summary>
    /// Unique identifier for this file record.
    /// </summary>
    public string Id { get; init; } = Guid.NewGuid().ToString();

    /// <summary>
    /// ID of the index containing this file.
    /// </summary>
    public required string IndexId { get; init; }

    /// <summary>
    /// Relative path to the file within the workspace.
    /// </summary>
    public required string FilePath { get; init; }

    /// <summary>
    /// Hash of the file contents for change detection.
    /// Uses SHA-256 by default.
    /// </summary>
    public required string FileHash { get; init; }

    /// <summary>
    /// Size of the file in bytes.
    /// </summary>
    public long FileSize { get; init; }

    /// <summary>
    /// Last modification time of the file.
    /// </summary>
    public DateTime LastModified { get; init; }

    /// <summary>
    /// When this file was indexed.
    /// </summary>
    public DateTime IndexedAt { get; init; } = DateTime.UtcNow;

    /// <summary>
    /// Number of chunks created from this file.
    /// </summary>
    public int ChunkCount { get; set; }

    /// <summary>
    /// Detected programming language.
    /// </summary>
    public string? Language { get; init; }

    /// <summary>
    /// File encoding (UTF-8, ASCII, etc.).
    /// </summary>
    public string? Encoding { get; init; }

    /// <summary>
    /// Line count in the file.
    /// </summary>
    public int LineCount { get; init; }

    /// <summary>
    /// Indexing status for this file.
    /// </summary>
    public FileIndexStatus Status { get; set; } = FileIndexStatus.Indexed;

    /// <summary>
    /// Error message if indexing failed.
    /// </summary>
    public string? ErrorMessage { get; set; }

    /// <summary>
    /// Time taken to index this file in milliseconds.
    /// </summary>
    public int IndexingDurationMs { get; init; }
}

/// <summary>
/// Status of file indexing.
/// </summary>
public enum FileIndexStatus
{
    /// <summary>
    /// File is pending indexing.
    /// </summary>
    Pending,

    /// <summary>
    /// File is currently being indexed.
    /// </summary>
    Indexing,

    /// <summary>
    /// File was successfully indexed.
    /// </summary>
    Indexed,

    /// <summary>
    /// File was skipped (too large, excluded, etc.).
    /// </summary>
    Skipped,

    /// <summary>
    /// Indexing failed with an error.
    /// </summary>
    Error
}
```

### File: `src/SeniorIntern.Core/Models/FileChangeSet.cs`

```csharp
using System.Collections.Generic;

namespace SeniorIntern.Core.Models;

/// <summary>
/// Set of file changes detected for incremental indexing.
/// </summary>
public sealed class FileChangeSet
{
    /// <summary>
    /// Files that are new and need to be indexed.
    /// </summary>
    public IReadOnlyList<string> AddedFiles { get; init; } = [];

    /// <summary>
    /// Files that have been modified and need re-indexing.
    /// </summary>
    public IReadOnlyList<string> ModifiedFiles { get; init; } = [];

    /// <summary>
    /// Files that have been deleted and should be removed from the index.
    /// </summary>
    public IReadOnlyList<string> DeletedFiles { get; init; } = [];

    /// <summary>
    /// Files that are unchanged.
    /// </summary>
    public IReadOnlyList<string> UnchangedFiles { get; init; } = [];

    /// <summary>
    /// Whether there are any changes.
    /// </summary>
    public bool HasChanges =>
        AddedFiles.Count > 0 ||
        ModifiedFiles.Count > 0 ||
        DeletedFiles.Count > 0;

    /// <summary>
    /// Total number of files that need processing.
    /// </summary>
    public int TotalChanges =>
        AddedFiles.Count +
        ModifiedFiles.Count +
        DeletedFiles.Count;
}
```

---

## v0.7.2c: Search Models & Options

### Objective
Define the models for vector search operations, including query options, filters, and result formatting.

### File: `src/SeniorIntern.Core/Models/VectorSearchOptions.cs`

```csharp
using System.Collections.Generic;

namespace SeniorIntern.Core.Models;

/// <summary>
/// Options for vector similarity search.
/// </summary>
public sealed class VectorSearchOptions
{
    /// <summary>
    /// Maximum number of results to return.
    /// </summary>
    public int TopK { get; init; } = 10;

    /// <summary>
    /// Minimum similarity score (0.0 to 1.0).
    /// Results below this threshold are excluded.
    /// </summary>
    public float MinScore { get; init; } = 0.5f;

    /// <summary>
    /// Filter results to specific file patterns (glob syntax).
    /// </summary>
    public IReadOnlyList<string>? FilePatterns { get; init; }

    /// <summary>
    /// Filter results to specific languages.
    /// </summary>
    public IReadOnlyList<string>? Languages { get; init; }

    /// <summary>
    /// Filter results to specific chunk types.
    /// </summary>
    public IReadOnlyList<ChunkType>? ChunkTypes { get; init; }

    /// <summary>
    /// Filter results to specific symbol types.
    /// </summary>
    public IReadOnlyList<SymbolType>? SymbolTypes { get; init; }

    /// <summary>
    /// Filter to chunks containing specific symbol names.
    /// </summary>
    public IReadOnlyList<string>? SymbolNames { get; init; }

    /// <summary>
    /// Whether to include full chunk content in results.
    /// </summary>
    public bool IncludeContent { get; init; } = true;

    /// <summary>
    /// Whether to include embedding vectors in results.
    /// </summary>
    public bool IncludeEmbeddings { get; init; } = false;

    /// <summary>
    /// Whether to deduplicate overlapping chunks from results.
    /// </summary>
    public bool DeduplicateOverlapping { get; init; } = true;

    /// <summary>
    /// Overlap threshold for deduplication (0.0 to 1.0).
    /// Higher values require more overlap to deduplicate.
    /// </summary>
    public float DeduplicationThreshold { get; init; } = 0.5f;

    /// <summary>
    /// Whether to expand context around matched chunks.
    /// </summary>
    public bool ExpandContext { get; init; } = false;

    /// <summary>
    /// Number of lines to expand above the match.
    /// </summary>
    public int ContextLinesAbove { get; init; } = 5;

    /// <summary>
    /// Number of lines to expand below the match.
    /// </summary>
    public int ContextLinesBelow { get; init; } = 5;

    /// <summary>
    /// Search algorithm to use.
    /// </summary>
    public VectorSearchAlgorithm Algorithm { get; init; } = VectorSearchAlgorithm.Auto;

    /// <summary>
    /// Number of probes for IVF index (higher = more accurate but slower).
    /// </summary>
    public int IvfProbes { get; init; } = 10;

    /// <summary>
    /// Creates default options for semantic code search.
    /// </summary>
    public static VectorSearchOptions DefaultCodeSearch => new()
    {
        TopK = 10,
        MinScore = 0.6f,
        ChunkTypes = [ChunkType.Code, ChunkType.TypeDefinition],
        DeduplicateOverlapping = true,
        ExpandContext = true,
        ContextLinesAbove = 3,
        ContextLinesBelow = 3
    };

    /// <summary>
    /// Creates options optimized for finding documentation.
    /// </summary>
    public static VectorSearchOptions DocumentationSearch => new()
    {
        TopK = 5,
        MinScore = 0.5f,
        ChunkTypes = [ChunkType.Documentation, ChunkType.Comment, ChunkType.Markdown],
        DeduplicateOverlapping = true
    };
}

/// <summary>
/// Vector search algorithm to use.
/// </summary>
public enum VectorSearchAlgorithm
{
    /// <summary>
    /// Automatically select based on index size.
    /// </summary>
    Auto,

    /// <summary>
    /// Exhaustive linear scan (most accurate, slowest).
    /// </summary>
    Linear,

    /// <summary>
    /// Inverted File Index (good balance).
    /// </summary>
    IVF
}
```

### File: `src/SeniorIntern.Core/Models/ChunkSearchResult.cs`

```csharp
using System;

namespace SeniorIntern.Core.Models;

/// <summary>
/// A chunk returned from vector similarity search.
/// </summary>
public sealed class ChunkSearchResult
{
    /// <summary>
    /// Unique identifier of the chunk.
    /// </summary>
    public required string ChunkId { get; init; }

    /// <summary>
    /// ID of the index containing this chunk.
    /// </summary>
    public required string IndexId { get; init; }

    /// <summary>
    /// Similarity score (0.0 to 1.0, higher is more similar).
    /// </summary>
    public float Score { get; init; }

    /// <summary>
    /// Distance from query vector (lower is more similar).
    /// </summary>
    public float Distance { get; init; }

    /// <summary>
    /// The matched chunk data.
    /// </summary>
    public required StoredChunk Chunk { get; init; }

    /// <summary>
    /// Expanded context around the chunk (if requested).
    /// </summary>
    public string? ExpandedContext { get; init; }

    /// <summary>
    /// Start line of expanded context.
    /// </summary>
    public int? ExpandedStartLine { get; init; }

    /// <summary>
    /// End line of expanded context.
    /// </summary>
    public int? ExpandedEndLine { get; init; }

    /// <summary>
    /// Rank position in the result set (1-based).
    /// </summary>
    public int Rank { get; init; }

    /// <summary>
    /// Converts score to a human-readable percentage.
    /// </summary>
    public string ScorePercentage => $"{Score * 100:F1}%";

    /// <summary>
    /// Creates a formatted reference string for display.
    /// </summary>
    public string FormatReference()
    {
        var location = $"{Chunk.FilePath}:{Chunk.StartLine}";
        if (Chunk.EndLine > Chunk.StartLine)
            location += $"-{Chunk.EndLine}";

        if (!string.IsNullOrEmpty(Chunk.SymbolName))
            return $"{Chunk.SymbolName} ({location})";

        return location;
    }
}
```

### File: `src/SeniorIntern.Core/Models/IndexStatistics.cs`

```csharp
using System;
using System.Collections.Generic;

namespace SeniorIntern.Core.Models;

/// <summary>
/// Statistics for a single vector index.
/// </summary>
public sealed class IndexStatistics
{
    /// <summary>
    /// ID of the index.
    /// </summary>
    public required string IndexId { get; init; }

    /// <summary>
    /// Name of the index.
    /// </summary>
    public required string IndexName { get; init; }

    /// <summary>
    /// Total number of indexed files.
    /// </summary>
    public int TotalFiles { get; init; }

    /// <summary>
    /// Total number of stored chunks.
    /// </summary>
    public int TotalChunks { get; init; }

    /// <summary>
    /// Total size of indexed files in bytes.
    /// </summary>
    public long TotalFileSizeBytes { get; init; }

    /// <summary>
    /// Size of the vector data in bytes.
    /// </summary>
    public long VectorDataSizeBytes { get; init; }

    /// <summary>
    /// Size of metadata in bytes.
    /// </summary>
    public long MetadataSizeBytes { get; init; }

    /// <summary>
    /// Total database size in bytes.
    /// </summary>
    public long TotalDatabaseSizeBytes => VectorDataSizeBytes + MetadataSizeBytes;

    /// <summary>
    /// Average chunks per file.
    /// </summary>
    public double AverageChunksPerFile => TotalFiles > 0
        ? (double)TotalChunks / TotalFiles
        : 0;

    /// <summary>
    /// Average tokens per chunk.
    /// </summary>
    public double AverageTokensPerChunk { get; init; }

    /// <summary>
    /// File count by programming language.
    /// </summary>
    public IReadOnlyDictionary<string, int> FilesByLanguage { get; init; } =
        new Dictionary<string, int>();

    /// <summary>
    /// Chunk count by type.
    /// </summary>
    public IReadOnlyDictionary<ChunkType, int> ChunksByType { get; init; } =
        new Dictionary<ChunkType, int>();

    /// <summary>
    /// Chunk count by symbol type.
    /// </summary>
    public IReadOnlyDictionary<SymbolType, int> ChunksBySymbolType { get; init; } =
        new Dictionary<SymbolType, int>();

    /// <summary>
    /// Number of files with errors.
    /// </summary>
    public int FilesWithErrors { get; init; }

    /// <summary>
    /// Number of skipped files.
    /// </summary>
    public int SkippedFiles { get; init; }

    /// <summary>
    /// When statistics were calculated.
    /// </summary>
    public DateTime CalculatedAt { get; init; } = DateTime.UtcNow;

    /// <summary>
    /// When the index was last modified.
    /// </summary>
    public DateTime LastModified { get; init; }

    /// <summary>
    /// Embedding model used.
    /// </summary>
    public required string EmbeddingModel { get; init; }

    /// <summary>
    /// Embedding dimension.
    /// </summary>
    public int EmbeddingDimension { get; init; }

    /// <summary>
    /// Formats total file size for display.
    /// </summary>
    public string FormattedFileSize => FormatBytes(TotalFileSizeBytes);

    /// <summary>
    /// Formats database size for display.
    /// </summary>
    public string FormattedDatabaseSize => FormatBytes(TotalDatabaseSizeBytes);

    private static string FormatBytes(long bytes)
    {
        string[] suffixes = ["B", "KB", "MB", "GB"];
        int i = 0;
        double size = bytes;
        while (size >= 1024 && i < suffixes.Length - 1)
        {
            size /= 1024;
            i++;
        }
        return $"{size:F2} {suffixes[i]}";
    }
}

/// <summary>
/// Aggregate statistics for the entire vector store.
/// </summary>
public sealed class VectorStoreStatistics
{
    /// <summary>
    /// Total number of indexes.
    /// </summary>
    public int TotalIndexes { get; init; }

    /// <summary>
    /// Number of active indexes.
    /// </summary>
    public int ActiveIndexes { get; init; }

    /// <summary>
    /// Total files across all indexes.
    /// </summary>
    public int TotalFiles { get; init; }

    /// <summary>
    /// Total chunks across all indexes.
    /// </summary>
    public int TotalChunks { get; init; }

    /// <summary>
    /// Total database size in bytes.
    /// </summary>
    public long TotalDatabaseSizeBytes { get; init; }

    /// <summary>
    /// Statistics per index.
    /// </summary>
    public IReadOnlyList<IndexStatistics> IndexStats { get; init; } = [];

    /// <summary>
    /// When statistics were calculated.
    /// </summary>
    public DateTime CalculatedAt { get; init; } = DateTime.UtcNow;
}
```

---

## v0.7.2d: SQLite-vec Database Schema

### Objective
Define the database schema for SQLite-vec, including vector tables, metadata tables, and indexes.

### File: `src/SeniorIntern.Services/VectorStore/VectorStoreSchema.cs`

```csharp
namespace SeniorIntern.Services.VectorStore;

/// <summary>
/// SQL schema definitions for the vector store database.
/// </summary>
public static class VectorStoreSchema
{
    /// <summary>
    /// Current schema version for migrations.
    /// </summary>
    public const int CurrentSchemaVersion = 1;

    /// <summary>
    /// SQL to create the schema version tracking table.
    /// </summary>
    public const string CreateSchemaVersionTable = """
        CREATE TABLE IF NOT EXISTS schema_version (
            version INTEGER PRIMARY KEY,
            applied_at TEXT NOT NULL DEFAULT (datetime('now')),
            description TEXT
        );
        """;

    /// <summary>
    /// SQL to create the vector indexes metadata table.
    /// </summary>
    public const string CreateVectorIndexesTable = """
        CREATE TABLE IF NOT EXISTS vector_indexes (
            id TEXT PRIMARY KEY,
            name TEXT NOT NULL,
            workspace_path TEXT NOT NULL UNIQUE,
            created_at TEXT NOT NULL DEFAULT (datetime('now')),
            updated_at TEXT NOT NULL DEFAULT (datetime('now')),
            embedding_model TEXT NOT NULL,
            embedding_dimension INTEGER NOT NULL,
            chunk_count INTEGER DEFAULT 0,
            file_count INTEGER DEFAULT 0,
            total_file_size_bytes INTEGER DEFAULT 0,
            status TEXT NOT NULL DEFAULT 'Active',
            settings_json TEXT,
            schema_version INTEGER DEFAULT 1,
            description TEXT,
            last_error TEXT,
            last_full_index_at TEXT,
            last_incremental_update_at TEXT
        );

        CREATE INDEX IF NOT EXISTS idx_vector_indexes_workspace
            ON vector_indexes(workspace_path);
        CREATE INDEX IF NOT EXISTS idx_vector_indexes_status
            ON vector_indexes(status);
        """;

    /// <summary>
    /// SQL to create the indexed files tracking table.
    /// </summary>
    public const string CreateIndexedFilesTable = """
        CREATE TABLE IF NOT EXISTS indexed_files (
            id TEXT PRIMARY KEY,
            index_id TEXT NOT NULL,
            file_path TEXT NOT NULL,
            file_hash TEXT NOT NULL,
            file_size INTEGER NOT NULL,
            last_modified TEXT NOT NULL,
            indexed_at TEXT NOT NULL DEFAULT (datetime('now')),
            chunk_count INTEGER DEFAULT 0,
            language TEXT,
            encoding TEXT,
            line_count INTEGER DEFAULT 0,
            status TEXT NOT NULL DEFAULT 'Indexed',
            error_message TEXT,
            indexing_duration_ms INTEGER DEFAULT 0,
            FOREIGN KEY (index_id) REFERENCES vector_indexes(id) ON DELETE CASCADE,
            UNIQUE(index_id, file_path)
        );

        CREATE INDEX IF NOT EXISTS idx_indexed_files_index
            ON indexed_files(index_id);
        CREATE INDEX IF NOT EXISTS idx_indexed_files_hash
            ON indexed_files(file_hash);
        CREATE INDEX IF NOT EXISTS idx_indexed_files_path
            ON indexed_files(file_path);
        CREATE INDEX IF NOT EXISTS idx_indexed_files_status
            ON indexed_files(status);
        """;

    /// <summary>
    /// SQL to create the chunk metadata table.
    /// </summary>
    public const string CreateChunkMetadataTable = """
        CREATE TABLE IF NOT EXISTS chunk_metadata (
            id TEXT PRIMARY KEY,
            index_id TEXT NOT NULL,
            file_id TEXT NOT NULL,
            content TEXT NOT NULL,
            start_line INTEGER NOT NULL,
            end_line INTEGER NOT NULL,
            start_offset INTEGER NOT NULL,
            end_offset INTEGER NOT NULL,
            chunk_type TEXT NOT NULL,
            language TEXT,
            symbol_name TEXT,
            symbol_type TEXT,
            parent_symbol TEXT,
            token_count INTEGER DEFAULT 0,
            indexed_at TEXT NOT NULL DEFAULT (datetime('now')),
            FOREIGN KEY (index_id) REFERENCES vector_indexes(id) ON DELETE CASCADE,
            FOREIGN KEY (file_id) REFERENCES indexed_files(id) ON DELETE CASCADE
        );

        CREATE INDEX IF NOT EXISTS idx_chunk_metadata_index
            ON chunk_metadata(index_id);
        CREATE INDEX IF NOT EXISTS idx_chunk_metadata_file
            ON chunk_metadata(file_id);
        CREATE INDEX IF NOT EXISTS idx_chunk_metadata_symbol
            ON chunk_metadata(symbol_name);
        CREATE INDEX IF NOT EXISTS idx_chunk_metadata_type
            ON chunk_metadata(chunk_type);
        CREATE INDEX IF NOT EXISTS idx_chunk_metadata_language
            ON chunk_metadata(language);
        CREATE INDEX IF NOT EXISTS idx_chunk_metadata_symbol_type
            ON chunk_metadata(symbol_type);
        """;

    /// <summary>
    /// SQL template to create a vector table for a specific index.
    /// Replace {index_id} with sanitized index ID and {dimension} with embedding dimension.
    /// </summary>
    public const string CreateVectorTableTemplate = """
        CREATE VIRTUAL TABLE IF NOT EXISTS vectors_{index_id} USING vec0(
            id TEXT PRIMARY KEY,
            embedding FLOAT[{dimension}]
        );
        """;

    /// <summary>
    /// SQL to create the search history table (for analytics).
    /// </summary>
    public const string CreateSearchHistoryTable = """
        CREATE TABLE IF NOT EXISTS search_history (
            id TEXT PRIMARY KEY,
            index_id TEXT NOT NULL,
            query_text TEXT,
            result_count INTEGER,
            top_score REAL,
            search_options_json TEXT,
            duration_ms INTEGER,
            searched_at TEXT NOT NULL DEFAULT (datetime('now')),
            FOREIGN KEY (index_id) REFERENCES vector_indexes(id) ON DELETE CASCADE
        );

        CREATE INDEX IF NOT EXISTS idx_search_history_index
            ON search_history(index_id);
        CREATE INDEX IF NOT EXISTS idx_search_history_time
            ON search_history(searched_at);
        """;

    /// <summary>
    /// Gets all schema creation statements in order.
    /// </summary>
    public static string[] GetSchemaCreationStatements() =>
    [
        CreateSchemaVersionTable,
        CreateVectorIndexesTable,
        CreateIndexedFilesTable,
        CreateChunkMetadataTable,
        CreateSearchHistoryTable
    ];

    /// <summary>
    /// SQL to drop all tables (for testing/reset).
    /// </summary>
    public const string DropAllTables = """
        DROP TABLE IF EXISTS search_history;
        DROP TABLE IF EXISTS chunk_metadata;
        DROP TABLE IF EXISTS indexed_files;
        DROP TABLE IF EXISTS vector_indexes;
        DROP TABLE IF EXISTS schema_version;
        """;
}
```

### File: `src/SeniorIntern.Services/VectorStore/VectorStoreMigrations.cs`

```csharp
using System;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.Data.Sqlite;
using Microsoft.Extensions.Logging;

namespace SeniorIntern.Services.VectorStore;

/// <summary>
/// Handles database schema migrations for the vector store.
/// </summary>
public sealed class VectorStoreMigrations
{
    private readonly SqliteConnection _connection;
    private readonly ILogger<VectorStoreMigrations> _logger;

    public VectorStoreMigrations(
        SqliteConnection connection,
        ILogger<VectorStoreMigrations> logger)
    {
        _connection = connection;
        _logger = logger;
    }

    /// <summary>
    /// Applies all pending migrations to bring database to current version.
    /// </summary>
    public async Task MigrateAsync(CancellationToken ct = default)
    {
        var currentVersion = await GetCurrentVersionAsync(ct);
        _logger.LogInformation(
            "Current schema version: {Current}, Target: {Target}",
            currentVersion,
            VectorStoreSchema.CurrentSchemaVersion);

        if (currentVersion < VectorStoreSchema.CurrentSchemaVersion)
        {
            await ApplyMigrationsAsync(currentVersion, ct);
        }
    }

    /// <summary>
    /// Creates fresh schema (for new databases).
    /// </summary>
    public async Task CreateSchemaAsync(CancellationToken ct = default)
    {
        _logger.LogInformation("Creating vector store schema...");

        foreach (var statement in VectorStoreSchema.GetSchemaCreationStatements())
        {
            await using var cmd = _connection.CreateCommand();
            cmd.CommandText = statement;
            await cmd.ExecuteNonQueryAsync(ct);
        }

        await RecordVersionAsync(VectorStoreSchema.CurrentSchemaVersion, "Initial schema", ct);
        _logger.LogInformation("Schema created at version {Version}", VectorStoreSchema.CurrentSchemaVersion);
    }

    /// <summary>
    /// Gets the current schema version, or 0 if not initialized.
    /// </summary>
    public async Task<int> GetCurrentVersionAsync(CancellationToken ct = default)
    {
        try
        {
            await using var cmd = _connection.CreateCommand();
            cmd.CommandText = "SELECT MAX(version) FROM schema_version";
            var result = await cmd.ExecuteScalarAsync(ct);
            return result is int version ? version : 0;
        }
        catch (SqliteException)
        {
            // Table doesn't exist yet
            return 0;
        }
    }

    private async Task ApplyMigrationsAsync(int fromVersion, CancellationToken ct)
    {
        _logger.LogInformation("Applying migrations from version {From}...", fromVersion);

        // Add migration handlers here as schema evolves
        // Example:
        // if (fromVersion < 2)
        //     await MigrateToVersion2Async(ct);

        _logger.LogInformation("Migrations complete");
    }

    private async Task RecordVersionAsync(int version, string description, CancellationToken ct)
    {
        await using var cmd = _connection.CreateCommand();
        cmd.CommandText = """
            INSERT INTO schema_version (version, description)
            VALUES (@version, @description)
            """;
        cmd.Parameters.AddWithValue("@version", version);
        cmd.Parameters.AddWithValue("@description", description);
        await cmd.ExecuteNonQueryAsync(ct);
    }

    // Future migration methods would go here:
    // private async Task MigrateToVersion2Async(CancellationToken ct) { ... }
}
```

---

## v0.7.2e: SQLite-vec Store Implementation

### Objective
Implement the main `SqliteVectorStore` class that provides the `IVectorStore` interface using SQLite-vec.

### File: `src/SeniorIntern.Core/Options/VectorStoreOptions.cs`

```csharp
namespace SeniorIntern.Core.Options;

/// <summary>
/// Configuration options for the vector store.
/// </summary>
public sealed class VectorStoreOptions
{
    /// <summary>
    /// Configuration section name.
    /// </summary>
    public const string SectionName = "VectorStore";

    /// <summary>
    /// Path to the SQLite database file.
    /// If not specified, uses default location in app data.
    /// </summary>
    public string? DatabasePath { get; set; }

    /// <summary>
    /// Path to the sqlite-vec extension library.
    /// If not specified, attempts to load from default locations.
    /// </summary>
    public string? ExtensionPath { get; set; }

    /// <summary>
    /// Connection pool size for concurrent operations.
    /// </summary>
    public int ConnectionPoolSize { get; set; } = 4;

    /// <summary>
    /// Default timeout for database operations in seconds.
    /// </summary>
    public int DefaultTimeoutSeconds { get; set; } = 30;

    /// <summary>
    /// Whether to enable WAL mode for better concurrent performance.
    /// </summary>
    public bool EnableWalMode { get; set; } = true;

    /// <summary>
    /// SQLite cache size in kilobytes.
    /// </summary>
    public int CacheSizeKb { get; set; } = 10240; // 10MB

    /// <summary>
    /// Whether to run VACUUM on startup.
    /// </summary>
    public bool VacuumOnStartup { get; set; } = false;

    /// <summary>
    /// Batch size for bulk insert operations.
    /// </summary>
    public int BulkInsertBatchSize { get; set; } = 100;
}
```

### File: `src/SeniorIntern.Services/VectorStore/SqliteVectorStore.cs`

```csharp
using System;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Runtime.InteropServices;
using System.Text;
using System.Text.Json;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.Data.Sqlite;
using Microsoft.Extensions.Logging;
using Microsoft.Extensions.Options;
using SeniorIntern.Core.Interfaces;
using SeniorIntern.Core.Models;
using SeniorIntern.Core.Options;

namespace SeniorIntern.Services.VectorStore;

/// <summary>
/// Vector store implementation using SQLite with sqlite-vec extension.
/// </summary>
public sealed partial class SqliteVectorStore : IVectorStore, IAsyncDisposable
{
    private readonly VectorStoreOptions _options;
    private readonly ILogger<SqliteVectorStore> _logger;
    private readonly SemaphoreSlim _initLock = new(1, 1);
    private readonly SemaphoreSlim _writeLock = new(1, 1);

    private SqliteConnection? _connection;
    private bool _isInitialized;
    private bool _isDisposed;

    public bool IsInitialized => _isInitialized;

    public event EventHandler<VectorStoreStateChangedEventArgs>? StateChanged;

    public SqliteVectorStore(
        IOptions<VectorStoreOptions> options,
        ILogger<SqliteVectorStore> logger)
    {
        _options = options.Value;
        _logger = logger;
    }

    public async Task InitializeAsync(CancellationToken ct = default)
    {
        if (_isInitialized) return;

        await _initLock.WaitAsync(ct);
        try
        {
            if (_isInitialized) return;

            _logger.LogInformation("Initializing vector store...");

            // Determine database path
            var dbPath = _options.DatabasePath ?? GetDefaultDatabasePath();
            Directory.CreateDirectory(Path.GetDirectoryName(dbPath)!);

            // Create and open connection
            var connectionString = new SqliteConnectionStringBuilder
            {
                DataSource = dbPath,
                Mode = SqliteOpenMode.ReadWriteCreate,
                Cache = SqliteCacheMode.Shared,
                DefaultTimeout = _options.DefaultTimeoutSeconds
            }.ToString();

            _connection = new SqliteConnection(connectionString);
            await _connection.OpenAsync(ct);

            // Configure SQLite settings
            await ConfigureSqliteAsync(ct);

            // Load sqlite-vec extension
            await LoadVecExtensionAsync(ct);

            // Create or migrate schema
            var migrations = new VectorStoreMigrations(
                _connection,
                _logger.CreateLogger<VectorStoreMigrations>());

            var currentVersion = await migrations.GetCurrentVersionAsync(ct);
            if (currentVersion == 0)
            {
                await migrations.CreateSchemaAsync(ct);
            }
            else
            {
                await migrations.MigrateAsync(ct);
            }

            _isInitialized = true;
            _logger.LogInformation("Vector store initialized at {Path}", dbPath);

            StateChanged?.Invoke(this, new VectorStoreStateChangedEventArgs
            {
                IsInitialized = true,
                DatabasePath = dbPath
            });
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Failed to initialize vector store");
            StateChanged?.Invoke(this, new VectorStoreStateChangedEventArgs
            {
                IsInitialized = false,
                Error = ex.Message
            });
            throw;
        }
        finally
        {
            _initLock.Release();
        }
    }

    private async Task ConfigureSqliteAsync(CancellationToken ct)
    {
        var pragmas = new[]
        {
            _options.EnableWalMode ? "PRAGMA journal_mode = WAL" : null,
            $"PRAGMA cache_size = -{_options.CacheSizeKb}",
            "PRAGMA synchronous = NORMAL",
            "PRAGMA temp_store = MEMORY",
            "PRAGMA mmap_size = 268435456", // 256MB memory-mapped I/O
            "PRAGMA foreign_keys = ON"
        };

        foreach (var pragma in pragmas.Where(p => p != null))
        {
            await using var cmd = _connection!.CreateCommand();
            cmd.CommandText = pragma;
            await cmd.ExecuteNonQueryAsync(ct);
        }

        if (_options.VacuumOnStartup)
        {
            await using var cmd = _connection!.CreateCommand();
            cmd.CommandText = "VACUUM";
            await cmd.ExecuteNonQueryAsync(ct);
        }
    }

    private async Task LoadVecExtensionAsync(CancellationToken ct)
    {
        var extensionPath = _options.ExtensionPath ?? FindVecExtension();

        if (string.IsNullOrEmpty(extensionPath))
        {
            throw new InvalidOperationException(
                "sqlite-vec extension not found. Please install it or specify ExtensionPath in configuration.");
        }

        _logger.LogDebug("Loading sqlite-vec from {Path}", extensionPath);

        await using var cmd = _connection!.CreateCommand();
        cmd.CommandText = "SELECT load_extension(@path, 'sqlite3_vec_init')";
        cmd.Parameters.AddWithValue("@path", extensionPath);

        try
        {
            await cmd.ExecuteNonQueryAsync(ct);
            _logger.LogInformation("sqlite-vec extension loaded successfully");
        }
        catch (SqliteException ex)
        {
            throw new InvalidOperationException(
                $"Failed to load sqlite-vec extension from {extensionPath}: {ex.Message}", ex);
        }
    }

    private static string? FindVecExtension()
    {
        // Platform-specific extension names
        var extensionName = RuntimeInformation.IsOSPlatform(OSPlatform.Windows)
            ? "vec0.dll"
            : RuntimeInformation.IsOSPlatform(OSPlatform.OSX)
                ? "vec0.dylib"
                : "vec0.so";

        // Search paths
        var searchPaths = new[]
        {
            AppDomain.CurrentDomain.BaseDirectory,
            Path.Combine(AppDomain.CurrentDomain.BaseDirectory, "runtimes", GetRuntimeIdentifier(), "native"),
            Environment.CurrentDirectory,
            "/usr/local/lib",
            "/usr/lib"
        };

        foreach (var basePath in searchPaths)
        {
            var fullPath = Path.Combine(basePath, extensionName);
            if (File.Exists(fullPath))
                return fullPath;
        }

        return null;
    }

    private static string GetRuntimeIdentifier()
    {
        var os = RuntimeInformation.IsOSPlatform(OSPlatform.Windows) ? "win"
            : RuntimeInformation.IsOSPlatform(OSPlatform.OSX) ? "osx"
            : "linux";

        var arch = RuntimeInformation.OSArchitecture switch
        {
            Architecture.X64 => "x64",
            Architecture.Arm64 => "arm64",
            Architecture.X86 => "x86",
            _ => "x64"
        };

        return $"{os}-{arch}";
    }

    private static string GetDefaultDatabasePath()
    {
        var appData = Environment.GetFolderPath(Environment.SpecialFolder.ApplicationData);
        return Path.Combine(appData, "SeniorIntern", "vectors.db");
    }

    private void EnsureInitialized()
    {
        if (!_isInitialized)
            throw new InvalidOperationException("Vector store not initialized. Call InitializeAsync first.");
    }

    public async ValueTask DisposeAsync()
    {
        if (_isDisposed) return;
        _isDisposed = true;

        _logger.LogInformation("Disposing vector store...");

        if (_connection != null)
        {
            await _connection.CloseAsync();
            await _connection.DisposeAsync();
            _connection = null;
        }

        _initLock.Dispose();
        _writeLock.Dispose();

        StateChanged?.Invoke(this, new VectorStoreStateChangedEventArgs
        {
            IsInitialized = false
        });
    }
}

/// <summary>
/// Event args for vector store state changes.
/// </summary>
public sealed class VectorStoreStateChangedEventArgs : EventArgs
{
    public bool IsInitialized { get; init; }
    public string? DatabasePath { get; init; }
    public string? Error { get; init; }
}
```

### File: `src/SeniorIntern.Services/VectorStore/SqliteVectorStore.IndexManagement.cs`

```csharp
using System;
using System.Collections.Generic;
using System.Text.Json;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.Data.Sqlite;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Services.VectorStore;

public sealed partial class SqliteVectorStore
{
    public async Task<VectorIndex> CreateIndexAsync(
        string workspacePath,
        string name,
        int embeddingDimension,
        VectorIndexSettings? settings = null,
        CancellationToken ct = default)
    {
        EnsureInitialized();

        var indexId = Guid.NewGuid().ToString();
        var sanitizedId = SanitizeTableName(indexId);
        settings ??= new VectorIndexSettings();

        _logger.LogInformation(
            "Creating index '{Name}' for workspace {Path} with dimension {Dim}",
            name, workspacePath, embeddingDimension);

        await _writeLock.WaitAsync(ct);
        try
        {
            await using var transaction = await _connection!.BeginTransactionAsync(ct);
            try
            {
                // Create the vector virtual table for this index
                var createVectorTable = VectorStoreSchema.CreateVectorTableTemplate
                    .Replace("{index_id}", sanitizedId)
                    .Replace("{dimension}", embeddingDimension.ToString());

                await using (var cmd = _connection.CreateCommand())
                {
                    cmd.Transaction = (SqliteTransaction)transaction;
                    cmd.CommandText = createVectorTable;
                    await cmd.ExecuteNonQueryAsync(ct);
                }

                // Insert index metadata
                await using (var cmd = _connection.CreateCommand())
                {
                    cmd.Transaction = (SqliteTransaction)transaction;
                    cmd.CommandText = """
                        INSERT INTO vector_indexes (
                            id, name, workspace_path, embedding_model,
                            embedding_dimension, settings_json
                        ) VALUES (
                            @id, @name, @workspace, @model, @dimension, @settings
                        )
                        """;

                    cmd.Parameters.AddWithValue("@id", indexId);
                    cmd.Parameters.AddWithValue("@name", name);
                    cmd.Parameters.AddWithValue("@workspace", workspacePath);
                    cmd.Parameters.AddWithValue("@model", "default");
                    cmd.Parameters.AddWithValue("@dimension", embeddingDimension);
                    cmd.Parameters.AddWithValue("@settings", JsonSerializer.Serialize(settings));

                    await cmd.ExecuteNonQueryAsync(ct);
                }

                await transaction.CommitAsync(ct);

                _logger.LogInformation("Created index {Id}", indexId);

                return new VectorIndex
                {
                    Id = indexId,
                    Name = name,
                    WorkspacePath = workspacePath,
                    EmbeddingModel = "default",
                    EmbeddingDimension = embeddingDimension,
                    Settings = settings
                };
            }
            catch
            {
                await transaction.RollbackAsync(ct);
                throw;
            }
        }
        finally
        {
            _writeLock.Release();
        }
    }

    public async Task<VectorIndex?> GetIndexAsync(string indexId, CancellationToken ct = default)
    {
        EnsureInitialized();

        await using var cmd = _connection!.CreateCommand();
        cmd.CommandText = """
            SELECT id, name, workspace_path, created_at, updated_at,
                   embedding_model, embedding_dimension, chunk_count, file_count,
                   total_file_size_bytes, status, settings_json, description,
                   last_error, last_full_index_at, last_incremental_update_at
            FROM vector_indexes
            WHERE id = @id
            """;
        cmd.Parameters.AddWithValue("@id", indexId);

        await using var reader = await cmd.ExecuteReaderAsync(ct);
        if (!await reader.ReadAsync(ct))
            return null;

        return ReadIndexFromReader(reader);
    }

    public async Task<VectorIndex?> GetIndexForWorkspaceAsync(
        string workspacePath,
        CancellationToken ct = default)
    {
        EnsureInitialized();

        await using var cmd = _connection!.CreateCommand();
        cmd.CommandText = """
            SELECT id, name, workspace_path, created_at, updated_at,
                   embedding_model, embedding_dimension, chunk_count, file_count,
                   total_file_size_bytes, status, settings_json, description,
                   last_error, last_full_index_at, last_incremental_update_at
            FROM vector_indexes
            WHERE workspace_path = @path AND status != 'Deleted'
            """;
        cmd.Parameters.AddWithValue("@path", workspacePath);

        await using var reader = await cmd.ExecuteReaderAsync(ct);
        if (!await reader.ReadAsync(ct))
            return null;

        return ReadIndexFromReader(reader);
    }

    public async Task<IReadOnlyList<VectorIndex>> ListIndexesAsync(CancellationToken ct = default)
    {
        EnsureInitialized();

        var indexes = new List<VectorIndex>();

        await using var cmd = _connection!.CreateCommand();
        cmd.CommandText = """
            SELECT id, name, workspace_path, created_at, updated_at,
                   embedding_model, embedding_dimension, chunk_count, file_count,
                   total_file_size_bytes, status, settings_json, description,
                   last_error, last_full_index_at, last_incremental_update_at
            FROM vector_indexes
            WHERE status != 'Deleted'
            ORDER BY updated_at DESC
            """;

        await using var reader = await cmd.ExecuteReaderAsync(ct);
        while (await reader.ReadAsync(ct))
        {
            indexes.Add(ReadIndexFromReader(reader));
        }

        return indexes;
    }

    public async Task UpdateIndexAsync(VectorIndex index, CancellationToken ct = default)
    {
        EnsureInitialized();

        await _writeLock.WaitAsync(ct);
        try
        {
            await using var cmd = _connection!.CreateCommand();
            cmd.CommandText = """
                UPDATE vector_indexes SET
                    name = @name,
                    updated_at = datetime('now'),
                    chunk_count = @chunkCount,
                    file_count = @fileCount,
                    total_file_size_bytes = @totalSize,
                    status = @status,
                    settings_json = @settings,
                    description = @description,
                    last_error = @lastError,
                    last_full_index_at = @lastFull,
                    last_incremental_update_at = @lastIncremental
                WHERE id = @id
                """;

            cmd.Parameters.AddWithValue("@id", index.Id);
            cmd.Parameters.AddWithValue("@name", index.Name);
            cmd.Parameters.AddWithValue("@chunkCount", index.ChunkCount);
            cmd.Parameters.AddWithValue("@fileCount", index.FileCount);
            cmd.Parameters.AddWithValue("@totalSize", index.TotalFileSizeBytes);
            cmd.Parameters.AddWithValue("@status", index.Status.ToString());
            cmd.Parameters.AddWithValue("@settings", JsonSerializer.Serialize(index.Settings));
            cmd.Parameters.AddWithValue("@description", (object?)index.Description ?? DBNull.Value);
            cmd.Parameters.AddWithValue("@lastError", (object?)index.LastError ?? DBNull.Value);
            cmd.Parameters.AddWithValue("@lastFull",
                index.LastFullIndexAt?.ToString("O") ?? (object)DBNull.Value);
            cmd.Parameters.AddWithValue("@lastIncremental",
                index.LastIncrementalUpdateAt?.ToString("O") ?? (object)DBNull.Value);

            await cmd.ExecuteNonQueryAsync(ct);
        }
        finally
        {
            _writeLock.Release();
        }
    }

    public async Task DeleteIndexAsync(string indexId, CancellationToken ct = default)
    {
        EnsureInitialized();

        _logger.LogInformation("Deleting index {Id}", indexId);

        await _writeLock.WaitAsync(ct);
        try
        {
            await using var transaction = await _connection!.BeginTransactionAsync(ct);
            try
            {
                // Drop the vector table
                var sanitizedId = SanitizeTableName(indexId);
                await using (var cmd = _connection.CreateCommand())
                {
                    cmd.Transaction = (SqliteTransaction)transaction;
                    cmd.CommandText = $"DROP TABLE IF EXISTS vectors_{sanitizedId}";
                    await cmd.ExecuteNonQueryAsync(ct);
                }

                // Delete metadata (cascade will handle chunks and files)
                await using (var cmd = _connection.CreateCommand())
                {
                    cmd.Transaction = (SqliteTransaction)transaction;
                    cmd.CommandText = "DELETE FROM vector_indexes WHERE id = @id";
                    cmd.Parameters.AddWithValue("@id", indexId);
                    await cmd.ExecuteNonQueryAsync(ct);
                }

                await transaction.CommitAsync(ct);
                _logger.LogInformation("Deleted index {Id}", indexId);
            }
            catch
            {
                await transaction.RollbackAsync(ct);
                throw;
            }
        }
        finally
        {
            _writeLock.Release();
        }
    }

    private static VectorIndex ReadIndexFromReader(SqliteDataReader reader)
    {
        var settingsJson = reader.GetString(reader.GetOrdinal("settings_json"));
        var settings = string.IsNullOrEmpty(settingsJson)
            ? new VectorIndexSettings()
            : JsonSerializer.Deserialize<VectorIndexSettings>(settingsJson) ?? new VectorIndexSettings();

        return new VectorIndex
        {
            Id = reader.GetString(reader.GetOrdinal("id")),
            Name = reader.GetString(reader.GetOrdinal("name")),
            WorkspacePath = reader.GetString(reader.GetOrdinal("workspace_path")),
            CreatedAt = DateTime.Parse(reader.GetString(reader.GetOrdinal("created_at"))),
            UpdatedAt = DateTime.Parse(reader.GetString(reader.GetOrdinal("updated_at"))),
            EmbeddingModel = reader.GetString(reader.GetOrdinal("embedding_model")),
            EmbeddingDimension = reader.GetInt32(reader.GetOrdinal("embedding_dimension")),
            ChunkCount = reader.GetInt32(reader.GetOrdinal("chunk_count")),
            FileCount = reader.GetInt32(reader.GetOrdinal("file_count")),
            TotalFileSizeBytes = reader.GetInt64(reader.GetOrdinal("total_file_size_bytes")),
            Status = Enum.Parse<IndexStatus>(reader.GetString(reader.GetOrdinal("status"))),
            Settings = settings,
            Description = reader.IsDBNull(reader.GetOrdinal("description"))
                ? null
                : reader.GetString(reader.GetOrdinal("description")),
            LastError = reader.IsDBNull(reader.GetOrdinal("last_error"))
                ? null
                : reader.GetString(reader.GetOrdinal("last_error")),
            LastFullIndexAt = reader.IsDBNull(reader.GetOrdinal("last_full_index_at"))
                ? null
                : DateTime.Parse(reader.GetString(reader.GetOrdinal("last_full_index_at"))),
            LastIncrementalUpdateAt = reader.IsDBNull(reader.GetOrdinal("last_incremental_update_at"))
                ? null
                : DateTime.Parse(reader.GetString(reader.GetOrdinal("last_incremental_update_at")))
        };
    }

    private static string SanitizeTableName(string input)
    {
        // Replace hyphens with underscores and remove any non-alphanumeric characters
        return new string(input.Replace("-", "_")
            .Where(c => char.IsLetterOrDigit(c) || c == '_')
            .ToArray());
    }
}
```

---

## v0.7.2f: Index Management Operations

### Objective
Implement chunk CRUD operations including adding, retrieving, and removing chunks.

### File: `src/SeniorIntern.Services/VectorStore/SqliteVectorStore.ChunkOperations.cs`

```csharp
using System;
using System.Buffers;
using System.Collections.Generic;
using System.Linq;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.Data.Sqlite;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Services.VectorStore;

public sealed partial class SqliteVectorStore
{
    public async Task AddChunksAsync(
        string indexId,
        IEnumerable<ChunkWithEmbedding> chunks,
        CancellationToken ct = default)
    {
        await AddChunksBatchAsync(indexId, chunks, null, ct);
    }

    public async Task AddChunksBatchAsync(
        string indexId,
        IEnumerable<ChunkWithEmbedding> chunks,
        IProgress<ChunkStorageProgress>? progress = null,
        CancellationToken ct = default)
    {
        EnsureInitialized();

        var chunkList = chunks.ToList();
        if (chunkList.Count == 0) return;

        var sanitizedId = SanitizeTableName(indexId);
        var batchSize = _options.BulkInsertBatchSize;
        var totalChunks = chunkList.Count;
        var processedCount = 0;

        _logger.LogDebug("Adding {Count} chunks to index {Id}", totalChunks, indexId);

        progress?.Report(new ChunkStorageProgress
        {
            Phase = ChunkStoragePhase.Preparing,
            TotalCount = totalChunks
        });

        await _writeLock.WaitAsync(ct);
        try
        {
            await using var transaction = await _connection!.BeginTransactionAsync(ct);
            try
            {
                // Process in batches
                foreach (var batch in chunkList.Chunk(batchSize))
                {
                    ct.ThrowIfCancellationRequested();

                    progress?.Report(new ChunkStorageProgress
                    {
                        Phase = ChunkStoragePhase.InsertingVectors,
                        StoredCount = processedCount,
                        TotalCount = totalChunks
                    });

                    // Insert vectors
                    foreach (var chunk in batch)
                    {
                        await InsertVectorAsync(
                            sanitizedId,
                            chunk.Chunk.Id.ToString(),
                            chunk.Embedding,
                            (SqliteTransaction)transaction,
                            ct);
                    }

                    progress?.Report(new ChunkStorageProgress
                    {
                        Phase = ChunkStoragePhase.InsertingMetadata,
                        StoredCount = processedCount,
                        TotalCount = totalChunks
                    });

                    // Insert metadata
                    foreach (var chunk in batch)
                    {
                        await InsertChunkMetadataAsync(
                            indexId,
                            chunk,
                            (SqliteTransaction)transaction,
                            ct);
                    }

                    processedCount += batch.Length;
                }

                progress?.Report(new ChunkStorageProgress
                {
                    Phase = ChunkStoragePhase.Committing,
                    StoredCount = processedCount,
                    TotalCount = totalChunks
                });

                // Update index chunk count
                await using (var cmd = _connection.CreateCommand())
                {
                    cmd.Transaction = (SqliteTransaction)transaction;
                    cmd.CommandText = """
                        UPDATE vector_indexes SET
                            chunk_count = chunk_count + @count,
                            updated_at = datetime('now')
                        WHERE id = @id
                        """;
                    cmd.Parameters.AddWithValue("@count", totalChunks);
                    cmd.Parameters.AddWithValue("@id", indexId);
                    await cmd.ExecuteNonQueryAsync(ct);
                }

                await transaction.CommitAsync(ct);

                progress?.Report(new ChunkStorageProgress
                {
                    Phase = ChunkStoragePhase.Complete,
                    StoredCount = totalChunks,
                    TotalCount = totalChunks
                });

                _logger.LogDebug("Added {Count} chunks to index {Id}", totalChunks, indexId);
            }
            catch
            {
                await transaction.RollbackAsync(ct);
                throw;
            }
        }
        finally
        {
            _writeLock.Release();
        }
    }

    private async Task InsertVectorAsync(
        string sanitizedIndexId,
        string chunkId,
        float[] embedding,
        SqliteTransaction transaction,
        CancellationToken ct)
    {
        await using var cmd = _connection!.CreateCommand();
        cmd.Transaction = transaction;
        cmd.CommandText = $"INSERT INTO vectors_{sanitizedIndexId} (id, embedding) VALUES (@id, @embedding)";
        cmd.Parameters.AddWithValue("@id", chunkId);
        cmd.Parameters.AddWithValue("@embedding", EmbeddingToBlob(embedding));
        await cmd.ExecuteNonQueryAsync(ct);
    }

    private async Task InsertChunkMetadataAsync(
        string indexId,
        ChunkWithEmbedding chunk,
        SqliteTransaction transaction,
        CancellationToken ct)
    {
        await using var cmd = _connection!.CreateCommand();
        cmd.Transaction = transaction;
        cmd.CommandText = """
            INSERT INTO chunk_metadata (
                id, index_id, file_id, content, start_line, end_line,
                start_offset, end_offset, chunk_type, language,
                symbol_name, symbol_type, parent_symbol, token_count
            ) VALUES (
                @id, @indexId, @fileId, @content, @startLine, @endLine,
                @startOffset, @endOffset, @chunkType, @language,
                @symbolName, @symbolType, @parentSymbol, @tokenCount
            )
            """;

        cmd.Parameters.AddWithValue("@id", chunk.Chunk.Id.ToString());
        cmd.Parameters.AddWithValue("@indexId", indexId);
        cmd.Parameters.AddWithValue("@fileId", chunk.FileId);
        cmd.Parameters.AddWithValue("@content", chunk.Chunk.Content);
        cmd.Parameters.AddWithValue("@startLine", chunk.Chunk.StartLine);
        cmd.Parameters.AddWithValue("@endLine", chunk.Chunk.EndLine);
        cmd.Parameters.AddWithValue("@startOffset", chunk.Chunk.StartOffset);
        cmd.Parameters.AddWithValue("@endOffset", chunk.Chunk.EndOffset);
        cmd.Parameters.AddWithValue("@chunkType", chunk.Chunk.Type.ToString());
        cmd.Parameters.AddWithValue("@language", (object?)chunk.Chunk.Language ?? DBNull.Value);
        cmd.Parameters.AddWithValue("@symbolName", (object?)chunk.Chunk.SymbolName ?? DBNull.Value);
        cmd.Parameters.AddWithValue("@symbolType",
            chunk.Chunk.SymbolType?.ToString() ?? (object)DBNull.Value);
        cmd.Parameters.AddWithValue("@parentSymbol", (object?)chunk.Chunk.ParentSymbol ?? DBNull.Value);
        cmd.Parameters.AddWithValue("@tokenCount", chunk.Chunk.TokenCount);

        await cmd.ExecuteNonQueryAsync(ct);
    }

    public async Task<StoredChunk?> GetChunkAsync(
        string chunkId,
        bool includeEmbedding = false,
        CancellationToken ct = default)
    {
        EnsureInitialized();

        await using var cmd = _connection!.CreateCommand();
        cmd.CommandText = """
            SELECT cm.id, cm.index_id, cm.file_id, cm.content, cm.start_line, cm.end_line,
                   cm.start_offset, cm.end_offset, cm.chunk_type, cm.language,
                   cm.symbol_name, cm.symbol_type, cm.parent_symbol, cm.token_count,
                   cm.indexed_at, if.file_path
            FROM chunk_metadata cm
            JOIN indexed_files if ON cm.file_id = if.id
            WHERE cm.id = @id
            """;
        cmd.Parameters.AddWithValue("@id", chunkId);

        await using var reader = await cmd.ExecuteReaderAsync(ct);
        if (!await reader.ReadAsync(ct))
            return null;

        var chunk = ReadStoredChunkFromReader(reader);

        if (includeEmbedding)
        {
            chunk = chunk with { Embedding = await GetEmbeddingAsync(chunk.IndexId, chunkId, ct) };
        }

        return chunk;
    }

    private async Task<float[]?> GetEmbeddingAsync(
        string indexId,
        string chunkId,
        CancellationToken ct)
    {
        var sanitizedId = SanitizeTableName(indexId);

        await using var cmd = _connection!.CreateCommand();
        cmd.CommandText = $"SELECT embedding FROM vectors_{sanitizedId} WHERE id = @id";
        cmd.Parameters.AddWithValue("@id", chunkId);

        var result = await cmd.ExecuteScalarAsync(ct);
        if (result is byte[] blob)
            return BlobToEmbedding(blob);

        return null;
    }

    public async Task<int> RemoveChunksForFileAsync(
        string indexId,
        string filePath,
        CancellationToken ct = default)
    {
        return await RemoveChunksForFilesAsync(indexId, [filePath], ct);
    }

    public async Task<int> RemoveChunksForFilesAsync(
        string indexId,
        IEnumerable<string> filePaths,
        CancellationToken ct = default)
    {
        EnsureInitialized();

        var filePathList = filePaths.ToList();
        if (filePathList.Count == 0) return 0;

        var sanitizedId = SanitizeTableName(indexId);
        var totalRemoved = 0;

        await _writeLock.WaitAsync(ct);
        try
        {
            await using var transaction = await _connection!.BeginTransactionAsync(ct);
            try
            {
                foreach (var filePath in filePathList)
                {
                    // Get file ID and chunk IDs
                    var chunkIds = new List<string>();

                    await using (var cmd = _connection.CreateCommand())
                    {
                        cmd.Transaction = (SqliteTransaction)transaction;
                        cmd.CommandText = """
                            SELECT cm.id FROM chunk_metadata cm
                            JOIN indexed_files if ON cm.file_id = if.id
                            WHERE if.index_id = @indexId AND if.file_path = @filePath
                            """;
                        cmd.Parameters.AddWithValue("@indexId", indexId);
                        cmd.Parameters.AddWithValue("@filePath", filePath);

                        await using var reader = await cmd.ExecuteReaderAsync(ct);
                        while (await reader.ReadAsync(ct))
                        {
                            chunkIds.Add(reader.GetString(0));
                        }
                    }

                    // Remove vectors
                    foreach (var chunkId in chunkIds)
                    {
                        await using var cmd = _connection.CreateCommand();
                        cmd.Transaction = (SqliteTransaction)transaction;
                        cmd.CommandText = $"DELETE FROM vectors_{sanitizedId} WHERE id = @id";
                        cmd.Parameters.AddWithValue("@id", chunkId);
                        await cmd.ExecuteNonQueryAsync(ct);
                    }

                    // Remove metadata (file deletion will cascade to chunks)
                    await using (var cmd = _connection.CreateCommand())
                    {
                        cmd.Transaction = (SqliteTransaction)transaction;
                        cmd.CommandText = """
                            DELETE FROM indexed_files
                            WHERE index_id = @indexId AND file_path = @filePath
                            """;
                        cmd.Parameters.AddWithValue("@indexId", indexId);
                        cmd.Parameters.AddWithValue("@filePath", filePath);
                        await cmd.ExecuteNonQueryAsync(ct);
                    }

                    totalRemoved += chunkIds.Count;
                }

                // Update index counts
                await using (var cmd = _connection.CreateCommand())
                {
                    cmd.Transaction = (SqliteTransaction)transaction;
                    cmd.CommandText = """
                        UPDATE vector_indexes SET
                            chunk_count = chunk_count - @count,
                            file_count = file_count - @fileCount,
                            updated_at = datetime('now')
                        WHERE id = @id
                        """;
                    cmd.Parameters.AddWithValue("@count", totalRemoved);
                    cmd.Parameters.AddWithValue("@fileCount", filePathList.Count);
                    cmd.Parameters.AddWithValue("@id", indexId);
                    await cmd.ExecuteNonQueryAsync(ct);
                }

                await transaction.CommitAsync(ct);
            }
            catch
            {
                await transaction.RollbackAsync(ct);
                throw;
            }
        }
        finally
        {
            _writeLock.Release();
        }

        _logger.LogDebug("Removed {Count} chunks for {FileCount} files", totalRemoved, filePathList.Count);
        return totalRemoved;
    }

    public async Task ClearIndexAsync(string indexId, CancellationToken ct = default)
    {
        EnsureInitialized();

        var sanitizedId = SanitizeTableName(indexId);

        _logger.LogInformation("Clearing all chunks from index {Id}", indexId);

        await _writeLock.WaitAsync(ct);
        try
        {
            await using var transaction = await _connection!.BeginTransactionAsync(ct);
            try
            {
                // Clear vectors
                await using (var cmd = _connection.CreateCommand())
                {
                    cmd.Transaction = (SqliteTransaction)transaction;
                    cmd.CommandText = $"DELETE FROM vectors_{sanitizedId}";
                    await cmd.ExecuteNonQueryAsync(ct);
                }

                // Clear metadata
                await using (var cmd = _connection.CreateCommand())
                {
                    cmd.Transaction = (SqliteTransaction)transaction;
                    cmd.CommandText = "DELETE FROM chunk_metadata WHERE index_id = @id";
                    cmd.Parameters.AddWithValue("@id", indexId);
                    await cmd.ExecuteNonQueryAsync(ct);
                }

                // Clear files
                await using (var cmd = _connection.CreateCommand())
                {
                    cmd.Transaction = (SqliteTransaction)transaction;
                    cmd.CommandText = "DELETE FROM indexed_files WHERE index_id = @id";
                    cmd.Parameters.AddWithValue("@id", indexId);
                    await cmd.ExecuteNonQueryAsync(ct);
                }

                // Update counts
                await using (var cmd = _connection.CreateCommand())
                {
                    cmd.Transaction = (SqliteTransaction)transaction;
                    cmd.CommandText = """
                        UPDATE vector_indexes SET
                            chunk_count = 0,
                            file_count = 0,
                            total_file_size_bytes = 0,
                            updated_at = datetime('now')
                        WHERE id = @id
                        """;
                    cmd.Parameters.AddWithValue("@id", indexId);
                    await cmd.ExecuteNonQueryAsync(ct);
                }

                await transaction.CommitAsync(ct);
            }
            catch
            {
                await transaction.RollbackAsync(ct);
                throw;
            }
        }
        finally
        {
            _writeLock.Release();
        }
    }

    private static StoredChunk ReadStoredChunkFromReader(SqliteDataReader reader)
    {
        return new StoredChunk
        {
            Id = reader.GetString(reader.GetOrdinal("id")),
            IndexId = reader.GetString(reader.GetOrdinal("index_id")),
            FileId = reader.GetString(reader.GetOrdinal("file_id")),
            Content = reader.GetString(reader.GetOrdinal("content")),
            FilePath = reader.GetString(reader.GetOrdinal("file_path")),
            StartLine = reader.GetInt32(reader.GetOrdinal("start_line")),
            EndLine = reader.GetInt32(reader.GetOrdinal("end_line")),
            StartOffset = reader.GetInt32(reader.GetOrdinal("start_offset")),
            EndOffset = reader.GetInt32(reader.GetOrdinal("end_offset")),
            ChunkType = Enum.Parse<ChunkType>(reader.GetString(reader.GetOrdinal("chunk_type"))),
            Language = reader.IsDBNull(reader.GetOrdinal("language"))
                ? null
                : reader.GetString(reader.GetOrdinal("language")),
            SymbolName = reader.IsDBNull(reader.GetOrdinal("symbol_name"))
                ? null
                : reader.GetString(reader.GetOrdinal("symbol_name")),
            SymbolType = reader.IsDBNull(reader.GetOrdinal("symbol_type"))
                ? null
                : Enum.Parse<SymbolType>(reader.GetString(reader.GetOrdinal("symbol_type"))),
            ParentSymbol = reader.IsDBNull(reader.GetOrdinal("parent_symbol"))
                ? null
                : reader.GetString(reader.GetOrdinal("parent_symbol")),
            TokenCount = reader.GetInt32(reader.GetOrdinal("token_count")),
            IndexedAt = DateTime.Parse(reader.GetString(reader.GetOrdinal("indexed_at")))
        };
    }

    private static byte[] EmbeddingToBlob(float[] embedding)
    {
        var bytes = new byte[embedding.Length * sizeof(float)];
        Buffer.BlockCopy(embedding, 0, bytes, 0, bytes.Length);
        return bytes;
    }

    private static float[] BlobToEmbedding(byte[] blob)
    {
        var embedding = new float[blob.Length / sizeof(float)];
        Buffer.BlockCopy(blob, 0, embedding, 0, blob.Length);
        return embedding;
    }
}
```

### File: `src/SeniorIntern.Services/VectorStore/SqliteVectorStore.FileTracking.cs`

```csharp
using System;
using System.Collections.Generic;
using System.Linq;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.Data.Sqlite;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Services.VectorStore;

public sealed partial class SqliteVectorStore
{
    public async Task<IndexedFile?> GetIndexedFileAsync(
        string indexId,
        string filePath,
        CancellationToken ct = default)
    {
        EnsureInitialized();

        await using var cmd = _connection!.CreateCommand();
        cmd.CommandText = """
            SELECT id, index_id, file_path, file_hash, file_size, last_modified,
                   indexed_at, chunk_count, language, encoding, line_count,
                   status, error_message, indexing_duration_ms
            FROM indexed_files
            WHERE index_id = @indexId AND file_path = @filePath
            """;
        cmd.Parameters.AddWithValue("@indexId", indexId);
        cmd.Parameters.AddWithValue("@filePath", filePath);

        await using var reader = await cmd.ExecuteReaderAsync(ct);
        if (!await reader.ReadAsync(ct))
            return null;

        return ReadIndexedFileFromReader(reader);
    }

    public async Task<IReadOnlyList<IndexedFile>> GetIndexedFilesAsync(
        string indexId,
        CancellationToken ct = default)
    {
        EnsureInitialized();

        var files = new List<IndexedFile>();

        await using var cmd = _connection!.CreateCommand();
        cmd.CommandText = """
            SELECT id, index_id, file_path, file_hash, file_size, last_modified,
                   indexed_at, chunk_count, language, encoding, line_count,
                   status, error_message, indexing_duration_ms
            FROM indexed_files
            WHERE index_id = @indexId
            ORDER BY file_path
            """;
        cmd.Parameters.AddWithValue("@indexId", indexId);

        await using var reader = await cmd.ExecuteReaderAsync(ct);
        while (await reader.ReadAsync(ct))
        {
            files.Add(ReadIndexedFileFromReader(reader));
        }

        return files;
    }

    public async Task UpsertIndexedFileAsync(
        string indexId,
        IndexedFile file,
        CancellationToken ct = default)
    {
        EnsureInitialized();

        await _writeLock.WaitAsync(ct);
        try
        {
            await using var cmd = _connection!.CreateCommand();
            cmd.CommandText = """
                INSERT INTO indexed_files (
                    id, index_id, file_path, file_hash, file_size, last_modified,
                    indexed_at, chunk_count, language, encoding, line_count,
                    status, error_message, indexing_duration_ms
                ) VALUES (
                    @id, @indexId, @filePath, @fileHash, @fileSize, @lastModified,
                    @indexedAt, @chunkCount, @language, @encoding, @lineCount,
                    @status, @errorMessage, @durationMs
                )
                ON CONFLICT(index_id, file_path) DO UPDATE SET
                    file_hash = @fileHash,
                    file_size = @fileSize,
                    last_modified = @lastModified,
                    indexed_at = @indexedAt,
                    chunk_count = @chunkCount,
                    language = @language,
                    encoding = @encoding,
                    line_count = @lineCount,
                    status = @status,
                    error_message = @errorMessage,
                    indexing_duration_ms = @durationMs
                """;

            cmd.Parameters.AddWithValue("@id", file.Id);
            cmd.Parameters.AddWithValue("@indexId", indexId);
            cmd.Parameters.AddWithValue("@filePath", file.FilePath);
            cmd.Parameters.AddWithValue("@fileHash", file.FileHash);
            cmd.Parameters.AddWithValue("@fileSize", file.FileSize);
            cmd.Parameters.AddWithValue("@lastModified", file.LastModified.ToString("O"));
            cmd.Parameters.AddWithValue("@indexedAt", file.IndexedAt.ToString("O"));
            cmd.Parameters.AddWithValue("@chunkCount", file.ChunkCount);
            cmd.Parameters.AddWithValue("@language", (object?)file.Language ?? DBNull.Value);
            cmd.Parameters.AddWithValue("@encoding", (object?)file.Encoding ?? DBNull.Value);
            cmd.Parameters.AddWithValue("@lineCount", file.LineCount);
            cmd.Parameters.AddWithValue("@status", file.Status.ToString());
            cmd.Parameters.AddWithValue("@errorMessage", (object?)file.ErrorMessage ?? DBNull.Value);
            cmd.Parameters.AddWithValue("@durationMs", file.IndexingDurationMs);

            await cmd.ExecuteNonQueryAsync(ct);
        }
        finally
        {
            _writeLock.Release();
        }
    }

    public async Task RemoveIndexedFileAsync(
        string indexId,
        string filePath,
        CancellationToken ct = default)
    {
        EnsureInitialized();

        await _writeLock.WaitAsync(ct);
        try
        {
            await using var cmd = _connection!.CreateCommand();
            cmd.CommandText = """
                DELETE FROM indexed_files
                WHERE index_id = @indexId AND file_path = @filePath
                """;
            cmd.Parameters.AddWithValue("@indexId", indexId);
            cmd.Parameters.AddWithValue("@filePath", filePath);
            await cmd.ExecuteNonQueryAsync(ct);
        }
        finally
        {
            _writeLock.Release();
        }
    }

    public async Task<FileChangeSet> DetectFileChangesAsync(
        string indexId,
        IReadOnlyDictionary<string, string> fileHashes,
        CancellationToken ct = default)
    {
        EnsureInitialized();

        var existingFiles = await GetIndexedFilesAsync(indexId, ct);
        var existingByPath = existingFiles.ToDictionary(f => f.FilePath, f => f.FileHash);

        var added = new List<string>();
        var modified = new List<string>();
        var unchanged = new List<string>();

        foreach (var (path, hash) in fileHashes)
        {
            if (!existingByPath.TryGetValue(path, out var existingHash))
            {
                added.Add(path);
            }
            else if (existingHash != hash)
            {
                modified.Add(path);
            }
            else
            {
                unchanged.Add(path);
            }
        }

        var currentPaths = fileHashes.Keys.ToHashSet();
        var deleted = existingByPath.Keys
            .Where(p => !currentPaths.Contains(p))
            .ToList();

        return new FileChangeSet
        {
            AddedFiles = added,
            ModifiedFiles = modified,
            DeletedFiles = deleted,
            UnchangedFiles = unchanged
        };
    }

    private static IndexedFile ReadIndexedFileFromReader(SqliteDataReader reader)
    {
        return new IndexedFile
        {
            Id = reader.GetString(reader.GetOrdinal("id")),
            IndexId = reader.GetString(reader.GetOrdinal("index_id")),
            FilePath = reader.GetString(reader.GetOrdinal("file_path")),
            FileHash = reader.GetString(reader.GetOrdinal("file_hash")),
            FileSize = reader.GetInt64(reader.GetOrdinal("file_size")),
            LastModified = DateTime.Parse(reader.GetString(reader.GetOrdinal("last_modified"))),
            IndexedAt = DateTime.Parse(reader.GetString(reader.GetOrdinal("indexed_at"))),
            ChunkCount = reader.GetInt32(reader.GetOrdinal("chunk_count")),
            Language = reader.IsDBNull(reader.GetOrdinal("language"))
                ? null
                : reader.GetString(reader.GetOrdinal("language")),
            Encoding = reader.IsDBNull(reader.GetOrdinal("encoding"))
                ? null
                : reader.GetString(reader.GetOrdinal("encoding")),
            LineCount = reader.GetInt32(reader.GetOrdinal("line_count")),
            Status = Enum.Parse<FileIndexStatus>(reader.GetString(reader.GetOrdinal("status"))),
            ErrorMessage = reader.IsDBNull(reader.GetOrdinal("error_message"))
                ? null
                : reader.GetString(reader.GetOrdinal("error_message")),
            IndexingDurationMs = reader.GetInt32(reader.GetOrdinal("indexing_duration_ms"))
        };
    }
}
```

---

## v0.7.2g: Vector Search Implementation

### Objective
Implement vector similarity search using sqlite-vec with support for filtering and deduplication.

### File: `src/SeniorIntern.Services/VectorStore/SqliteVectorStore.Search.cs`

```csharp
using System;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Text;
using System.Text.RegularExpressions;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.Data.Sqlite;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Services.VectorStore;

public sealed partial class SqliteVectorStore
{
    public async Task<IReadOnlyList<ChunkSearchResult>> SearchAsync(
        string indexId,
        float[] queryEmbedding,
        VectorSearchOptions options,
        CancellationToken ct = default)
    {
        EnsureInitialized();

        var sanitizedId = SanitizeTableName(indexId);
        var embeddingBlob = EmbeddingToBlob(queryEmbedding);

        // Over-fetch to allow for post-filtering and deduplication
        var fetchCount = options.TopK * 3;

        _logger.LogDebug(
            "Searching index {Id} with TopK={TopK}, MinScore={MinScore}",
            indexId, options.TopK, options.MinScore);

        var results = new List<ChunkSearchResult>();

        // Build the search query
        var sql = BuildSearchQuery(sanitizedId, options);

        await using var cmd = _connection!.CreateCommand();
        cmd.CommandText = sql;
        cmd.Parameters.AddWithValue("@embedding", embeddingBlob);
        cmd.Parameters.AddWithValue("@topK", fetchCount);

        // Add filter parameters
        AddFilterParameters(cmd, options);

        await using var reader = await cmd.ExecuteReaderAsync(ct);
        var rank = 0;

        while (await reader.ReadAsync(ct))
        {
            var distance = reader.GetFloat(reader.GetOrdinal("distance"));
            var score = DistanceToScore(distance);

            if (score < options.MinScore)
                continue;

            rank++;
            var chunk = ReadStoredChunkFromSearchReader(reader);

            var result = new ChunkSearchResult
            {
                ChunkId = chunk.Id,
                IndexId = indexId,
                Score = score,
                Distance = distance,
                Chunk = chunk,
                Rank = rank
            };

            // Expand context if requested
            if (options.ExpandContext)
            {
                result = await ExpandContextAsync(result, options, ct);
            }

            results.Add(result);
        }

        // Apply deduplication if requested
        if (options.DeduplicateOverlapping)
        {
            results = DeduplicateResults(results, options.DeduplicationThreshold);
        }

        // Apply file pattern filters (post-query for glob support)
        if (options.FilePatterns?.Count > 0)
        {
            results = FilterByFilePatterns(results, options.FilePatterns);
        }

        // Return top K results
        return results.Take(options.TopK).ToList();
    }

    public async Task<IReadOnlyList<ChunkSearchResult>> SearchMultipleAsync(
        IEnumerable<string> indexIds,
        float[] queryEmbedding,
        VectorSearchOptions options,
        CancellationToken ct = default)
    {
        var allResults = new List<ChunkSearchResult>();

        foreach (var indexId in indexIds)
        {
            var indexResults = await SearchAsync(indexId, queryEmbedding, options, ct);
            allResults.AddRange(indexResults);
        }

        // Re-sort and deduplicate across indexes
        allResults = allResults
            .OrderByDescending(r => r.Score)
            .ToList();

        if (options.DeduplicateOverlapping)
        {
            allResults = DeduplicateResults(allResults, options.DeduplicationThreshold);
        }

        // Re-rank
        var rank = 0;
        foreach (var result in allResults)
        {
            // Note: ChunkSearchResult is a record, so we need to create new instances
            // This is a simplification - in production you might want mutable Rank
        }

        return allResults.Take(options.TopK).ToList();
    }

    private string BuildSearchQuery(string sanitizedIndexId, VectorSearchOptions options)
    {
        var sql = new StringBuilder();

        sql.AppendLine($"""
            SELECT
                v.id,
                v.distance,
                cm.index_id,
                cm.file_id,
                cm.content,
                cm.start_line,
                cm.end_line,
                cm.start_offset,
                cm.end_offset,
                cm.chunk_type,
                cm.language,
                cm.symbol_name,
                cm.symbol_type,
                cm.parent_symbol,
                cm.token_count,
                cm.indexed_at,
                if.file_path
            FROM vectors_{sanitizedIndexId} v
            JOIN chunk_metadata cm ON v.id = cm.id
            JOIN indexed_files if ON cm.file_id = if.id
            WHERE v.embedding MATCH @embedding
              AND k = @topK
            """);

        // Add SQL-level filters
        if (options.Languages?.Count > 0)
        {
            var placeholders = string.Join(", ", options.Languages.Select((_, i) => $"@lang{i}"));
            sql.AppendLine($"  AND cm.language IN ({placeholders})");
        }

        if (options.ChunkTypes?.Count > 0)
        {
            var placeholders = string.Join(", ", options.ChunkTypes.Select((_, i) => $"@chunkType{i}"));
            sql.AppendLine($"  AND cm.chunk_type IN ({placeholders})");
        }

        if (options.SymbolTypes?.Count > 0)
        {
            var placeholders = string.Join(", ", options.SymbolTypes.Select((_, i) => $"@symbolType{i}"));
            sql.AppendLine($"  AND cm.symbol_type IN ({placeholders})");
        }

        if (options.SymbolNames?.Count > 0)
        {
            var placeholders = string.Join(", ", options.SymbolNames.Select((_, i) => $"@symbolName{i}"));
            sql.AppendLine($"  AND cm.symbol_name IN ({placeholders})");
        }

        sql.AppendLine("ORDER BY v.distance ASC");

        return sql.ToString();
    }

    private static void AddFilterParameters(SqliteCommand cmd, VectorSearchOptions options)
    {
        if (options.Languages?.Count > 0)
        {
            for (int i = 0; i < options.Languages.Count; i++)
            {
                cmd.Parameters.AddWithValue($"@lang{i}", options.Languages[i]);
            }
        }

        if (options.ChunkTypes?.Count > 0)
        {
            for (int i = 0; i < options.ChunkTypes.Count; i++)
            {
                cmd.Parameters.AddWithValue($"@chunkType{i}", options.ChunkTypes[i].ToString());
            }
        }

        if (options.SymbolTypes?.Count > 0)
        {
            for (int i = 0; i < options.SymbolTypes.Count; i++)
            {
                cmd.Parameters.AddWithValue($"@symbolType{i}", options.SymbolTypes[i].ToString());
            }
        }

        if (options.SymbolNames?.Count > 0)
        {
            for (int i = 0; i < options.SymbolNames.Count; i++)
            {
                cmd.Parameters.AddWithValue($"@symbolName{i}", options.SymbolNames[i]);
            }
        }
    }

    private async Task<ChunkSearchResult> ExpandContextAsync(
        ChunkSearchResult result,
        VectorSearchOptions options,
        CancellationToken ct)
    {
        // Get the full file content to expand context
        // This is simplified - in production, you might cache file contents
        // or store them in the database

        var expandedStartLine = Math.Max(1, result.Chunk.StartLine - options.ContextLinesAbove);
        var expandedEndLine = result.Chunk.EndLine + options.ContextLinesBelow;

        // For now, return the result as-is with expanded line numbers
        // Full implementation would read the file and extract context
        return result with
        {
            ExpandedStartLine = expandedStartLine,
            ExpandedEndLine = expandedEndLine
        };
    }

    private static List<ChunkSearchResult> DeduplicateResults(
        List<ChunkSearchResult> results,
        float threshold)
    {
        var deduplicated = new List<ChunkSearchResult>();

        foreach (var result in results.OrderByDescending(r => r.Score))
        {
            var overlaps = deduplicated.Any(existing =>
                existing.Chunk.FilePath == result.Chunk.FilePath &&
                RangesOverlap(
                    existing.Chunk.StartLine, existing.Chunk.EndLine,
                    result.Chunk.StartLine, result.Chunk.EndLine,
                    threshold));

            if (!overlaps)
            {
                deduplicated.Add(result);
            }
        }

        return deduplicated;
    }

    private static bool RangesOverlap(
        int start1, int end1,
        int start2, int end2,
        float threshold)
    {
        var overlapStart = Math.Max(start1, start2);
        var overlapEnd = Math.Min(end1, end2);
        var overlap = Math.Max(0, overlapEnd - overlapStart + 1);
        var smallerRange = Math.Min(end1 - start1 + 1, end2 - start2 + 1);

        return smallerRange > 0 && (float)overlap / smallerRange > threshold;
    }

    private static List<ChunkSearchResult> FilterByFilePatterns(
        List<ChunkSearchResult> results,
        IReadOnlyList<string> patterns)
    {
        return results.Where(r => patterns.Any(p => MatchGlob(r.Chunk.FilePath, p))).ToList();
    }

    private static bool MatchGlob(string path, string pattern)
    {
        // Simple glob matching - convert glob to regex
        var regexPattern = "^" + Regex.Escape(pattern)
            .Replace(@"\*\*", ".*")
            .Replace(@"\*", "[^/]*")
            .Replace(@"\?", ".") + "$";

        return Regex.IsMatch(path, regexPattern, RegexOptions.IgnoreCase);
    }

    private static float DistanceToScore(float distance)
    {
        // Convert L2 distance to similarity score (0-1)
        // Using exponential decay: score = e^(-distance)
        return MathF.Exp(-distance);
    }

    private static StoredChunk ReadStoredChunkFromSearchReader(SqliteDataReader reader)
    {
        return new StoredChunk
        {
            Id = reader.GetString(reader.GetOrdinal("id")),
            IndexId = reader.GetString(reader.GetOrdinal("index_id")),
            FileId = reader.GetString(reader.GetOrdinal("file_id")),
            Content = reader.GetString(reader.GetOrdinal("content")),
            FilePath = reader.GetString(reader.GetOrdinal("file_path")),
            StartLine = reader.GetInt32(reader.GetOrdinal("start_line")),
            EndLine = reader.GetInt32(reader.GetOrdinal("end_line")),
            StartOffset = reader.GetInt32(reader.GetOrdinal("start_offset")),
            EndOffset = reader.GetInt32(reader.GetOrdinal("end_offset")),
            ChunkType = Enum.Parse<ChunkType>(reader.GetString(reader.GetOrdinal("chunk_type"))),
            Language = reader.IsDBNull(reader.GetOrdinal("language"))
                ? null
                : reader.GetString(reader.GetOrdinal("language")),
            SymbolName = reader.IsDBNull(reader.GetOrdinal("symbol_name"))
                ? null
                : reader.GetString(reader.GetOrdinal("symbol_name")),
            SymbolType = reader.IsDBNull(reader.GetOrdinal("symbol_type"))
                ? null
                : Enum.Parse<SymbolType>(reader.GetString(reader.GetOrdinal("symbol_type"))),
            ParentSymbol = reader.IsDBNull(reader.GetOrdinal("parent_symbol"))
                ? null
                : reader.GetString(reader.GetOrdinal("parent_symbol")),
            TokenCount = reader.GetInt32(reader.GetOrdinal("token_count")),
            IndexedAt = DateTime.Parse(reader.GetString(reader.GetOrdinal("indexed_at")))
        };
    }
}
```

### File: `src/SeniorIntern.Services/VectorStore/VectorSearchHelper.cs`

```csharp
using System;
using System.Collections.Generic;
using System.Linq;
using System.Numerics;
using System.Runtime.InteropServices;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Services.VectorStore;

/// <summary>
/// Helper methods for vector search operations.
/// </summary>
public static class VectorSearchHelper
{
    /// <summary>
    /// Calculate cosine similarity between two vectors using SIMD.
    /// </summary>
    public static float CosineSimilarity(ReadOnlySpan<float> a, ReadOnlySpan<float> b)
    {
        if (a.Length != b.Length)
            throw new ArgumentException("Vectors must have the same dimension");

        if (a.Length == 0)
            return 0;

        float dotProduct = 0;
        float normA = 0;
        float normB = 0;

        // Use SIMD if available
        if (Vector.IsHardwareAccelerated && a.Length >= Vector<float>.Count)
        {
            var dotVec = Vector<float>.Zero;
            var normAVec = Vector<float>.Zero;
            var normBVec = Vector<float>.Zero;

            int i = 0;
            int lastBlockIndex = a.Length - (a.Length % Vector<float>.Count);

            for (; i < lastBlockIndex; i += Vector<float>.Count)
            {
                var va = new Vector<float>(a.Slice(i, Vector<float>.Count));
                var vb = new Vector<float>(b.Slice(i, Vector<float>.Count));

                dotVec += va * vb;
                normAVec += va * va;
                normBVec += vb * vb;
            }

            dotProduct = Vector.Sum(dotVec);
            normA = Vector.Sum(normAVec);
            normB = Vector.Sum(normBVec);

            // Handle remaining elements
            for (; i < a.Length; i++)
            {
                dotProduct += a[i] * b[i];
                normA += a[i] * a[i];
                normB += b[i] * b[i];
            }
        }
        else
        {
            for (int i = 0; i < a.Length; i++)
            {
                dotProduct += a[i] * b[i];
                normA += a[i] * a[i];
                normB += b[i] * b[i];
            }
        }

        var denominator = MathF.Sqrt(normA) * MathF.Sqrt(normB);
        return denominator > 0 ? dotProduct / denominator : 0;
    }

    /// <summary>
    /// Normalize a vector to unit length in-place.
    /// </summary>
    public static void Normalize(Span<float> vector)
    {
        float norm = 0;

        if (Vector.IsHardwareAccelerated && vector.Length >= Vector<float>.Count)
        {
            var normVec = Vector<float>.Zero;
            int i = 0;
            int lastBlockIndex = vector.Length - (vector.Length % Vector<float>.Count);

            for (; i < lastBlockIndex; i += Vector<float>.Count)
            {
                var v = new Vector<float>(vector.Slice(i, Vector<float>.Count));
                normVec += v * v;
            }

            norm = Vector.Sum(normVec);

            for (; i < vector.Length; i++)
            {
                norm += vector[i] * vector[i];
            }
        }
        else
        {
            foreach (var v in vector)
            {
                norm += v * v;
            }
        }

        norm = MathF.Sqrt(norm);

        if (norm > 0)
        {
            var invNorm = 1.0f / norm;
            for (int i = 0; i < vector.Length; i++)
            {
                vector[i] *= invNorm;
            }
        }
    }

    /// <summary>
    /// Calculate L2 (Euclidean) distance between two vectors.
    /// </summary>
    public static float L2Distance(ReadOnlySpan<float> a, ReadOnlySpan<float> b)
    {
        if (a.Length != b.Length)
            throw new ArgumentException("Vectors must have the same dimension");

        float sum = 0;

        if (Vector.IsHardwareAccelerated && a.Length >= Vector<float>.Count)
        {
            var sumVec = Vector<float>.Zero;
            int i = 0;
            int lastBlockIndex = a.Length - (a.Length % Vector<float>.Count);

            for (; i < lastBlockIndex; i += Vector<float>.Count)
            {
                var va = new Vector<float>(a.Slice(i, Vector<float>.Count));
                var vb = new Vector<float>(b.Slice(i, Vector<float>.Count));
                var diff = va - vb;
                sumVec += diff * diff;
            }

            sum = Vector.Sum(sumVec);

            for (; i < a.Length; i++)
            {
                var diff = a[i] - b[i];
                sum += diff * diff;
            }
        }
        else
        {
            for (int i = 0; i < a.Length; i++)
            {
                var diff = a[i] - b[i];
                sum += diff * diff;
            }
        }

        return MathF.Sqrt(sum);
    }

    /// <summary>
    /// Rank search results by combining multiple scoring factors.
    /// </summary>
    public static IReadOnlyList<ChunkSearchResult> RerankResults(
        IEnumerable<ChunkSearchResult> results,
        RerankingOptions options)
    {
        return results
            .Select(r => r with
            {
                Score = CalculateCombinedScore(r, options)
            })
            .OrderByDescending(r => r.Score)
            .ToList();
    }

    private static float CalculateCombinedScore(ChunkSearchResult result, RerankingOptions options)
    {
        var score = result.Score * options.SimilarityWeight;

        // Boost for symbol matches
        if (!string.IsNullOrEmpty(result.Chunk.SymbolName))
        {
            score += options.SymbolBoost;
        }

        // Boost for specific chunk types
        if (options.PreferredChunkTypes?.Contains(result.Chunk.ChunkType) == true)
        {
            score += options.ChunkTypeBoost;
        }

        // Recency boost (more recent files score higher)
        if (options.RecencyWeight > 0)
        {
            var age = DateTime.UtcNow - result.Chunk.IndexedAt;
            var recencyScore = MathF.Exp(-(float)age.TotalDays / 30); // Decay over 30 days
            score += recencyScore * options.RecencyWeight;
        }

        return score;
    }
}

/// <summary>
/// Options for result reranking.
/// </summary>
public sealed class RerankingOptions
{
    /// <summary>
    /// Weight for vector similarity score.
    /// </summary>
    public float SimilarityWeight { get; init; } = 1.0f;

    /// <summary>
    /// Boost for chunks with symbol names.
    /// </summary>
    public float SymbolBoost { get; init; } = 0.1f;

    /// <summary>
    /// Boost for preferred chunk types.
    /// </summary>
    public float ChunkTypeBoost { get; init; } = 0.05f;

    /// <summary>
    /// Weight for recency scoring.
    /// </summary>
    public float RecencyWeight { get; init; } = 0.0f;

    /// <summary>
    /// Chunk types to prefer in results.
    /// </summary>
    public IReadOnlyList<ChunkType>? PreferredChunkTypes { get; init; }
}
```

---

## v0.7.2h: File Tracking & Change Detection

### Objective
Implement utilities for tracking file changes and computing content hashes for incremental indexing.

### File: `src/SeniorIntern.Services/VectorStore/FileHasher.cs`

```csharp
using System;
using System.Buffers;
using System.IO;
using System.Security.Cryptography;
using System.Text;
using System.Threading;
using System.Threading.Tasks;

namespace SeniorIntern.Services.VectorStore;

/// <summary>
/// Utility for computing file content hashes.
/// </summary>
public static class FileHasher
{
    private const int BufferSize = 81920; // 80KB buffer

    /// <summary>
    /// Compute SHA-256 hash of file contents.
    /// </summary>
    public static async Task<string> ComputeHashAsync(
        string filePath,
        CancellationToken ct = default)
    {
        await using var stream = new FileStream(
            filePath,
            FileMode.Open,
            FileAccess.Read,
            FileShare.Read,
            bufferSize: BufferSize,
            useAsync: true);

        return await ComputeHashAsync(stream, ct);
    }

    /// <summary>
    /// Compute SHA-256 hash of stream contents.
    /// </summary>
    public static async Task<string> ComputeHashAsync(
        Stream stream,
        CancellationToken ct = default)
    {
        using var sha256 = SHA256.Create();
        var hashBytes = await sha256.ComputeHashAsync(stream, ct);
        return Convert.ToHexString(hashBytes).ToLowerInvariant();
    }

    /// <summary>
    /// Compute SHA-256 hash of string content.
    /// </summary>
    public static string ComputeHash(string content)
    {
        var bytes = Encoding.UTF8.GetBytes(content);
        var hashBytes = SHA256.HashData(bytes);
        return Convert.ToHexString(hashBytes).ToLowerInvariant();
    }

    /// <summary>
    /// Compute a quick hash combining file size and modification time.
    /// Useful for fast change detection without reading file contents.
    /// </summary>
    public static string ComputeQuickHash(string filePath)
    {
        var info = new FileInfo(filePath);
        if (!info.Exists)
            return string.Empty;

        var combined = $"{info.Length}|{info.LastWriteTimeUtc.Ticks}";
        return ComputeHash(combined);
    }

    /// <summary>
    /// Check if file has changed by comparing quick hashes.
    /// </summary>
    public static bool HasFileChanged(
        string filePath,
        string previousQuickHash)
    {
        var currentHash = ComputeQuickHash(filePath);
        return currentHash != previousQuickHash;
    }
}
```

### File: `src/SeniorIntern.Services/VectorStore/FileScanner.cs`

```csharp
using System;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Text.RegularExpressions;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.Extensions.FileSystemGlobbing;
using Microsoft.Extensions.FileSystemGlobbing.Abstractions;
using Microsoft.Extensions.Logging;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Services.VectorStore;

/// <summary>
/// Scans directories for files to index based on patterns.
/// </summary>
public sealed class FileScanner
{
    private readonly ILogger<FileScanner> _logger;

    public FileScanner(ILogger<FileScanner> logger)
    {
        _logger = logger;
    }

    /// <summary>
    /// Scan a directory for files matching the index settings.
    /// </summary>
    public async Task<IReadOnlyList<ScannedFile>> ScanAsync(
        string rootPath,
        VectorIndexSettings settings,
        IProgress<FileScanProgress>? progress = null,
        CancellationToken ct = default)
    {
        _logger.LogInformation("Scanning {Path} for indexable files", rootPath);

        var files = new List<ScannedFile>();
        var matcher = new Matcher();

        // Add include patterns
        foreach (var pattern in settings.IncludePatterns)
        {
            matcher.AddInclude(pattern);
        }

        // Add exclude patterns
        foreach (var pattern in settings.ExcludePatterns)
        {
            matcher.AddExclude(pattern);
        }

        // Get .gitignore patterns if enabled
        var gitignorePatterns = settings.RespectGitignore
            ? await LoadGitignoreAsync(rootPath, ct)
            : Array.Empty<string>();

        foreach (var pattern in gitignorePatterns)
        {
            matcher.AddExclude(pattern);
        }

        // Execute matching
        var directory = new DirectoryInfoWrapper(new DirectoryInfo(rootPath));
        var matches = matcher.Execute(directory);

        var maxSizeBytes = settings.MaxFileSizeKb * 1024;
        var processedCount = 0;

        foreach (var match in matches.Files)
        {
            ct.ThrowIfCancellationRequested();

            var fullPath = Path.Combine(rootPath, match.Path);
            var relativePath = match.Path.Replace('\\', '/');

            try
            {
                var fileInfo = new FileInfo(fullPath);

                // Skip if doesn't exist or is a directory
                if (!fileInfo.Exists || (fileInfo.Attributes & FileAttributes.Directory) != 0)
                    continue;

                // Skip hidden files if configured
                if (!settings.IndexHiddenFiles && IsHidden(fileInfo))
                    continue;

                // Skip files that are too small or too large
                if (fileInfo.Length < settings.MinFileSizeBytes ||
                    fileInfo.Length > maxSizeBytes)
                {
                    _logger.LogDebug("Skipping {Path}: size {Size} bytes", relativePath, fileInfo.Length);
                    continue;
                }

                // Skip symlinks if configured
                if (!settings.FollowSymlinks && IsSymlink(fileInfo))
                    continue;

                var hash = await FileHasher.ComputeHashAsync(fullPath, ct);
                var language = DetectLanguage(relativePath);
                var lineCount = await CountLinesAsync(fullPath, ct);

                files.Add(new ScannedFile
                {
                    RelativePath = relativePath,
                    AbsolutePath = fullPath,
                    Size = fileInfo.Length,
                    LastModified = fileInfo.LastWriteTimeUtc,
                    ContentHash = hash,
                    Language = language,
                    LineCount = lineCount
                });

                processedCount++;

                if (processedCount % 100 == 0)
                {
                    progress?.Report(new FileScanProgress
                    {
                        ScannedCount = processedCount,
                        CurrentPath = relativePath
                    });
                }
            }
            catch (Exception ex) when (ex is IOException or UnauthorizedAccessException)
            {
                _logger.LogWarning("Cannot access file {Path}: {Message}", relativePath, ex.Message);
            }
        }

        _logger.LogInformation("Scan complete: found {Count} indexable files", files.Count);

        progress?.Report(new FileScanProgress
        {
            ScannedCount = files.Count,
            IsComplete = true
        });

        return files;
    }

    /// <summary>
    /// Scan for files that have changed since last index.
    /// </summary>
    public async Task<FileChangeSet> ScanForChangesAsync(
        string rootPath,
        VectorIndexSettings settings,
        IReadOnlyDictionary<string, string> existingFileHashes,
        CancellationToken ct = default)
    {
        var currentFiles = await ScanAsync(rootPath, settings, null, ct);

        var currentByPath = currentFiles.ToDictionary(f => f.RelativePath, f => f.ContentHash);

        var added = currentFiles
            .Where(f => !existingFileHashes.ContainsKey(f.RelativePath))
            .Select(f => f.RelativePath)
            .ToList();

        var modified = currentFiles
            .Where(f => existingFileHashes.TryGetValue(f.RelativePath, out var hash) &&
                       hash != f.ContentHash)
            .Select(f => f.RelativePath)
            .ToList();

        var deleted = existingFileHashes.Keys
            .Where(p => !currentByPath.ContainsKey(p))
            .ToList();

        var unchanged = currentFiles
            .Where(f => existingFileHashes.TryGetValue(f.RelativePath, out var hash) &&
                       hash == f.ContentHash)
            .Select(f => f.RelativePath)
            .ToList();

        return new FileChangeSet
        {
            AddedFiles = added,
            ModifiedFiles = modified,
            DeletedFiles = deleted,
            UnchangedFiles = unchanged
        };
    }

    private async Task<IReadOnlyList<string>> LoadGitignoreAsync(
        string rootPath,
        CancellationToken ct)
    {
        var gitignorePath = Path.Combine(rootPath, ".gitignore");
        if (!File.Exists(gitignorePath))
            return Array.Empty<string>();

        var patterns = new List<string>();
        var lines = await File.ReadAllLinesAsync(gitignorePath, ct);

        foreach (var line in lines)
        {
            var trimmed = line.Trim();
            if (string.IsNullOrEmpty(trimmed) || trimmed.StartsWith('#'))
                continue;

            // Convert gitignore pattern to glob pattern
            var pattern = ConvertGitignoreToGlob(trimmed);
            if (!string.IsNullOrEmpty(pattern))
                patterns.Add(pattern);
        }

        return patterns;
    }

    private static string ConvertGitignoreToGlob(string gitignorePattern)
    {
        // Basic conversion - gitignore patterns are similar to globs
        var pattern = gitignorePattern.TrimStart('/');

        // Handle directory patterns
        if (pattern.EndsWith('/'))
        {
            pattern = pattern.TrimEnd('/') + "/**";
        }

        // Add wildcard prefix if not anchored
        if (!gitignorePattern.StartsWith('/') && !pattern.StartsWith("**/"))
        {
            pattern = "**/" + pattern;
        }

        return pattern;
    }

    private static bool IsHidden(FileInfo file)
    {
        return file.Name.StartsWith('.') ||
               (file.Attributes & FileAttributes.Hidden) != 0;
    }

    private static bool IsSymlink(FileInfo file)
    {
        return (file.Attributes & FileAttributes.ReparsePoint) != 0;
    }

    private static string? DetectLanguage(string relativePath)
    {
        var ext = Path.GetExtension(relativePath).ToLowerInvariant();
        return ext switch
        {
            ".cs" => "csharp",
            ".fs" => "fsharp",
            ".vb" => "vb",
            ".ts" or ".tsx" => "typescript",
            ".js" or ".jsx" => "javascript",
            ".py" or ".pyi" => "python",
            ".java" => "java",
            ".kt" => "kotlin",
            ".go" => "go",
            ".rs" => "rust",
            ".cpp" or ".cc" or ".cxx" => "cpp",
            ".c" => "c",
            ".h" or ".hpp" => "cpp",
            ".swift" => "swift",
            ".rb" => "ruby",
            ".php" => "php",
            ".lua" => "lua",
            ".scala" => "scala",
            ".groovy" => "groovy",
            ".md" => "markdown",
            ".json" => "json",
            ".yaml" or ".yml" => "yaml",
            ".xml" => "xml",
            ".html" or ".htm" => "html",
            ".css" => "css",
            ".scss" or ".sass" => "scss",
            _ => null
        };
    }

    private static async Task<int> CountLinesAsync(string filePath, CancellationToken ct)
    {
        var lineCount = 0;
        await using var stream = File.OpenRead(filePath);
        using var reader = new StreamReader(stream);

        while (await reader.ReadLineAsync(ct) != null)
        {
            lineCount++;
        }

        return lineCount;
    }
}

/// <summary>
/// Information about a scanned file.
/// </summary>
public sealed class ScannedFile
{
    public required string RelativePath { get; init; }
    public required string AbsolutePath { get; init; }
    public long Size { get; init; }
    public DateTime LastModified { get; init; }
    public required string ContentHash { get; init; }
    public string? Language { get; init; }
    public int LineCount { get; init; }
}

/// <summary>
/// Progress information for file scanning.
/// </summary>
public sealed class FileScanProgress
{
    public int ScannedCount { get; init; }
    public string? CurrentPath { get; init; }
    public bool IsComplete { get; init; }
}
```

### File: `src/SeniorIntern.Services/VectorStore/FileContentReader.cs`

```csharp
using System;
using System.IO;
using System.Text;
using System.Threading;
using System.Threading.Tasks;
using UtfUnknown;

namespace SeniorIntern.Services.VectorStore;

/// <summary>
/// Reads file content with automatic encoding detection.
/// </summary>
public static class FileContentReader
{
    private const int DetectionSampleSize = 65536; // 64KB for encoding detection

    /// <summary>
    /// Read file content with automatic encoding detection.
    /// </summary>
    public static async Task<FileContent> ReadAsync(
        string filePath,
        CancellationToken ct = default)
    {
        var encoding = await DetectEncodingAsync(filePath, ct);

        var content = await File.ReadAllTextAsync(filePath, encoding, ct);

        // Normalize line endings to LF
        content = content.Replace("\r\n", "\n").Replace("\r", "\n");

        return new FileContent
        {
            Content = content,
            Encoding = encoding.WebName,
            LineCount = CountLines(content),
            CharacterCount = content.Length
        };
    }

    /// <summary>
    /// Detect the encoding of a file.
    /// </summary>
    public static async Task<Encoding> DetectEncodingAsync(
        string filePath,
        CancellationToken ct = default)
    {
        await using var stream = File.OpenRead(filePath);

        // Read a sample for detection
        var buffer = new byte[Math.Min(DetectionSampleSize, stream.Length)];
        var bytesRead = await stream.ReadAsync(buffer.AsMemory(0, buffer.Length), ct);

        if (bytesRead == 0)
            return Encoding.UTF8;

        // Check for BOM first
        if (bytesRead >= 3 && buffer[0] == 0xEF && buffer[1] == 0xBB && buffer[2] == 0xBF)
            return Encoding.UTF8;
        if (bytesRead >= 2 && buffer[0] == 0xFF && buffer[1] == 0xFE)
            return Encoding.Unicode;
        if (bytesRead >= 2 && buffer[0] == 0xFE && buffer[1] == 0xFF)
            return Encoding.BigEndianUnicode;

        // Use character detection library
        var result = CharsetDetector.DetectFromBytes(buffer.AsSpan(0, bytesRead));

        if (result.Detected != null && result.Detected.Confidence > 0.7f)
        {
            try
            {
                return Encoding.GetEncoding(result.Detected.EncodingName);
            }
            catch
            {
                // Fall back to UTF-8
            }
        }

        return Encoding.UTF8;
    }

    /// <summary>
    /// Extract lines from content within a range.
    /// </summary>
    public static string ExtractLines(
        string content,
        int startLine,
        int endLine)
    {
        var lines = content.Split('\n');
        var sb = new StringBuilder();

        // Convert to 0-based index
        var start = Math.Max(0, startLine - 1);
        var end = Math.Min(lines.Length, endLine);

        for (int i = start; i < end; i++)
        {
            if (sb.Length > 0)
                sb.Append('\n');
            sb.Append(lines[i]);
        }

        return sb.ToString();
    }

    /// <summary>
    /// Get the line number for a character offset.
    /// </summary>
    public static int GetLineNumber(string content, int offset)
    {
        var line = 1;
        for (int i = 0; i < Math.Min(offset, content.Length); i++)
        {
            if (content[i] == '\n')
                line++;
        }
        return line;
    }

    /// <summary>
    /// Get the character offset for a line number.
    /// </summary>
    public static int GetLineOffset(string content, int lineNumber)
    {
        var currentLine = 1;
        for (int i = 0; i < content.Length; i++)
        {
            if (currentLine == lineNumber)
                return i;
            if (content[i] == '\n')
                currentLine++;
        }
        return content.Length;
    }

    private static int CountLines(string content)
    {
        if (string.IsNullOrEmpty(content))
            return 0;

        var count = 1;
        foreach (var c in content)
        {
            if (c == '\n')
                count++;
        }
        return count;
    }
}

/// <summary>
/// Result of reading file content.
/// </summary>
public sealed class FileContent
{
    public required string Content { get; init; }
    public required string Encoding { get; init; }
    public int LineCount { get; init; }
    public int CharacterCount { get; init; }
}
```

---

## v0.7.2i: Statistics & Health Monitoring

### Objective
Implement statistics gathering and health monitoring for vector indexes.

### File: `src/SeniorIntern.Services/VectorStore/SqliteVectorStore.Statistics.cs`

```csharp
using System;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.Data.Sqlite;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Services.VectorStore;

public sealed partial class SqliteVectorStore
{
    public async Task<IndexStatistics> GetStatisticsAsync(
        string indexId,
        CancellationToken ct = default)
    {
        EnsureInitialized();

        var index = await GetIndexAsync(indexId, ct)
            ?? throw new ArgumentException($"Index not found: {indexId}");

        // Get file statistics by language
        var filesByLanguage = new Dictionary<string, int>();
        await using (var cmd = _connection!.CreateCommand())
        {
            cmd.CommandText = """
                SELECT language, COUNT(*) as count
                FROM indexed_files
                WHERE index_id = @indexId AND language IS NOT NULL
                GROUP BY language
                """;
            cmd.Parameters.AddWithValue("@indexId", indexId);

            await using var reader = await cmd.ExecuteReaderAsync(ct);
            while (await reader.ReadAsync(ct))
            {
                filesByLanguage[reader.GetString(0)] = reader.GetInt32(1);
            }
        }

        // Get chunk statistics by type
        var chunksByType = new Dictionary<ChunkType, int>();
        await using (var cmd = _connection.CreateCommand())
        {
            cmd.CommandText = """
                SELECT chunk_type, COUNT(*) as count
                FROM chunk_metadata
                WHERE index_id = @indexId
                GROUP BY chunk_type
                """;
            cmd.Parameters.AddWithValue("@indexId", indexId);

            await using var reader = await cmd.ExecuteReaderAsync(ct);
            while (await reader.ReadAsync(ct))
            {
                var type = Enum.Parse<ChunkType>(reader.GetString(0));
                chunksByType[type] = reader.GetInt32(1);
            }
        }

        // Get chunk statistics by symbol type
        var chunksBySymbolType = new Dictionary<SymbolType, int>();
        await using (var cmd = _connection.CreateCommand())
        {
            cmd.CommandText = """
                SELECT symbol_type, COUNT(*) as count
                FROM chunk_metadata
                WHERE index_id = @indexId AND symbol_type IS NOT NULL
                GROUP BY symbol_type
                """;
            cmd.Parameters.AddWithValue("@indexId", indexId);

            await using var reader = await cmd.ExecuteReaderAsync(ct);
            while (await reader.ReadAsync(ct))
            {
                var type = Enum.Parse<SymbolType>(reader.GetString(0));
                chunksBySymbolType[type] = reader.GetInt32(1);
            }
        }

        // Get average tokens per chunk
        double avgTokens = 0;
        await using (var cmd = _connection.CreateCommand())
        {
            cmd.CommandText = """
                SELECT AVG(token_count)
                FROM chunk_metadata
                WHERE index_id = @indexId
                """;
            cmd.Parameters.AddWithValue("@indexId", indexId);

            var result = await cmd.ExecuteScalarAsync(ct);
            if (result is double avg)
                avgTokens = avg;
        }

        // Get files with errors/skipped
        int filesWithErrors = 0, skippedFiles = 0;
        await using (var cmd = _connection.CreateCommand())
        {
            cmd.CommandText = """
                SELECT status, COUNT(*) as count
                FROM indexed_files
                WHERE index_id = @indexId
                GROUP BY status
                """;
            cmd.Parameters.AddWithValue("@indexId", indexId);

            await using var reader = await cmd.ExecuteReaderAsync(ct);
            while (await reader.ReadAsync(ct))
            {
                var status = Enum.Parse<FileIndexStatus>(reader.GetString(0));
                var count = reader.GetInt32(1);

                if (status == FileIndexStatus.Error)
                    filesWithErrors = count;
                else if (status == FileIndexStatus.Skipped)
                    skippedFiles = count;
            }
        }

        // Estimate storage sizes
        var dbPath = _options.DatabasePath ?? GetDefaultDatabasePath();
        var dbSize = File.Exists(dbPath) ? new FileInfo(dbPath).Length : 0;

        // Estimate vector data size (rough calculation)
        var vectorDataSize = (long)index.ChunkCount * index.EmbeddingDimension * sizeof(float);

        return new IndexStatistics
        {
            IndexId = index.Id,
            IndexName = index.Name,
            TotalFiles = index.FileCount,
            TotalChunks = index.ChunkCount,
            TotalFileSizeBytes = index.TotalFileSizeBytes,
            VectorDataSizeBytes = vectorDataSize,
            MetadataSizeBytes = dbSize - vectorDataSize,
            AverageTokensPerChunk = avgTokens,
            FilesByLanguage = filesByLanguage,
            ChunksByType = chunksByType,
            ChunksBySymbolType = chunksBySymbolType,
            FilesWithErrors = filesWithErrors,
            SkippedFiles = skippedFiles,
            LastModified = index.UpdatedAt,
            EmbeddingModel = index.EmbeddingModel,
            EmbeddingDimension = index.EmbeddingDimension
        };
    }

    public async Task<VectorStoreStatistics> GetOverallStatisticsAsync(CancellationToken ct = default)
    {
        EnsureInitialized();

        var indexes = await ListIndexesAsync(ct);
        var indexStats = new List<IndexStatistics>();

        foreach (var index in indexes)
        {
            indexStats.Add(await GetStatisticsAsync(index.Id, ct));
        }

        var dbPath = _options.DatabasePath ?? GetDefaultDatabasePath();
        var dbSize = File.Exists(dbPath) ? new FileInfo(dbPath).Length : 0;

        return new VectorStoreStatistics
        {
            TotalIndexes = indexes.Count,
            ActiveIndexes = indexes.Count(i => i.Status == IndexStatus.Active),
            TotalFiles = indexStats.Sum(s => s.TotalFiles),
            TotalChunks = indexStats.Sum(s => s.TotalChunks),
            TotalDatabaseSizeBytes = dbSize,
            IndexStats = indexStats
        };
    }

    /// <summary>
    /// Perform database maintenance (vacuum, optimize).
    /// </summary>
    public async Task OptimizeAsync(CancellationToken ct = default)
    {
        EnsureInitialized();

        _logger.LogInformation("Running database optimization...");

        await _writeLock.WaitAsync(ct);
        try
        {
            // Analyze tables for query optimization
            await using (var cmd = _connection!.CreateCommand())
            {
                cmd.CommandText = "ANALYZE";
                await cmd.ExecuteNonQueryAsync(ct);
            }

            // Vacuum to reclaim space
            await using (var cmd = _connection.CreateCommand())
            {
                cmd.CommandText = "VACUUM";
                await cmd.ExecuteNonQueryAsync(ct);
            }

            _logger.LogInformation("Database optimization complete");
        }
        finally
        {
            _writeLock.Release();
        }
    }

    /// <summary>
    /// Check database integrity.
    /// </summary>
    public async Task<DatabaseHealthCheck> CheckHealthAsync(CancellationToken ct = default)
    {
        EnsureInitialized();

        var issues = new List<string>();
        var isHealthy = true;

        // Check SQLite integrity
        await using (var cmd = _connection!.CreateCommand())
        {
            cmd.CommandText = "PRAGMA integrity_check";
            await using var reader = await cmd.ExecuteReaderAsync(ct);

            while (await reader.ReadAsync(ct))
            {
                var result = reader.GetString(0);
                if (result != "ok")
                {
                    issues.Add($"Integrity check: {result}");
                    isHealthy = false;
                }
            }
        }

        // Check foreign key violations
        await using (var cmd = _connection.CreateCommand())
        {
            cmd.CommandText = "PRAGMA foreign_key_check";
            await using var reader = await cmd.ExecuteReaderAsync(ct);

            while (await reader.ReadAsync(ct))
            {
                issues.Add($"Foreign key violation in {reader.GetString(0)}");
                isHealthy = false;
            }
        }

        // Check for orphaned chunks (chunks without files)
        await using (var cmd = _connection.CreateCommand())
        {
            cmd.CommandText = """
                SELECT COUNT(*) FROM chunk_metadata cm
                LEFT JOIN indexed_files if ON cm.file_id = if.id
                WHERE if.id IS NULL
                """;

            var orphanedChunks = Convert.ToInt32(await cmd.ExecuteScalarAsync(ct));
            if (orphanedChunks > 0)
            {
                issues.Add($"Found {orphanedChunks} orphaned chunks");
            }
        }

        return new DatabaseHealthCheck
        {
            IsHealthy = isHealthy,
            Issues = issues,
            CheckedAt = DateTime.UtcNow
        };
    }
}

/// <summary>
/// Result of database health check.
/// </summary>
public sealed class DatabaseHealthCheck
{
    public bool IsHealthy { get; init; }
    public IReadOnlyList<string> Issues { get; init; } = [];
    public DateTime CheckedAt { get; init; }
}
```

### File: `src/SeniorIntern.Services/VectorStore/VectorStoreHealthService.cs`

```csharp
using System;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.Extensions.Hosting;
using Microsoft.Extensions.Logging;
using SeniorIntern.Core.Interfaces;

namespace SeniorIntern.Services.VectorStore;

/// <summary>
/// Background service for periodic health monitoring of vector store.
/// </summary>
public sealed class VectorStoreHealthService : BackgroundService
{
    private readonly IVectorStore _vectorStore;
    private readonly ILogger<VectorStoreHealthService> _logger;
    private readonly TimeSpan _checkInterval = TimeSpan.FromHours(1);

    public VectorStoreHealthService(
        IVectorStore vectorStore,
        ILogger<VectorStoreHealthService> logger)
    {
        _vectorStore = vectorStore;
        _logger = logger;
    }

    protected override async Task ExecuteAsync(CancellationToken stoppingToken)
    {
        _logger.LogInformation("Vector store health monitoring started");

        while (!stoppingToken.IsCancellationRequested)
        {
            try
            {
                await Task.Delay(_checkInterval, stoppingToken);

                if (_vectorStore is SqliteVectorStore sqliteStore)
                {
                    var health = await sqliteStore.CheckHealthAsync(stoppingToken);

                    if (health.IsHealthy)
                    {
                        _logger.LogDebug("Vector store health check passed");
                    }
                    else
                    {
                        _logger.LogWarning(
                            "Vector store health check found issues: {Issues}",
                            string.Join(", ", health.Issues));
                    }
                }
            }
            catch (OperationCanceledException)
            {
                break;
            }
            catch (Exception ex)
            {
                _logger.LogError(ex, "Error during vector store health check");
            }
        }

        _logger.LogInformation("Vector store health monitoring stopped");
    }
}
```

---

## v0.7.2j: Unit Testing & Integration

### Objective
Implement comprehensive unit tests and integration tests for vector storage components.

### File: `tests/SeniorIntern.Tests/VectorStore/SqliteVectorStoreTests.cs`

```csharp
using System;
using System.IO;
using System.Linq;
using System.Threading.Tasks;
using Microsoft.Extensions.Logging.Abstractions;
using Microsoft.Extensions.Options;
using SeniorIntern.Core.Models;
using SeniorIntern.Core.Options;
using SeniorIntern.Services.VectorStore;
using Xunit;

namespace SeniorIntern.Tests.VectorStore;

public class SqliteVectorStoreTests : IAsyncLifetime
{
    private readonly string _dbPath;
    private SqliteVectorStore _store = null!;

    public SqliteVectorStoreTests()
    {
        _dbPath = Path.Combine(Path.GetTempPath(), $"test_vectors_{Guid.NewGuid()}.db");
    }

    public async Task InitializeAsync()
    {
        var options = Options.Create(new VectorStoreOptions
        {
            DatabasePath = _dbPath
        });

        _store = new SqliteVectorStore(
            options,
            NullLogger<SqliteVectorStore>.Instance);

        await _store.InitializeAsync();
    }

    public async Task DisposeAsync()
    {
        await _store.DisposeAsync();

        if (File.Exists(_dbPath))
            File.Delete(_dbPath);
    }

    [Fact]
    public async Task CreateIndex_ShouldCreateNewIndex()
    {
        // Arrange
        var workspacePath = "/test/workspace";
        var name = "Test Index";
        var dimension = 384;

        // Act
        var index = await _store.CreateIndexAsync(workspacePath, name, dimension);

        // Assert
        Assert.NotNull(index);
        Assert.Equal(name, index.Name);
        Assert.Equal(workspacePath, index.WorkspacePath);
        Assert.Equal(dimension, index.EmbeddingDimension);
        Assert.Equal(IndexStatus.Active, index.Status);
    }

    [Fact]
    public async Task GetIndex_ShouldReturnExistingIndex()
    {
        // Arrange
        var created = await _store.CreateIndexAsync("/test", "Test", 384);

        // Act
        var retrieved = await _store.GetIndexAsync(created.Id);

        // Assert
        Assert.NotNull(retrieved);
        Assert.Equal(created.Id, retrieved.Id);
        Assert.Equal(created.Name, retrieved.Name);
    }

    [Fact]
    public async Task GetIndexForWorkspace_ShouldFindByPath()
    {
        // Arrange
        var workspacePath = "/unique/workspace/path";
        await _store.CreateIndexAsync(workspacePath, "Test", 384);

        // Act
        var found = await _store.GetIndexForWorkspaceAsync(workspacePath);

        // Assert
        Assert.NotNull(found);
        Assert.Equal(workspacePath, found.WorkspacePath);
    }

    [Fact]
    public async Task DeleteIndex_ShouldRemoveIndex()
    {
        // Arrange
        var index = await _store.CreateIndexAsync("/test", "Test", 384);

        // Act
        await _store.DeleteIndexAsync(index.Id);
        var retrieved = await _store.GetIndexAsync(index.Id);

        // Assert
        Assert.Null(retrieved);
    }

    [Fact]
    public async Task AddChunks_ShouldStoreChunksWithEmbeddings()
    {
        // Arrange
        var index = await _store.CreateIndexAsync("/test", "Test", 4);

        var file = new IndexedFile
        {
            IndexId = index.Id,
            FilePath = "test.cs",
            FileHash = "abc123",
            FileSize = 100,
            LastModified = DateTime.UtcNow
        };
        await _store.UpsertIndexedFileAsync(index.Id, file);

        var chunks = new[]
        {
            new ChunkWithEmbedding
            {
                Chunk = new TextChunk
                {
                    Content = "public class Test { }",
                    FilePath = "test.cs",
                    StartLine = 1,
                    EndLine = 1,
                    Type = ChunkType.Code,
                    Language = "csharp"
                },
                Embedding = new float[] { 0.1f, 0.2f, 0.3f, 0.4f },
                FileId = file.Id
            }
        };

        // Act
        await _store.AddChunksAsync(index.Id, chunks);

        // Assert
        var updatedIndex = await _store.GetIndexAsync(index.Id);
        Assert.Equal(1, updatedIndex!.ChunkCount);
    }

    [Fact]
    public async Task RemoveChunksForFile_ShouldRemoveAllFileChunks()
    {
        // Arrange
        var index = await _store.CreateIndexAsync("/test", "Test", 4);

        var file = new IndexedFile
        {
            IndexId = index.Id,
            FilePath = "test.cs",
            FileHash = "abc123",
            FileSize = 100,
            LastModified = DateTime.UtcNow
        };
        await _store.UpsertIndexedFileAsync(index.Id, file);

        var chunks = Enumerable.Range(0, 5).Select(i => new ChunkWithEmbedding
        {
            Chunk = new TextChunk
            {
                Content = $"Chunk {i}",
                FilePath = "test.cs",
                StartLine = i + 1,
                EndLine = i + 1,
                Type = ChunkType.Code
            },
            Embedding = new float[] { i * 0.1f, i * 0.2f, i * 0.3f, i * 0.4f },
            FileId = file.Id
        }).ToList();

        await _store.AddChunksAsync(index.Id, chunks);

        // Act
        var removed = await _store.RemoveChunksForFileAsync(index.Id, "test.cs");

        // Assert
        Assert.Equal(5, removed);
    }
}
```

### File: `tests/SeniorIntern.Tests/VectorStore/VectorSearchTests.cs`

```csharp
using System;
using System.IO;
using System.Linq;
using System.Threading.Tasks;
using Microsoft.Extensions.Logging.Abstractions;
using Microsoft.Extensions.Options;
using SeniorIntern.Core.Models;
using SeniorIntern.Core.Options;
using SeniorIntern.Services.VectorStore;
using Xunit;

namespace SeniorIntern.Tests.VectorStore;

public class VectorSearchTests : IAsyncLifetime
{
    private readonly string _dbPath;
    private SqliteVectorStore _store = null!;
    private string _indexId = null!;

    public VectorSearchTests()
    {
        _dbPath = Path.Combine(Path.GetTempPath(), $"test_search_{Guid.NewGuid()}.db");
    }

    public async Task InitializeAsync()
    {
        var options = Options.Create(new VectorStoreOptions
        {
            DatabasePath = _dbPath
        });

        _store = new SqliteVectorStore(
            options,
            NullLogger<SqliteVectorStore>.Instance);

        await _store.InitializeAsync();

        // Create test index with sample data
        var index = await _store.CreateIndexAsync("/test", "Search Test", 4);
        _indexId = index.Id;

        await SetupTestDataAsync();
    }

    private async Task SetupTestDataAsync()
    {
        var file = new IndexedFile
        {
            IndexId = _indexId,
            FilePath = "test.cs",
            FileHash = "hash",
            FileSize = 1000,
            LastModified = DateTime.UtcNow
        };
        await _store.UpsertIndexedFileAsync(_indexId, file);

        var chunks = new[]
        {
            CreateChunk("public class UserService", ChunkType.Code, "csharp",
                new[] { 0.9f, 0.1f, 0.1f, 0.1f }, file.Id, 1),
            CreateChunk("public void GetUser(int id)", ChunkType.Code, "csharp",
                new[] { 0.8f, 0.2f, 0.1f, 0.1f }, file.Id, 5),
            CreateChunk("// This handles user retrieval", ChunkType.Comment, "csharp",
                new[] { 0.7f, 0.3f, 0.1f, 0.1f }, file.Id, 4),
            CreateChunk("public class ProductService", ChunkType.Code, "csharp",
                new[] { 0.1f, 0.9f, 0.1f, 0.1f }, file.Id, 20),
        };

        await _store.AddChunksAsync(_indexId, chunks);
    }

    private static ChunkWithEmbedding CreateChunk(
        string content,
        ChunkType type,
        string language,
        float[] embedding,
        string fileId,
        int line)
    {
        return new ChunkWithEmbedding
        {
            Chunk = new TextChunk
            {
                Content = content,
                FilePath = "test.cs",
                StartLine = line,
                EndLine = line,
                Type = type,
                Language = language
            },
            Embedding = embedding,
            FileId = fileId
        };
    }

    public async Task DisposeAsync()
    {
        await _store.DisposeAsync();

        if (File.Exists(_dbPath))
            File.Delete(_dbPath);
    }

    [Fact]
    public async Task Search_ShouldReturnSimilarChunks()
    {
        // Arrange
        var queryEmbedding = new float[] { 0.85f, 0.15f, 0.1f, 0.1f }; // Similar to UserService
        var options = new VectorSearchOptions { TopK = 2 };

        // Act
        var results = await _store.SearchAsync(_indexId, queryEmbedding, options);

        // Assert
        Assert.NotEmpty(results);
        Assert.True(results[0].Chunk.Content.Contains("User"));
    }

    [Fact]
    public async Task Search_WithLanguageFilter_ShouldFilterResults()
    {
        // Arrange
        var queryEmbedding = new float[] { 0.5f, 0.5f, 0.1f, 0.1f };
        var options = new VectorSearchOptions
        {
            TopK = 10,
            Languages = ["csharp"]
        };

        // Act
        var results = await _store.SearchAsync(_indexId, queryEmbedding, options);

        // Assert
        Assert.All(results, r => Assert.Equal("csharp", r.Chunk.Language));
    }

    [Fact]
    public async Task Search_WithChunkTypeFilter_ShouldFilterResults()
    {
        // Arrange
        var queryEmbedding = new float[] { 0.5f, 0.5f, 0.1f, 0.1f };
        var options = new VectorSearchOptions
        {
            TopK = 10,
            ChunkTypes = [ChunkType.Comment]
        };

        // Act
        var results = await _store.SearchAsync(_indexId, queryEmbedding, options);

        // Assert
        Assert.All(results, r => Assert.Equal(ChunkType.Comment, r.Chunk.ChunkType));
    }

    [Fact]
    public async Task Search_WithMinScore_ShouldExcludeLowScores()
    {
        // Arrange
        var queryEmbedding = new float[] { 0.9f, 0.1f, 0.1f, 0.1f };
        var options = new VectorSearchOptions
        {
            TopK = 10,
            MinScore = 0.8f
        };

        // Act
        var results = await _store.SearchAsync(_indexId, queryEmbedding, options);

        // Assert
        Assert.All(results, r => Assert.True(r.Score >= 0.8f));
    }
}
```

### File: `tests/SeniorIntern.Tests/VectorStore/VectorSearchHelperTests.cs`

```csharp
using System;
using SeniorIntern.Services.VectorStore;
using Xunit;

namespace SeniorIntern.Tests.VectorStore;

public class VectorSearchHelperTests
{
    [Fact]
    public void CosineSimilarity_IdenticalVectors_ShouldReturnOne()
    {
        // Arrange
        var a = new float[] { 1, 2, 3 };
        var b = new float[] { 1, 2, 3 };

        // Act
        var similarity = VectorSearchHelper.CosineSimilarity(a, b);

        // Assert
        Assert.Equal(1.0f, similarity, precision: 5);
    }

    [Fact]
    public void CosineSimilarity_OrthogonalVectors_ShouldReturnZero()
    {
        // Arrange
        var a = new float[] { 1, 0, 0 };
        var b = new float[] { 0, 1, 0 };

        // Act
        var similarity = VectorSearchHelper.CosineSimilarity(a, b);

        // Assert
        Assert.Equal(0.0f, similarity, precision: 5);
    }

    [Fact]
    public void CosineSimilarity_OppositeVectors_ShouldReturnNegativeOne()
    {
        // Arrange
        var a = new float[] { 1, 0, 0 };
        var b = new float[] { -1, 0, 0 };

        // Act
        var similarity = VectorSearchHelper.CosineSimilarity(a, b);

        // Assert
        Assert.Equal(-1.0f, similarity, precision: 5);
    }

    [Fact]
    public void Normalize_ShouldMakeUnitLength()
    {
        // Arrange
        var vector = new float[] { 3, 4, 0 };

        // Act
        VectorSearchHelper.Normalize(vector);

        // Assert
        var magnitude = MathF.Sqrt(vector[0] * vector[0] + vector[1] * vector[1] + vector[2] * vector[2]);
        Assert.Equal(1.0f, magnitude, precision: 5);
    }

    [Fact]
    public void L2Distance_IdenticalVectors_ShouldReturnZero()
    {
        // Arrange
        var a = new float[] { 1, 2, 3 };
        var b = new float[] { 1, 2, 3 };

        // Act
        var distance = VectorSearchHelper.L2Distance(a, b);

        // Assert
        Assert.Equal(0.0f, distance, precision: 5);
    }

    [Fact]
    public void L2Distance_DifferentVectors_ShouldReturnCorrectDistance()
    {
        // Arrange
        var a = new float[] { 0, 0, 0 };
        var b = new float[] { 3, 4, 0 };

        // Act
        var distance = VectorSearchHelper.L2Distance(a, b);

        // Assert
        Assert.Equal(5.0f, distance, precision: 5);
    }
}
```

### File: `tests/SeniorIntern.Tests/VectorStore/FileHasherTests.cs`

```csharp
using System.IO;
using System.Threading.Tasks;
using SeniorIntern.Services.VectorStore;
using Xunit;

namespace SeniorIntern.Tests.VectorStore;

public class FileHasherTests
{
    [Fact]
    public void ComputeHash_SameContent_ShouldReturnSameHash()
    {
        // Arrange
        var content = "Hello, World!";

        // Act
        var hash1 = FileHasher.ComputeHash(content);
        var hash2 = FileHasher.ComputeHash(content);

        // Assert
        Assert.Equal(hash1, hash2);
    }

    [Fact]
    public void ComputeHash_DifferentContent_ShouldReturnDifferentHash()
    {
        // Arrange
        var content1 = "Hello";
        var content2 = "World";

        // Act
        var hash1 = FileHasher.ComputeHash(content1);
        var hash2 = FileHasher.ComputeHash(content2);

        // Assert
        Assert.NotEqual(hash1, hash2);
    }

    [Fact]
    public async Task ComputeHashAsync_File_ShouldMatchStringHash()
    {
        // Arrange
        var tempPath = Path.GetTempFileName();
        var content = "Test content for hashing";
        await File.WriteAllTextAsync(tempPath, content);

        try
        {
            // Act
            var fileHash = await FileHasher.ComputeHashAsync(tempPath);

            // Assert
            Assert.NotEmpty(fileHash);
            Assert.Equal(64, fileHash.Length); // SHA-256 produces 64 hex characters
        }
        finally
        {
            File.Delete(tempPath);
        }
    }

    [Fact]
    public void ComputeQuickHash_ShouldIncludeSizeAndTime()
    {
        // Arrange
        var tempPath = Path.GetTempFileName();
        File.WriteAllText(tempPath, "Test");

        try
        {
            // Act
            var hash = FileHasher.ComputeQuickHash(tempPath);

            // Assert
            Assert.NotEmpty(hash);
        }
        finally
        {
            File.Delete(tempPath);
        }
    }
}
```

### File: `tests/SeniorIntern.Tests/VectorStore/FileScannerTests.cs`

```csharp
using System.IO;
using System.Linq;
using System.Threading.Tasks;
using Microsoft.Extensions.Logging.Abstractions;
using SeniorIntern.Core.Models;
using SeniorIntern.Services.VectorStore;
using Xunit;

namespace SeniorIntern.Tests.VectorStore;

public class FileScannerTests
{
    private readonly FileScanner _scanner;

    public FileScannerTests()
    {
        _scanner = new FileScanner(NullLogger<FileScanner>.Instance);
    }

    [Fact]
    public async Task Scan_ShouldFindMatchingFiles()
    {
        // Arrange
        var tempDir = Path.Combine(Path.GetTempPath(), $"scan_test_{Path.GetRandomFileName()}");
        Directory.CreateDirectory(tempDir);

        File.WriteAllText(Path.Combine(tempDir, "test.cs"), "public class Test { }");
        File.WriteAllText(Path.Combine(tempDir, "test.txt"), "Plain text");

        var settings = new VectorIndexSettings
        {
            IncludePatterns = ["**/*.cs"],
            ExcludePatterns = []
        };

        try
        {
            // Act
            var files = await _scanner.ScanAsync(tempDir, settings);

            // Assert
            Assert.Single(files);
            Assert.Equal("test.cs", files[0].RelativePath);
            Assert.Equal("csharp", files[0].Language);
        }
        finally
        {
            Directory.Delete(tempDir, recursive: true);
        }
    }

    [Fact]
    public async Task Scan_ShouldExcludePatterns()
    {
        // Arrange
        var tempDir = Path.Combine(Path.GetTempPath(), $"scan_test_{Path.GetRandomFileName()}");
        var nodeModules = Path.Combine(tempDir, "node_modules");
        Directory.CreateDirectory(nodeModules);

        File.WriteAllText(Path.Combine(tempDir, "app.ts"), "const x = 1;");
        File.WriteAllText(Path.Combine(nodeModules, "lib.ts"), "export const y = 2;");

        var settings = new VectorIndexSettings
        {
            IncludePatterns = ["**/*.ts"],
            ExcludePatterns = ["**/node_modules/**"]
        };

        try
        {
            // Act
            var files = await _scanner.ScanAsync(tempDir, settings);

            // Assert
            Assert.Single(files);
            Assert.Equal("app.ts", files[0].RelativePath);
        }
        finally
        {
            Directory.Delete(tempDir, recursive: true);
        }
    }

    [Fact]
    public async Task Scan_ShouldSkipLargeFiles()
    {
        // Arrange
        var tempDir = Path.Combine(Path.GetTempPath(), $"scan_test_{Path.GetRandomFileName()}");
        Directory.CreateDirectory(tempDir);

        var smallContent = "small";
        var largeContent = new string('x', 2 * 1024 * 1024); // 2MB

        File.WriteAllText(Path.Combine(tempDir, "small.cs"), smallContent);
        File.WriteAllText(Path.Combine(tempDir, "large.cs"), largeContent);

        var settings = new VectorIndexSettings
        {
            IncludePatterns = ["**/*.cs"],
            MaxFileSizeKb = 1024 // 1MB max
        };

        try
        {
            // Act
            var files = await _scanner.ScanAsync(tempDir, settings);

            // Assert
            Assert.Single(files);
            Assert.Equal("small.cs", files[0].RelativePath);
        }
        finally
        {
            Directory.Delete(tempDir, recursive: true);
        }
    }

    [Fact]
    public async Task ScanForChanges_ShouldDetectAddedFiles()
    {
        // Arrange
        var tempDir = Path.Combine(Path.GetTempPath(), $"scan_test_{Path.GetRandomFileName()}");
        Directory.CreateDirectory(tempDir);

        File.WriteAllText(Path.Combine(tempDir, "existing.cs"), "existing");
        File.WriteAllText(Path.Combine(tempDir, "new.cs"), "new file");

        var existingHashes = new Dictionary<string, string>
        {
            ["existing.cs"] = FileHasher.ComputeHash("existing")
        };

        var settings = new VectorIndexSettings
        {
            IncludePatterns = ["**/*.cs"]
        };

        try
        {
            // Act
            var changes = await _scanner.ScanForChangesAsync(tempDir, settings, existingHashes);

            // Assert
            Assert.Single(changes.AddedFiles);
            Assert.Contains("new.cs", changes.AddedFiles);
            Assert.Single(changes.UnchangedFiles);
        }
        finally
        {
            Directory.Delete(tempDir, recursive: true);
        }
    }
}
```

### File: `tests/SeniorIntern.Tests/VectorStore/FileContentReaderTests.cs`

```csharp
using System.IO;
using System.Text;
using System.Threading.Tasks;
using SeniorIntern.Services.VectorStore;
using Xunit;

namespace SeniorIntern.Tests.VectorStore;

public class FileContentReaderTests
{
    [Fact]
    public async Task ReadAsync_Utf8File_ShouldReadCorrectly()
    {
        // Arrange
        var tempPath = Path.GetTempFileName();
        var content = "Hello, World!\nLine 2\nLine 3";
        await File.WriteAllTextAsync(tempPath, content, Encoding.UTF8);

        try
        {
            // Act
            var result = await FileContentReader.ReadAsync(tempPath);

            // Assert
            Assert.Equal("Hello, World!\nLine 2\nLine 3", result.Content);
            Assert.Equal(3, result.LineCount);
            Assert.Equal("utf-8", result.Encoding.ToLowerInvariant());
        }
        finally
        {
            File.Delete(tempPath);
        }
    }

    [Fact]
    public async Task ReadAsync_ShouldNormalizeLineEndings()
    {
        // Arrange
        var tempPath = Path.GetTempFileName();
        var content = "Line 1\r\nLine 2\rLine 3\nLine 4";
        await File.WriteAllBytesAsync(tempPath, Encoding.UTF8.GetBytes(content));

        try
        {
            // Act
            var result = await FileContentReader.ReadAsync(tempPath);

            // Assert
            Assert.DoesNotContain("\r", result.Content);
            Assert.Equal(4, result.LineCount);
        }
        finally
        {
            File.Delete(tempPath);
        }
    }

    [Fact]
    public void ExtractLines_ShouldExtractCorrectRange()
    {
        // Arrange
        var content = "Line 1\nLine 2\nLine 3\nLine 4\nLine 5";

        // Act
        var extracted = FileContentReader.ExtractLines(content, 2, 4);

        // Assert
        Assert.Equal("Line 2\nLine 3\nLine 4", extracted);
    }

    [Fact]
    public void GetLineNumber_ShouldReturnCorrectLine()
    {
        // Arrange
        var content = "Line 1\nLine 2\nLine 3";

        // Act & Assert
        Assert.Equal(1, FileContentReader.GetLineNumber(content, 0));
        Assert.Equal(1, FileContentReader.GetLineNumber(content, 5));
        Assert.Equal(2, FileContentReader.GetLineNumber(content, 7));
        Assert.Equal(3, FileContentReader.GetLineNumber(content, 14));
    }

    [Fact]
    public void GetLineOffset_ShouldReturnCorrectOffset()
    {
        // Arrange
        var content = "Line 1\nLine 2\nLine 3";

        // Act & Assert
        Assert.Equal(0, FileContentReader.GetLineOffset(content, 1));
        Assert.Equal(7, FileContentReader.GetLineOffset(content, 2));
        Assert.Equal(14, FileContentReader.GetLineOffset(content, 3));
    }
}
```

### File: `tests/SeniorIntern.Tests/VectorStore/VectorIndexSettingsTests.cs`

```csharp
using System.Linq;
using SeniorIntern.Core.Models;
using Xunit;

namespace SeniorIntern.Tests.VectorStore;

public class VectorIndexSettingsTests
{
    [Fact]
    public void DefaultSettings_ShouldHaveReasonableDefaults()
    {
        // Arrange & Act
        var settings = new VectorIndexSettings();

        // Assert
        Assert.NotEmpty(settings.IncludePatterns);
        Assert.NotEmpty(settings.ExcludePatterns);
        Assert.Equal(1024, settings.MaxFileSizeKb);
        Assert.True(settings.RespectGitignore);
        Assert.False(settings.IndexHiddenFiles);
    }

    [Fact]
    public void DefaultSettings_ShouldIncludeCommonCodeFiles()
    {
        // Arrange & Act
        var settings = new VectorIndexSettings();

        // Assert
        Assert.Contains("**/*.cs", settings.IncludePatterns);
        Assert.Contains("**/*.ts", settings.IncludePatterns);
        Assert.Contains("**/*.py", settings.IncludePatterns);
        Assert.Contains("**/*.java", settings.IncludePatterns);
    }

    [Fact]
    public void DefaultSettings_ShouldExcludeCommonDirectories()
    {
        // Arrange & Act
        var settings = new VectorIndexSettings();

        // Assert
        Assert.Contains("**/node_modules/**", settings.ExcludePatterns);
        Assert.Contains("**/bin/**", settings.ExcludePatterns);
        Assert.Contains("**/obj/**", settings.ExcludePatterns);
        Assert.Contains("**/.git/**", settings.ExcludePatterns);
    }

    [Fact]
    public void ChunkingOptions_ShouldHaveValidDefaults()
    {
        // Arrange & Act
        var settings = new VectorIndexSettings();

        // Assert
        Assert.True(settings.ChunkingOptions.TargetChunkSize > 0);
        Assert.True(settings.ChunkingOptions.MaxChunkSize > settings.ChunkingOptions.TargetChunkSize);
        Assert.True(settings.ChunkingOptions.ChunkOverlap > 0);
    }
}
```

### File: `tests/SeniorIntern.Tests/VectorStore/IndexStatisticsTests.cs`

```csharp
using System.Collections.Generic;
using SeniorIntern.Core.Models;
using Xunit;

namespace SeniorIntern.Tests.VectorStore;

public class IndexStatisticsTests
{
    [Fact]
    public void AverageChunksPerFile_ShouldCalculateCorrectly()
    {
        // Arrange
        var stats = new IndexStatistics
        {
            IndexId = "test",
            IndexName = "Test",
            TotalFiles = 10,
            TotalChunks = 50,
            EmbeddingModel = "test"
        };

        // Act & Assert
        Assert.Equal(5.0, stats.AverageChunksPerFile);
    }

    [Fact]
    public void AverageChunksPerFile_ZeroFiles_ShouldReturnZero()
    {
        // Arrange
        var stats = new IndexStatistics
        {
            IndexId = "test",
            IndexName = "Test",
            TotalFiles = 0,
            TotalChunks = 0,
            EmbeddingModel = "test"
        };

        // Act & Assert
        Assert.Equal(0, stats.AverageChunksPerFile);
    }

    [Theory]
    [InlineData(1024, "1.00 KB")]
    [InlineData(1048576, "1.00 MB")]
    [InlineData(1073741824, "1.00 GB")]
    public void FormattedFileSize_ShouldFormatCorrectly(long bytes, string expected)
    {
        // Arrange
        var stats = new IndexStatistics
        {
            IndexId = "test",
            IndexName = "Test",
            TotalFileSizeBytes = bytes,
            EmbeddingModel = "test"
        };

        // Act & Assert
        Assert.Equal(expected, stats.FormattedFileSize);
    }
}
```

---

## Dependencies & NuGet Packages

### Required NuGet Packages

| Package | Version | Purpose |
|---------|---------|---------|
| `Microsoft.Data.Sqlite` | 9.0.x | SQLite database connectivity |
| `sqlite-vec` | 0.1.x | Vector extension for SQLite |
| `Microsoft.Extensions.FileSystemGlobbing` | 9.0.x | Glob pattern matching for file scanning |
| `UtfUnknown` | 2.5.x | Encoding detection for file reading |
| `System.Text.Json` | 9.0.x | JSON serialization for settings |

### Native Dependencies

The `sqlite-vec` extension requires native binaries for each platform:
- Windows: `vec0.dll`
- macOS: `vec0.dylib`
- Linux: `vec0.so`

These should be included in the `runtimes/{rid}/native/` folder structure.

---

## Implementation Sequence

```

  v0.7.2a: Vector Store Interface                                    
   IVectorStore.cs, ChunkWithEmbedding.cs, StoredChunk.cs         

                           
                           

  v0.7.2b: Vector Index Models                                       
   VectorIndex.cs, VectorIndexSettings.cs, IndexedFile.cs         

                           
                           

  v0.7.2c: Search Models & Options                                   
   VectorSearchOptions.cs, ChunkSearchResult.cs, IndexStatistics   

                           
                           

  v0.7.2d: SQLite-vec Database Schema                               
   VectorStoreSchema.cs, VectorStoreMigrations.cs                 

                           
                           

  v0.7.2e: SQLite-vec Store Implementation                          
   VectorStoreOptions.cs, SqliteVectorStore.cs (main)             

                           
                           

  v0.7.2f: Index Management Operations                              
   SqliteVectorStore.ChunkOperations.cs                           

                           
                           

  v0.7.2g: Vector Search Implementation                             
   SqliteVectorStore.Search.cs, VectorSearchHelper.cs             

                           
                           

  v0.7.2h: File Tracking & Change Detection                         
   FileHasher.cs, FileScanner.cs, FileContentReader.cs            

                           
                           

  v0.7.2i: Statistics & Health Monitoring                           
   SqliteVectorStore.Statistics.cs, VectorStoreHealthService.cs   

                           
                           

  v0.7.2j: Unit Testing & Integration                               
   All test files in tests/SeniorIntern.Tests/VectorStore/        

```

---

## Summary

This design specification covers v0.7.2 Vector Storage with 10 sub-parts:

| Sub-version | Focus | Key Deliverables |
|-------------|-------|------------------|
| v0.7.2a | Vector Store Interface | IVectorStore, ChunkWithEmbedding, StoredChunk |
| v0.7.2b | Vector Index Models | VectorIndex, VectorIndexSettings, IndexedFile |
| v0.7.2c | Search Models & Options | VectorSearchOptions, ChunkSearchResult, IndexStatistics |
| v0.7.2d | Database Schema | VectorStoreSchema, VectorStoreMigrations |
| v0.7.2e | Store Implementation | SqliteVectorStore main class |
| v0.7.2f | Index Management | Chunk CRUD operations |
| v0.7.2g | Search Implementation | Vector similarity search with filters |
| v0.7.2h | File Tracking | FileHasher, FileScanner, FileContentReader |
| v0.7.2i | Statistics & Health | Statistics queries, health monitoring |
| v0.7.2j | Testing | Comprehensive unit and integration tests |

**Totals:**
- **33 files to create**
- **1 file to modify** (AppSettings/DI registration)
- **4 NuGet packages required**
