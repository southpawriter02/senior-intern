# Design Specification: AIntern v0.7.3 "Indexing Pipeline"

## Overview

This document provides a comprehensive design specification for v0.7.3 of The Senior Intern project. This version implements the complete file scanning and indexing pipeline that processes workspace files into searchable embeddings. It includes incremental updates, background processing, progress tracking, and integration with the embedding and vector storage systems from v0.7.1 and v0.7.2.

### Objectives
- Implement the `IIndexingService` interface for workspace indexing
- Build file scanning with glob pattern matching and .gitignore support
- Create incremental indexing that only processes changed files
- Implement background processing with cancellation support
- Provide detailed progress tracking and event notifications
- Build the indexing job queue for background operations
- Implement file watcher for automatic re-indexing
- Create robust error handling and recovery mechanisms

### Prerequisites
- v0.7.1 (Embedding Foundation) completed - `IEmbeddingService`, `IChunkingService`
- v0.7.2 (Vector Storage) completed - `IVectorStore`, file tracking
- Understanding of async/parallel processing patterns

### Architecture Overview

```
┌─────────────────────────────────────────────────────────────────────────┐
│                         Indexing Pipeline                                │
│                                                                          │
│  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐              │
│  │   Trigger    │───▶│  Job Queue   │───▶│   Worker     │              │
│  │  (Manual/    │    │              │    │   (Background)│              │
│  │   FileWatch) │    └──────────────┘    └──────┬───────┘              │
│  └──────────────┘                               │                       │
│                                                 ▼                       │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                    IndexingService                               │   │
│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌────────┐ │   │
│  │  │  Scan   │─▶│  Filter │─▶│  Chunk  │─▶│  Embed  │─▶│ Store  │ │   │
│  │  │  Files  │  │ Changed │  │  Files  │  │  Batch  │  │ Vectors│ │   │
│  │  └─────────┘  └─────────┘  └─────────┘  └─────────┘  └────────┘ │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                    │                                    │
│                                    ▼                                    │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                    Progress & Events                             │   │
│  │  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐  │   │
│  │  │ Progress Report │  │  State Changed  │  │  File Indexed   │  │   │
│  │  └─────────────────┘  └─────────────────┘  └─────────────────┘  │   │
│  └─────────────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## Sub-version Breakdown

| Version | Focus | Files to Create | Files to Modify |
|---------|-------|-----------------|-----------------|
| v0.7.3a | Indexing Service Interface | 4 | 0 |
| v0.7.3b | Indexing Models & Options | 4 | 0 |
| v0.7.3c | Indexing Service Implementation | 3 | 1 |
| v0.7.3d | File Processing Pipeline | 3 | 0 |
| v0.7.3e | Gitignore Pattern Matching | 2 | 0 |
| v0.7.3f | Incremental Indexing Logic | 2 | 0 |
| v0.7.3g | Background Job Queue | 3 | 0 |
| v0.7.3h | File Watcher Service | 3 | 0 |
| v0.7.3i | Progress Tracking & Events | 3 | 0 |
| v0.7.3j | Unit Testing & Integration | 10 | 0 |

**Totals: 37 files to create, 1 file to modify**

---

## v0.7.3a: Indexing Service Interface

### Objective
Define the core interface for the indexing service, establishing the contract for workspace indexing operations.

### File: `src/SeniorIntern.Core/Interfaces/IIndexingService.cs`

```csharp
using System;
using System.Collections.Generic;
using System.Threading;
using System.Threading.Tasks;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Core.Interfaces;

/// <summary>
/// Service for indexing workspace files into the vector store.
/// Provides full workspace indexing, incremental updates, and file management.
/// </summary>
public interface IIndexingService
{
    /// <summary>
    /// Current status of the indexing service.
    /// </summary>
    IndexingStatus Status { get; }

    /// <summary>
    /// Whether an indexing operation is currently in progress.
    /// </summary>
    bool IsIndexing { get; }

    /// <summary>
    /// The currently active indexing job, if any.
    /// </summary>
    IndexingJob? CurrentJob { get; }

    #region Workspace Indexing

    /// <summary>
    /// Index an entire workspace directory.
    /// Creates a new index or updates an existing one.
    /// </summary>
    /// <param name="workspacePath">Absolute path to the workspace directory.</param>
    /// <param name="options">Indexing configuration options.</param>
    /// <param name="progress">Optional progress reporter.</param>
    /// <param name="ct">Cancellation token.</param>
    /// <returns>Result of the indexing operation.</returns>
    Task<IndexingResult> IndexWorkspaceAsync(
        string workspacePath,
        IndexingOptions options,
        IProgress<IndexingProgress>? progress = null,
        CancellationToken ct = default);

    /// <summary>
    /// Queue a workspace indexing job for background processing.
    /// </summary>
    /// <param name="workspacePath">Absolute path to the workspace directory.</param>
    /// <param name="options">Indexing configuration options.</param>
    /// <returns>The queued job information.</returns>
    IndexingJob QueueWorkspaceIndexing(
        string workspacePath,
        IndexingOptions options);

    #endregion

    #region Incremental Updates

    /// <summary>
    /// Update an existing index with only changed files.
    /// </summary>
    /// <param name="indexId">ID of the index to update.</param>
    /// <param name="progress">Optional progress reporter.</param>
    /// <param name="ct">Cancellation token.</param>
    /// <returns>Result of the update operation.</returns>
    Task<IndexingResult> UpdateIndexAsync(
        string indexId,
        IProgress<IndexingProgress>? progress = null,
        CancellationToken ct = default);

    /// <summary>
    /// Index specific files within an existing index.
    /// </summary>
    /// <param name="indexId">ID of the index to update.</param>
    /// <param name="filePaths">Relative paths to the files to index.</param>
    /// <param name="progress">Optional progress reporter.</param>
    /// <param name="ct">Cancellation token.</param>
    /// <returns>Result of the indexing operation.</returns>
    Task<IndexingResult> IndexFilesAsync(
        string indexId,
        IEnumerable<string> filePaths,
        IProgress<IndexingProgress>? progress = null,
        CancellationToken ct = default);

    /// <summary>
    /// Re-index a specific file (remove old chunks and create new ones).
    /// </summary>
    /// <param name="indexId">ID of the index.</param>
    /// <param name="filePath">Relative path to the file.</param>
    /// <param name="ct">Cancellation token.</param>
    /// <returns>Number of chunks created.</returns>
    Task<int> ReindexFileAsync(
        string indexId,
        string filePath,
        CancellationToken ct = default);

    #endregion

    #region File Management

    /// <summary>
    /// Remove files from an index.
    /// </summary>
    /// <param name="indexId">ID of the index.</param>
    /// <param name="filePaths">Relative paths to remove.</param>
    /// <param name="ct">Cancellation token.</param>
    /// <returns>Number of chunks removed.</returns>
    Task<int> RemoveFilesAsync(
        string indexId,
        IEnumerable<string> filePaths,
        CancellationToken ct = default);

    /// <summary>
    /// Synchronize index with filesystem (add new, update changed, remove deleted).
    /// </summary>
    /// <param name="indexId">ID of the index to sync.</param>
    /// <param name="progress">Optional progress reporter.</param>
    /// <param name="ct">Cancellation token.</param>
    /// <returns>Sync operation result.</returns>
    Task<IndexSyncResult> SyncIndexAsync(
        string indexId,
        IProgress<IndexingProgress>? progress = null,
        CancellationToken ct = default);

    #endregion

    #region Job Control

    /// <summary>
    /// Cancel the current indexing operation.
    /// </summary>
    void CancelIndexing();

    /// <summary>
    /// Pause the current indexing operation.
    /// </summary>
    void PauseIndexing();

    /// <summary>
    /// Resume a paused indexing operation.
    /// </summary>
    void ResumeIndexing();

    /// <summary>
    /// Get status of a specific job.
    /// </summary>
    /// <param name="jobId">The job ID to query.</param>
    /// <returns>Job status if found.</returns>
    IndexingJob? GetJob(string jobId);

    /// <summary>
    /// Get all pending and active jobs.
    /// </summary>
    IReadOnlyList<IndexingJob> GetActiveJobs();

    /// <summary>
    /// Cancel a specific queued job.
    /// </summary>
    /// <param name="jobId">The job ID to cancel.</param>
    /// <returns>True if job was found and cancelled.</returns>
    bool CancelJob(string jobId);

    #endregion

    #region Events

    /// <summary>
    /// Raised when indexing status changes.
    /// </summary>
    event EventHandler<IndexingStateChangedEventArgs>? StateChanged;

    /// <summary>
    /// Raised when a file is successfully indexed.
    /// </summary>
    event EventHandler<FileIndexedEventArgs>? FileIndexed;

    /// <summary>
    /// Raised when a file indexing fails.
    /// </summary>
    event EventHandler<FileIndexingErrorEventArgs>? FileError;

    /// <summary>
    /// Raised when an indexing job starts.
    /// </summary>
    event EventHandler<IndexingJobEventArgs>? JobStarted;

    /// <summary>
    /// Raised when an indexing job completes.
    /// </summary>
    event EventHandler<IndexingJobCompletedEventArgs>? JobCompleted;

    /// <summary>
    /// Raised periodically with progress updates.
    /// </summary>
    event EventHandler<IndexingProgressEventArgs>? ProgressUpdated;

    #endregion
}
```

### File: `src/SeniorIntern.Core/Interfaces/IIndexingJobQueue.cs`

```csharp
using System;
using System.Collections.Generic;
using System.Threading;
using System.Threading.Tasks;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Core.Interfaces;

/// <summary>
/// Queue for managing background indexing jobs.
/// </summary>
public interface IIndexingJobQueue
{
    /// <summary>
    /// Number of jobs currently in the queue.
    /// </summary>
    int QueueLength { get; }

    /// <summary>
    /// Whether the queue is currently processing jobs.
    /// </summary>
    bool IsProcessing { get; }

    /// <summary>
    /// Enqueue a new indexing job.
    /// </summary>
    /// <param name="job">The job to enqueue.</param>
    void Enqueue(IndexingJob job);

    /// <summary>
    /// Try to dequeue the next job.
    /// </summary>
    /// <param name="job">The dequeued job if available.</param>
    /// <returns>True if a job was dequeued.</returns>
    bool TryDequeue(out IndexingJob? job);

    /// <summary>
    /// Wait for and dequeue the next job.
    /// </summary>
    /// <param name="ct">Cancellation token.</param>
    /// <returns>The next job in queue.</returns>
    Task<IndexingJob> DequeueAsync(CancellationToken ct = default);

    /// <summary>
    /// Get all jobs in the queue.
    /// </summary>
    IReadOnlyList<IndexingJob> GetQueuedJobs();

    /// <summary>
    /// Remove a specific job from the queue.
    /// </summary>
    /// <param name="jobId">ID of the job to remove.</param>
    /// <returns>True if job was found and removed.</returns>
    bool Remove(string jobId);

    /// <summary>
    /// Clear all jobs from the queue.
    /// </summary>
    void Clear();

    /// <summary>
    /// Prioritize a job (move to front of queue).
    /// </summary>
    /// <param name="jobId">ID of the job to prioritize.</param>
    /// <returns>True if job was found and prioritized.</returns>
    bool Prioritize(string jobId);

    /// <summary>
    /// Raised when a job is enqueued.
    /// </summary>
    event EventHandler<IndexingJobEventArgs>? JobEnqueued;

    /// <summary>
    /// Raised when a job is dequeued for processing.
    /// </summary>
    event EventHandler<IndexingJobEventArgs>? JobDequeued;
}
```

### File: `src/SeniorIntern.Core/Interfaces/IFileWatcherService.cs`

```csharp
using System;
using System.Collections.Generic;
using System.Threading;
using System.Threading.Tasks;

namespace SeniorIntern.Core.Interfaces;

/// <summary>
/// Service for watching file system changes and triggering re-indexing.
/// </summary>
public interface IFileWatcherService : IAsyncDisposable
{
    /// <summary>
    /// Whether file watching is currently enabled.
    /// </summary>
    bool IsEnabled { get; }

    /// <summary>
    /// Workspaces currently being watched.
    /// </summary>
    IReadOnlyList<string> WatchedWorkspaces { get; }

    /// <summary>
    /// Start watching a workspace for file changes.
    /// </summary>
    /// <param name="indexId">ID of the index to update on changes.</param>
    /// <param name="workspacePath">Path to watch.</param>
    /// <param name="options">Watch configuration options.</param>
    void StartWatching(
        string indexId,
        string workspacePath,
        FileWatchOptions? options = null);

    /// <summary>
    /// Stop watching a workspace.
    /// </summary>
    /// <param name="workspacePath">Path to stop watching.</param>
    void StopWatching(string workspacePath);

    /// <summary>
    /// Stop watching all workspaces.
    /// </summary>
    void StopAll();

    /// <summary>
    /// Enable or disable all file watching.
    /// </summary>
    /// <param name="enabled">Whether to enable watching.</param>
    void SetEnabled(bool enabled);

    /// <summary>
    /// Get pending file changes that haven't been processed yet.
    /// </summary>
    /// <param name="workspacePath">Workspace to query.</param>
    /// <returns>List of pending file paths.</returns>
    IReadOnlyList<string> GetPendingChanges(string workspacePath);

    /// <summary>
    /// Raised when files change and need re-indexing.
    /// </summary>
    event EventHandler<FileChangesDetectedEventArgs>? ChangesDetected;
}

/// <summary>
/// Options for file watching behavior.
/// </summary>
public sealed class FileWatchOptions
{
    /// <summary>
    /// Delay in milliseconds before triggering re-indexing after changes.
    /// Helps batch rapid file changes.
    /// </summary>
    public int DebounceDelayMs { get; init; } = 2000;

    /// <summary>
    /// Whether to watch subdirectories.
    /// </summary>
    public bool IncludeSubdirectories { get; init; } = true;

    /// <summary>
    /// File patterns to watch (default: all files).
    /// </summary>
    public IReadOnlyList<string>? FilePatterns { get; init; }

    /// <summary>
    /// Patterns to exclude from watching.
    /// </summary>
    public IReadOnlyList<string>? ExcludePatterns { get; init; }

    /// <summary>
    /// Whether to automatically queue re-indexing on changes.
    /// </summary>
    public bool AutoReindex { get; init; } = true;

    /// <summary>
    /// Maximum number of changes to batch before triggering.
    /// </summary>
    public int MaxBatchSize { get; init; } = 100;
}

/// <summary>
/// Event args for file changes detection.
/// </summary>
public sealed class FileChangesDetectedEventArgs : EventArgs
{
    public required string IndexId { get; init; }
    public required string WorkspacePath { get; init; }
    public required IReadOnlyList<FileChange> Changes { get; init; }
    public DateTime DetectedAt { get; init; } = DateTime.UtcNow;
}

/// <summary>
/// Represents a single file change.
/// </summary>
public sealed class FileChange
{
    public required string FilePath { get; init; }
    public required FileChangeType ChangeType { get; init; }
    public string? OldPath { get; init; } // For renames
    public DateTime ChangedAt { get; init; } = DateTime.UtcNow;
}

/// <summary>
/// Type of file change detected.
/// </summary>
public enum FileChangeType
{
    Created,
    Modified,
    Deleted,
    Renamed
}
```

### File: `src/SeniorIntern.Core/Interfaces/IGitignoreParser.cs`

```csharp
using System.Collections.Generic;

namespace SeniorIntern.Core.Interfaces;

/// <summary>
/// Parser for .gitignore files.
/// </summary>
public interface IGitignoreParser
{
    /// <summary>
    /// Load gitignore patterns from a file.
    /// </summary>
    /// <param name="gitignorePath">Path to the .gitignore file.</param>
    /// <returns>Parser instance with loaded patterns.</returns>
    IGitignoreMatcher LoadFromFile(string gitignorePath);

    /// <summary>
    /// Load gitignore patterns from content.
    /// </summary>
    /// <param name="content">Gitignore file content.</param>
    /// <param name="basePath">Base path for relative pattern matching.</param>
    /// <returns>Parser instance with loaded patterns.</returns>
    IGitignoreMatcher LoadFromContent(string content, string basePath);

    /// <summary>
    /// Load all gitignore files from a directory tree.
    /// </summary>
    /// <param name="rootPath">Root directory to search from.</param>
    /// <returns>Combined matcher for all found gitignore files.</returns>
    IGitignoreMatcher LoadFromDirectory(string rootPath);
}

/// <summary>
/// Matcher for gitignore patterns.
/// </summary>
public interface IGitignoreMatcher
{
    /// <summary>
    /// Number of patterns loaded.
    /// </summary>
    int PatternCount { get; }

    /// <summary>
    /// Check if a path should be ignored.
    /// </summary>
    /// <param name="relativePath">Relative path from the repository root.</param>
    /// <param name="isDirectory">Whether the path is a directory.</param>
    /// <returns>True if the path should be ignored.</returns>
    bool IsIgnored(string relativePath, bool isDirectory = false);

    /// <summary>
    /// Filter a list of paths, removing ignored ones.
    /// </summary>
    /// <param name="relativePaths">Paths to filter.</param>
    /// <returns>Paths that are not ignored.</returns>
    IEnumerable<string> Filter(IEnumerable<string> relativePaths);

    /// <summary>
    /// Get all patterns as strings.
    /// </summary>
    IReadOnlyList<string> GetPatterns();
}
```

---

## v0.7.3b: Indexing Models & Options

### Objective
Define all the models used for indexing operations, including options, progress, results, and job tracking.

### File: `src/SeniorIntern.Core/Models/IndexingOptions.cs`

```csharp
using System;
using System.Collections.Generic;

namespace SeniorIntern.Core.Models;

/// <summary>
/// Configuration options for indexing operations.
/// </summary>
public sealed class IndexingOptions
{
    /// <summary>
    /// Human-readable name for the index.
    /// </summary>
    public string IndexName { get; init; } = "Default";

    /// <summary>
    /// Path to the embedding model to use.
    /// If empty, uses the default loaded model.
    /// </summary>
    public string? EmbeddingModelPath { get; init; }

    /// <summary>
    /// Chunking configuration for text processing.
    /// </summary>
    public ChunkingOptions ChunkingOptions { get; init; } = new();

    /// <summary>
    /// Glob patterns for files to include.
    /// </summary>
    public IReadOnlyList<string> IncludePatterns { get; init; } = DefaultIncludePatterns;

    /// <summary>
    /// Glob patterns for files to exclude.
    /// </summary>
    public IReadOnlyList<string> ExcludePatterns { get; init; } = DefaultExcludePatterns;

    /// <summary>
    /// Maximum file size to index in kilobytes.
    /// Files larger than this are skipped.
    /// </summary>
    public int MaxFileSizeKb { get; init; } = 1024;

    /// <summary>
    /// Minimum file size to index in bytes.
    /// Very small files are often not useful.
    /// </summary>
    public int MinFileSizeBytes { get; init; } = 10;

    /// <summary>
    /// Number of parallel file processing tasks.
    /// </summary>
    public int ParallelProcessing { get; init; } = 4;

    /// <summary>
    /// Batch size for embedding generation.
    /// </summary>
    public int EmbeddingBatchSize { get; init; } = 32;

    /// <summary>
    /// Whether to force re-index all files, ignoring change detection.
    /// </summary>
    public bool ForceReindex { get; init; } = false;

    /// <summary>
    /// Whether to respect .gitignore rules.
    /// </summary>
    public bool RespectGitignore { get; init; } = true;

    /// <summary>
    /// Whether to index hidden files (starting with .).
    /// </summary>
    public bool IndexHiddenFiles { get; init; } = false;

    /// <summary>
    /// Whether to follow symbolic links.
    /// </summary>
    public bool FollowSymlinks { get; init; } = false;

    /// <summary>
    /// Maximum directory depth to traverse (-1 for unlimited).
    /// </summary>
    public int MaxDepth { get; init; } = -1;

    /// <summary>
    /// Whether to continue on errors instead of stopping.
    /// </summary>
    public bool ContinueOnError { get; init; } = true;

    /// <summary>
    /// Maximum number of errors before stopping (0 for unlimited).
    /// </summary>
    public int MaxErrors { get; init; } = 100;

    /// <summary>
    /// Priority for the indexing job (higher = more important).
    /// </summary>
    public int Priority { get; init; } = 0;

    /// <summary>
    /// Whether to enable file watching after indexing.
    /// </summary>
    public bool EnableFileWatching { get; init; } = true;

    /// <summary>
    /// Delay before file watching triggers re-indexing.
    /// </summary>
    public int FileWatchDebounceMs { get; init; } = 2000;

    /// <summary>
    /// Default include patterns for source code files.
    /// </summary>
    public static readonly IReadOnlyList<string> DefaultIncludePatterns = new[]
    {
        // .NET
        "**/*.cs", "**/*.fs", "**/*.vb", "**/*.csproj", "**/*.fsproj", "**/*.sln",
        // Web/JavaScript
        "**/*.ts", "**/*.tsx", "**/*.js", "**/*.jsx", "**/*.vue", "**/*.svelte",
        "**/*.html", "**/*.css", "**/*.scss", "**/*.less", "**/*.sass",
        // Python
        "**/*.py", "**/*.pyi", "**/*.pyx", "**/*.pxd",
        // JVM
        "**/*.java", "**/*.kt", "**/*.scala", "**/*.groovy", "**/*.gradle",
        // Systems
        "**/*.go", "**/*.rs", "**/*.cpp", "**/*.c", "**/*.h", "**/*.hpp", "**/*.cc",
        // Other languages
        "**/*.swift", "**/*.rb", "**/*.php", "**/*.lua", "**/*.r", "**/*.R",
        // Config/Docs
        "**/*.md", "**/*.json", "**/*.yaml", "**/*.yml", "**/*.xml", "**/*.toml",
        "**/*.ini", "**/*.cfg", "**/*.conf",
        // Shell
        "**/*.sh", "**/*.bash", "**/*.zsh", "**/*.ps1", "**/*.bat", "**/*.cmd"
    };

    /// <summary>
    /// Default exclude patterns for build artifacts and dependencies.
    /// </summary>
    public static readonly IReadOnlyList<string> DefaultExcludePatterns = new[]
    {
        // Build outputs
        "**/node_modules/**", "**/bin/**", "**/obj/**", "**/dist/**", "**/build/**",
        "**/target/**", "**/out/**", "**/.next/**", "**/.nuxt/**", "**/coverage/**",
        // Package managers
        "**/packages/**", "**/vendor/**", "**/.npm/**", "**/.yarn/**", "**/bower_components/**",
        // Version control
        "**/.git/**", "**/.svn/**", "**/.hg/**", "**/.bzr/**",
        // IDE
        "**/.vs/**", "**/.idea/**", "**/.vscode/**", "**/*.suo", "**/*.user",
        // Minified/generated
        "**/*.min.js", "**/*.min.css", "**/*.map", "**/*.d.ts", "**/*.generated.*",
        // Lock files (often large)
        "**/*.lock", "**/package-lock.json", "**/yarn.lock", "**/pnpm-lock.yaml",
        "**/Cargo.lock", "**/poetry.lock", "**/Gemfile.lock",
        // Temporary
        "**/tmp/**", "**/temp/**", "**/.cache/**", "**/__pycache__/**",
        // Logs
        "**/logs/**", "**/*.log",
        // Secrets (should never be indexed)
        "**/.env", "**/.env.*", "**/secrets.*", "**/credentials.*"
    };

    /// <summary>
    /// Create options for a quick index (minimal processing).
    /// </summary>
    public static IndexingOptions QuickIndex => new()
    {
        ParallelProcessing = 8,
        EmbeddingBatchSize = 64,
        ChunkingOptions = new ChunkingOptions
        {
            TargetChunkSize = 256,
            ChunkOverlap = 32
        }
    };

    /// <summary>
    /// Create options for a thorough index (maximum quality).
    /// </summary>
    public static IndexingOptions ThoroughIndex => new()
    {
        ParallelProcessing = 2,
        EmbeddingBatchSize = 16,
        ChunkingOptions = new ChunkingOptions
        {
            TargetChunkSize = 512,
            ChunkOverlap = 128,
            IncludeLineNumbers = true
        }
    };
}
```

### File: `src/SeniorIntern.Core/Models/IndexingProgress.cs`

```csharp
using System;

namespace SeniorIntern.Core.Models;

/// <summary>
/// Progress information for indexing operations.
/// </summary>
public sealed class IndexingProgress
{
    /// <summary>
    /// Current phase of the indexing operation.
    /// </summary>
    public IndexingPhase Phase { get; init; }

    /// <summary>
    /// Total number of files to process.
    /// </summary>
    public int TotalFiles { get; init; }

    /// <summary>
    /// Number of files processed so far.
    /// </summary>
    public int ProcessedFiles { get; init; }

    /// <summary>
    /// Number of files skipped (unchanged or filtered).
    /// </summary>
    public int SkippedFiles { get; init; }

    /// <summary>
    /// Number of files that failed to process.
    /// </summary>
    public int FailedFiles { get; init; }

    /// <summary>
    /// Total number of chunks created.
    /// </summary>
    public int TotalChunks { get; init; }

    /// <summary>
    /// Number of chunks processed (embedded and stored).
    /// </summary>
    public int ProcessedChunks { get; init; }

    /// <summary>
    /// Current file being processed.
    /// </summary>
    public string? CurrentFile { get; init; }

    /// <summary>
    /// Human-readable status message.
    /// </summary>
    public string? Message { get; init; }

    /// <summary>
    /// Overall completion percentage (0-100).
    /// </summary>
    public double PercentComplete { get; init; }

    /// <summary>
    /// Time elapsed since indexing started.
    /// </summary>
    public TimeSpan Elapsed { get; init; }

    /// <summary>
    /// Estimated time remaining (if calculable).
    /// </summary>
    public TimeSpan? EstimatedRemaining { get; init; }

    /// <summary>
    /// Processing rate in files per second.
    /// </summary>
    public double FilesPerSecond { get; init; }

    /// <summary>
    /// Processing rate in chunks per second.
    /// </summary>
    public double ChunksPerSecond { get; init; }

    /// <summary>
    /// Bytes processed so far.
    /// </summary>
    public long BytesProcessed { get; init; }

    /// <summary>
    /// Total bytes to process.
    /// </summary>
    public long TotalBytes { get; init; }

    /// <summary>
    /// Whether the operation can be paused.
    /// </summary>
    public bool CanPause { get; init; } = true;

    /// <summary>
    /// Whether the operation can be cancelled.
    /// </summary>
    public bool CanCancel { get; init; } = true;

    /// <summary>
    /// Create an initial progress for scanning phase.
    /// </summary>
    public static IndexingProgress Scanning(string? message = null) => new()
    {
        Phase = IndexingPhase.Scanning,
        Message = message ?? "Scanning workspace for files..."
    };

    /// <summary>
    /// Create progress for completion.
    /// </summary>
    public static IndexingProgress Complete(int files, int chunks, TimeSpan elapsed) => new()
    {
        Phase = IndexingPhase.Complete,
        TotalFiles = files,
        ProcessedFiles = files,
        TotalChunks = chunks,
        ProcessedChunks = chunks,
        PercentComplete = 100,
        Elapsed = elapsed,
        Message = $"Indexed {files} files with {chunks} chunks"
    };
}

/// <summary>
/// Phases of the indexing operation.
/// </summary>
public enum IndexingPhase
{
    /// <summary>
    /// Initializing the indexing operation.
    /// </summary>
    Initializing,

    /// <summary>
    /// Scanning filesystem for files to index.
    /// </summary>
    Scanning,

    /// <summary>
    /// Analyzing files to determine which need indexing.
    /// </summary>
    Analyzing,

    /// <summary>
    /// Reading and chunking file contents.
    /// </summary>
    Chunking,

    /// <summary>
    /// Generating embeddings for chunks.
    /// </summary>
    Embedding,

    /// <summary>
    /// Storing embeddings in vector database.
    /// </summary>
    Storing,

    /// <summary>
    /// Finalizing the index (cleanup, statistics).
    /// </summary>
    Finalizing,

    /// <summary>
    /// Indexing completed successfully.
    /// </summary>
    Complete,

    /// <summary>
    /// Indexing was cancelled by user.
    /// </summary>
    Cancelled,

    /// <summary>
    /// Indexing paused by user.
    /// </summary>
    Paused,

    /// <summary>
    /// Indexing failed with an error.
    /// </summary>
    Error
}
```

### File: `src/SeniorIntern.Core/Models/IndexingResult.cs`

```csharp
using System;
using System.Collections.Generic;

namespace SeniorIntern.Core.Models;

/// <summary>
/// Result of an indexing operation.
/// </summary>
public sealed class IndexingResult
{
    /// <summary>
    /// Whether the operation completed successfully.
    /// </summary>
    public bool Success { get; init; }

    /// <summary>
    /// ID of the index that was created or updated.
    /// </summary>
    public string IndexId { get; init; } = string.Empty;

    /// <summary>
    /// Number of files successfully indexed.
    /// </summary>
    public int FilesIndexed { get; init; }

    /// <summary>
    /// Number of files skipped (unchanged or filtered).
    /// </summary>
    public int FilesSkipped { get; init; }

    /// <summary>
    /// Number of files that failed to index.
    /// </summary>
    public int FilesErrored { get; init; }

    /// <summary>
    /// Total number of chunks created.
    /// </summary>
    public int ChunksCreated { get; init; }

    /// <summary>
    /// Total number of chunks removed (for re-indexing).
    /// </summary>
    public int ChunksRemoved { get; init; }

    /// <summary>
    /// Total bytes processed.
    /// </summary>
    public long BytesProcessed { get; init; }

    /// <summary>
    /// Total duration of the operation.
    /// </summary>
    public TimeSpan Duration { get; init; }

    /// <summary>
    /// When the operation started.
    /// </summary>
    public DateTime StartedAt { get; init; }

    /// <summary>
    /// When the operation completed.
    /// </summary>
    public DateTime CompletedAt { get; init; }

    /// <summary>
    /// List of errors that occurred during indexing.
    /// </summary>
    public IReadOnlyList<IndexingError> Errors { get; init; } = [];

    /// <summary>
    /// Top-level error message if the operation failed.
    /// </summary>
    public string? ErrorMessage { get; init; }

    /// <summary>
    /// Whether the operation was cancelled.
    /// </summary>
    public bool WasCancelled { get; init; }

    /// <summary>
    /// Files per second processing rate.
    /// </summary>
    public double FilesPerSecond => Duration.TotalSeconds > 0
        ? FilesIndexed / Duration.TotalSeconds
        : 0;

    /// <summary>
    /// Chunks per second processing rate.
    /// </summary>
    public double ChunksPerSecond => Duration.TotalSeconds > 0
        ? ChunksCreated / Duration.TotalSeconds
        : 0;

    /// <summary>
    /// Create a successful result.
    /// </summary>
    public static IndexingResult Successful(
        string indexId,
        int filesIndexed,
        int chunksCreated,
        TimeSpan duration) => new()
    {
        Success = true,
        IndexId = indexId,
        FilesIndexed = filesIndexed,
        ChunksCreated = chunksCreated,
        Duration = duration,
        CompletedAt = DateTime.UtcNow
    };

    /// <summary>
    /// Create a failed result.
    /// </summary>
    public static IndexingResult Failed(
        string errorMessage,
        TimeSpan duration,
        IReadOnlyList<IndexingError>? errors = null) => new()
    {
        Success = false,
        ErrorMessage = errorMessage,
        Duration = duration,
        Errors = errors ?? [],
        CompletedAt = DateTime.UtcNow
    };

    /// <summary>
    /// Create a cancelled result.
    /// </summary>
    public static IndexingResult Cancelled(
        int filesIndexed,
        int chunksCreated,
        TimeSpan duration) => new()
    {
        Success = false,
        WasCancelled = true,
        FilesIndexed = filesIndexed,
        ChunksCreated = chunksCreated,
        Duration = duration,
        ErrorMessage = "Indexing was cancelled",
        CompletedAt = DateTime.UtcNow
    };
}

/// <summary>
/// Error that occurred during indexing.
/// </summary>
public sealed class IndexingError
{
    /// <summary>
    /// Path to the file that caused the error.
    /// </summary>
    public required string FilePath { get; init; }

    /// <summary>
    /// Error message.
    /// </summary>
    public required string Message { get; init; }

    /// <summary>
    /// Type of error.
    /// </summary>
    public IndexingErrorType Type { get; init; }

    /// <summary>
    /// Exception type if available.
    /// </summary>
    public string? ExceptionType { get; init; }

    /// <summary>
    /// Stack trace if available.
    /// </summary>
    public string? StackTrace { get; init; }

    /// <summary>
    /// When the error occurred.
    /// </summary>
    public DateTime OccurredAt { get; init; } = DateTime.UtcNow;

    /// <summary>
    /// Create from an exception.
    /// </summary>
    public static IndexingError FromException(string filePath, Exception ex) => new()
    {
        FilePath = filePath,
        Message = ex.Message,
        Type = ClassifyException(ex),
        ExceptionType = ex.GetType().Name,
        StackTrace = ex.StackTrace
    };

    private static IndexingErrorType ClassifyException(Exception ex) => ex switch
    {
        System.IO.IOException => IndexingErrorType.FileRead,
        System.IO.FileNotFoundException => IndexingErrorType.FileNotFound,
        System.IO.DirectoryNotFoundException => IndexingErrorType.FileNotFound,
        UnauthorizedAccessException => IndexingErrorType.PermissionDenied,
        System.Text.Json.JsonException => IndexingErrorType.Parsing,
        System.Text.DecoderFallbackException => IndexingErrorType.Encoding,
        OutOfMemoryException => IndexingErrorType.OutOfMemory,
        _ => IndexingErrorType.Unknown
    };
}

/// <summary>
/// Types of indexing errors.
/// </summary>
public enum IndexingErrorType
{
    /// <summary>
    /// Unknown error type.
    /// </summary>
    Unknown,

    /// <summary>
    /// Error reading file contents.
    /// </summary>
    FileRead,

    /// <summary>
    /// File not found.
    /// </summary>
    FileNotFound,

    /// <summary>
    /// Permission denied accessing file.
    /// </summary>
    PermissionDenied,

    /// <summary>
    /// Error parsing file contents.
    /// </summary>
    Parsing,

    /// <summary>
    /// Error with file encoding.
    /// </summary>
    Encoding,

    /// <summary>
    /// Error during chunking.
    /// </summary>
    Chunking,

    /// <summary>
    /// Error generating embeddings.
    /// </summary>
    Embedding,

    /// <summary>
    /// Error storing in vector database.
    /// </summary>
    Storage,

    /// <summary>
    /// Ran out of memory.
    /// </summary>
    OutOfMemory,

    /// <summary>
    /// Operation timed out.
    /// </summary>
    Timeout
}

/// <summary>
/// Result of a sync operation.
/// </summary>
public sealed class IndexSyncResult
{
    /// <summary>
    /// Whether the sync completed successfully.
    /// </summary>
    public bool Success { get; init; }

    /// <summary>
    /// Number of new files added.
    /// </summary>
    public int FilesAdded { get; init; }

    /// <summary>
    /// Number of files updated.
    /// </summary>
    public int FilesUpdated { get; init; }

    /// <summary>
    /// Number of files removed.
    /// </summary>
    public int FilesRemoved { get; init; }

    /// <summary>
    /// Number of files unchanged.
    /// </summary>
    public int FilesUnchanged { get; init; }

    /// <summary>
    /// Net change in chunk count.
    /// </summary>
    public int ChunksDelta { get; init; }

    /// <summary>
    /// Duration of the sync operation.
    /// </summary>
    public TimeSpan Duration { get; init; }

    /// <summary>
    /// List of errors that occurred.
    /// </summary>
    public IReadOnlyList<IndexingError> Errors { get; init; } = [];
}
```

### File: `src/SeniorIntern.Core/Models/IndexingJob.cs`

```csharp
using System;

namespace SeniorIntern.Core.Models;

/// <summary>
/// Represents a queued or active indexing job.
/// </summary>
public sealed class IndexingJob
{
    /// <summary>
    /// Unique identifier for this job.
    /// </summary>
    public string Id { get; init; } = Guid.NewGuid().ToString();

    /// <summary>
    /// Type of indexing operation.
    /// </summary>
    public IndexingJobType Type { get; init; }

    /// <summary>
    /// ID of the target index (for updates).
    /// </summary>
    public string? IndexId { get; init; }

    /// <summary>
    /// Path to the workspace being indexed.
    /// </summary>
    public required string WorkspacePath { get; init; }

    /// <summary>
    /// Indexing options for this job.
    /// </summary>
    public required IndexingOptions Options { get; init; }

    /// <summary>
    /// Current status of the job.
    /// </summary>
    public IndexingJobStatus Status { get; set; } = IndexingJobStatus.Queued;

    /// <summary>
    /// Priority of the job (higher = more important).
    /// </summary>
    public int Priority { get; init; }

    /// <summary>
    /// When the job was created.
    /// </summary>
    public DateTime CreatedAt { get; init; } = DateTime.UtcNow;

    /// <summary>
    /// When the job started processing.
    /// </summary>
    public DateTime? StartedAt { get; set; }

    /// <summary>
    /// When the job completed.
    /// </summary>
    public DateTime? CompletedAt { get; set; }

    /// <summary>
    /// Current progress of the job.
    /// </summary>
    public IndexingProgress? Progress { get; set; }

    /// <summary>
    /// Result of the job (when completed).
    /// </summary>
    public IndexingResult? Result { get; set; }

    /// <summary>
    /// Error message if job failed.
    /// </summary>
    public string? ErrorMessage { get; set; }

    /// <summary>
    /// Number of retry attempts.
    /// </summary>
    public int RetryCount { get; set; }

    /// <summary>
    /// Maximum retry attempts allowed.
    /// </summary>
    public int MaxRetries { get; init; } = 3;

    /// <summary>
    /// How long the job has been running.
    /// </summary>
    public TimeSpan? RunningTime => StartedAt.HasValue
        ? (CompletedAt ?? DateTime.UtcNow) - StartedAt.Value
        : null;

    /// <summary>
    /// How long the job waited in queue.
    /// </summary>
    public TimeSpan? QueueTime => StartedAt.HasValue
        ? StartedAt.Value - CreatedAt
        : null;

    /// <summary>
    /// Whether the job can be retried.
    /// </summary>
    public bool CanRetry => RetryCount < MaxRetries &&
        (Status == IndexingJobStatus.Failed || Status == IndexingJobStatus.Cancelled);
}

/// <summary>
/// Types of indexing jobs.
/// </summary>
public enum IndexingJobType
{
    /// <summary>
    /// Full workspace indexing.
    /// </summary>
    FullIndex,

    /// <summary>
    /// Incremental update of existing index.
    /// </summary>
    IncrementalUpdate,

    /// <summary>
    /// Index specific files only.
    /// </summary>
    PartialIndex,

    /// <summary>
    /// Sync with filesystem (add/update/remove).
    /// </summary>
    Sync,

    /// <summary>
    /// Re-index triggered by file watcher.
    /// </summary>
    FileWatchUpdate
}

/// <summary>
/// Status of an indexing job.
/// </summary>
public enum IndexingJobStatus
{
    /// <summary>
    /// Job is waiting in queue.
    /// </summary>
    Queued,

    /// <summary>
    /// Job is currently running.
    /// </summary>
    Running,

    /// <summary>
    /// Job is paused.
    /// </summary>
    Paused,

    /// <summary>
    /// Job completed successfully.
    /// </summary>
    Completed,

    /// <summary>
    /// Job failed with errors.
    /// </summary>
    Failed,

    /// <summary>
    /// Job was cancelled.
    /// </summary>
    Cancelled
}
```

---

## v0.7.3c: Indexing Service Implementation

### Objective
Implement the main `IndexingService` class that orchestrates the entire indexing pipeline.

### File: `src/SeniorIntern.Core/Options/IndexingOptions.cs`

```csharp
namespace SeniorIntern.Core.Options;

/// <summary>
/// Application configuration for the indexing service.
/// </summary>
public sealed class IndexingServiceOptions
{
    /// <summary>
    /// Configuration section name.
    /// </summary>
    public const string SectionName = "Indexing";

    /// <summary>
    /// Maximum concurrent indexing jobs.
    /// </summary>
    public int MaxConcurrentJobs { get; set; } = 1;

    /// <summary>
    /// Default parallel file processing count.
    /// </summary>
    public int DefaultParallelism { get; set; } = 4;

    /// <summary>
    /// Default embedding batch size.
    /// </summary>
    public int DefaultBatchSize { get; set; } = 32;

    /// <summary>
    /// How often to report progress (milliseconds).
    /// </summary>
    public int ProgressReportIntervalMs { get; set; } = 500;

    /// <summary>
    /// Whether to enable background job processing.
    /// </summary>
    public bool EnableBackgroundProcessing { get; set; } = true;

    /// <summary>
    /// Whether to enable file watching by default.
    /// </summary>
    public bool EnableFileWatchingByDefault { get; set; } = true;

    /// <summary>
    /// Maximum jobs to keep in history.
    /// </summary>
    public int MaxJobHistory { get; set; } = 100;

    /// <summary>
    /// Timeout for individual file processing (seconds).
    /// </summary>
    public int FileProcessingTimeoutSeconds { get; set; } = 60;

    /// <summary>
    /// Whether to automatically retry failed files.
    /// </summary>
    public bool AutoRetryFailedFiles { get; set; } = true;

    /// <summary>
    /// Maximum retries per file.
    /// </summary>
    public int MaxRetriesPerFile { get; set; } = 2;
}
```

### File: `src/SeniorIntern.Services/Indexing/IndexingService.cs`

```csharp
using System;
using System.Collections.Concurrent;
using System.Collections.Generic;
using System.Diagnostics;
using System.IO;
using System.Linq;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.Extensions.Logging;
using Microsoft.Extensions.Options;
using SeniorIntern.Core.Interfaces;
using SeniorIntern.Core.Models;
using SeniorIntern.Core.Options;

namespace SeniorIntern.Services.Indexing;

/// <summary>
/// Main indexing service that orchestrates the indexing pipeline.
/// </summary>
public sealed class IndexingService : IIndexingService, IAsyncDisposable
{
    private readonly IEmbeddingService _embeddingService;
    private readonly IChunkingService _chunkingService;
    private readonly IVectorStore _vectorStore;
    private readonly IIndexingJobQueue _jobQueue;
    private readonly IGitignoreParser _gitignoreParser;
    private readonly IndexingServiceOptions _options;
    private readonly ILogger<IndexingService> _logger;

    private readonly ConcurrentDictionary<string, IndexingJob> _activeJobs = new();
    private readonly ConcurrentDictionary<string, IndexingJob> _jobHistory = new();
    private readonly SemaphoreSlim _indexingLock = new(1, 1);

    private CancellationTokenSource? _currentJobCts;
    private IndexingStatus _status = IndexingStatus.Idle;
    private IndexingJob? _currentJob;
    private bool _isPaused;
    private bool _isDisposed;

    public IndexingStatus Status => _status;
    public bool IsIndexing => _status is IndexingStatus.Indexing or IndexingStatus.Scanning;
    public IndexingJob? CurrentJob => _currentJob;

    public event EventHandler<IndexingStateChangedEventArgs>? StateChanged;
    public event EventHandler<FileIndexedEventArgs>? FileIndexed;
    public event EventHandler<FileIndexingErrorEventArgs>? FileError;
    public event EventHandler<IndexingJobEventArgs>? JobStarted;
    public event EventHandler<IndexingJobCompletedEventArgs>? JobCompleted;
    public event EventHandler<IndexingProgressEventArgs>? ProgressUpdated;

    public IndexingService(
        IEmbeddingService embeddingService,
        IChunkingService chunkingService,
        IVectorStore vectorStore,
        IIndexingJobQueue jobQueue,
        IGitignoreParser gitignoreParser,
        IOptions<IndexingServiceOptions> options,
        ILogger<IndexingService> logger)
    {
        _embeddingService = embeddingService;
        _chunkingService = chunkingService;
        _vectorStore = vectorStore;
        _jobQueue = jobQueue;
        _gitignoreParser = gitignoreParser;
        _options = options.Value;
        _logger = logger;
    }

    public async Task<IndexingResult> IndexWorkspaceAsync(
        string workspacePath,
        IndexingOptions options,
        IProgress<IndexingProgress>? progress = null,
        CancellationToken ct = default)
    {
        if (!await _indexingLock.WaitAsync(0, ct))
        {
            throw new InvalidOperationException("Another indexing operation is in progress");
        }

        var job = new IndexingJob
        {
            Type = IndexingJobType.FullIndex,
            WorkspacePath = workspacePath,
            Options = options,
            Status = IndexingJobStatus.Running,
            StartedAt = DateTime.UtcNow
        };

        _currentJob = job;
        _currentJobCts = CancellationTokenSource.CreateLinkedTokenSource(ct);
        var stopwatch = Stopwatch.StartNew();
        var errors = new ConcurrentBag<IndexingError>();

        try
        {
            _logger.LogInformation("Starting workspace indexing: {Path}", workspacePath);
            SetStatus(IndexingStatus.Scanning);
            JobStarted?.Invoke(this, new IndexingJobEventArgs { Job = job });

            // Phase 1: Scan files
            var scanProgress = new Progress<IndexingProgress>(p =>
            {
                job.Progress = p;
                progress?.Report(p);
                ProgressUpdated?.Invoke(this, new IndexingProgressEventArgs { Progress = p });
            });

            progress?.Report(IndexingProgress.Scanning());
            var files = await ScanFilesAsync(workspacePath, options, scanProgress, _currentJobCts.Token);

            _logger.LogInformation("Found {Count} files to index", files.Count);

            // Phase 2: Create or get index
            var index = await GetOrCreateIndexAsync(workspacePath, options, _currentJobCts.Token);
            job = job with { IndexId = index.Id };

            // Phase 3: Filter to changed files if not forcing
            var filesToIndex = files;
            if (!options.ForceReindex)
            {
                SetStatus(IndexingStatus.Scanning);
                progress?.Report(new IndexingProgress
                {
                    Phase = IndexingPhase.Analyzing,
                    Message = "Detecting changed files..."
                });

                filesToIndex = await FilterChangedFilesAsync(index.Id, files, _currentJobCts.Token);
                _logger.LogInformation(
                    "After filtering: {Changed} files to index, {Skipped} unchanged",
                    filesToIndex.Count, files.Count - filesToIndex.Count);
            }

            // Phase 4: Process files
            SetStatus(IndexingStatus.Indexing);
            var result = await ProcessFilesAsync(
                index.Id,
                workspacePath,
                filesToIndex,
                options,
                errors,
                progress,
                _currentJobCts.Token);

            // Phase 5: Finalize
            progress?.Report(new IndexingProgress
            {
                Phase = IndexingPhase.Finalizing,
                Message = "Finalizing index..."
            });

            await FinalizeIndexAsync(index.Id, result, _currentJobCts.Token);

            stopwatch.Stop();

            var finalResult = new IndexingResult
            {
                Success = true,
                IndexId = index.Id,
                FilesIndexed = result.FilesIndexed,
                FilesSkipped = files.Count - filesToIndex.Count,
                FilesErrored = errors.Count,
                ChunksCreated = result.ChunksCreated,
                BytesProcessed = result.BytesProcessed,
                Duration = stopwatch.Elapsed,
                StartedAt = job.StartedAt ?? DateTime.UtcNow,
                CompletedAt = DateTime.UtcNow,
                Errors = errors.ToList()
            };

            job.Result = finalResult;
            job.Status = IndexingJobStatus.Completed;
            job.CompletedAt = DateTime.UtcNow;

            progress?.Report(IndexingProgress.Complete(
                finalResult.FilesIndexed,
                finalResult.ChunksCreated,
                stopwatch.Elapsed));

            JobCompleted?.Invoke(this, new IndexingJobCompletedEventArgs
            {
                Job = job,
                Result = finalResult
            });

            _logger.LogInformation(
                "Indexing complete: {Files} files, {Chunks} chunks in {Duration}",
                finalResult.FilesIndexed, finalResult.ChunksCreated, stopwatch.Elapsed);

            return finalResult;
        }
        catch (OperationCanceledException)
        {
            stopwatch.Stop();
            job.Status = IndexingJobStatus.Cancelled;
            job.CompletedAt = DateTime.UtcNow;

            var result = IndexingResult.Cancelled(
                job.Progress?.ProcessedFiles ?? 0,
                job.Progress?.ProcessedChunks ?? 0,
                stopwatch.Elapsed);

            job.Result = result;
            JobCompleted?.Invoke(this, new IndexingJobCompletedEventArgs { Job = job, Result = result });

            _logger.LogInformation("Indexing cancelled after {Duration}", stopwatch.Elapsed);
            return result;
        }
        catch (Exception ex)
        {
            stopwatch.Stop();
            job.Status = IndexingJobStatus.Failed;
            job.CompletedAt = DateTime.UtcNow;
            job.ErrorMessage = ex.Message;

            var result = IndexingResult.Failed(ex.Message, stopwatch.Elapsed, errors.ToList());
            job.Result = result;

            JobCompleted?.Invoke(this, new IndexingJobCompletedEventArgs { Job = job, Result = result });

            _logger.LogError(ex, "Indexing failed after {Duration}", stopwatch.Elapsed);
            return result;
        }
        finally
        {
            _currentJob = null;
            _currentJobCts?.Dispose();
            _currentJobCts = null;
            SetStatus(IndexingStatus.Idle);
            _indexingLock.Release();

            // Add to history
            _jobHistory.TryAdd(job.Id, job);
            TrimJobHistory();
        }
    }

    public IndexingJob QueueWorkspaceIndexing(string workspacePath, IndexingOptions options)
    {
        var job = new IndexingJob
        {
            Type = IndexingJobType.FullIndex,
            WorkspacePath = workspacePath,
            Options = options,
            Priority = options.Priority
        };

        _jobQueue.Enqueue(job);
        _activeJobs.TryAdd(job.Id, job);

        _logger.LogInformation("Queued indexing job {JobId} for {Path}", job.Id, workspacePath);
        return job;
    }

    public async Task<IndexingResult> UpdateIndexAsync(
        string indexId,
        IProgress<IndexingProgress>? progress = null,
        CancellationToken ct = default)
    {
        var index = await _vectorStore.GetIndexAsync(indexId, ct)
            ?? throw new ArgumentException($"Index not found: {indexId}");

        var options = index.Settings.ToIndexingOptions();
        options = options with { ForceReindex = false };

        return await IndexWorkspaceAsync(index.WorkspacePath, options, progress, ct);
    }

    public async Task<IndexingResult> IndexFilesAsync(
        string indexId,
        IEnumerable<string> filePaths,
        IProgress<IndexingProgress>? progress = null,
        CancellationToken ct = default)
    {
        var index = await _vectorStore.GetIndexAsync(indexId, ct)
            ?? throw new ArgumentException($"Index not found: {indexId}");

        var options = index.Settings.ToIndexingOptions();
        var files = filePaths.ToList();
        var errors = new ConcurrentBag<IndexingError>();
        var stopwatch = Stopwatch.StartNew();

        SetStatus(IndexingStatus.Indexing);

        try
        {
            var result = await ProcessFilesAsync(
                indexId,
                index.WorkspacePath,
                files,
                options,
                errors,
                progress,
                ct);

            stopwatch.Stop();

            return new IndexingResult
            {
                Success = true,
                IndexId = indexId,
                FilesIndexed = result.FilesIndexed,
                ChunksCreated = result.ChunksCreated,
                Duration = stopwatch.Elapsed,
                Errors = errors.ToList()
            };
        }
        finally
        {
            SetStatus(IndexingStatus.Idle);
        }
    }

    public async Task<int> ReindexFileAsync(
        string indexId,
        string filePath,
        CancellationToken ct = default)
    {
        // Remove old chunks
        await _vectorStore.RemoveChunksForFileAsync(indexId, filePath, ct);

        // Re-index
        var index = await _vectorStore.GetIndexAsync(indexId, ct)
            ?? throw new ArgumentException($"Index not found: {indexId}");

        var options = index.Settings.ToIndexingOptions();
        var fullPath = Path.Combine(index.WorkspacePath, filePath);

        return await ProcessSingleFileAsync(indexId, index.WorkspacePath, fullPath, options, ct);
    }

    public async Task<int> RemoveFilesAsync(
        string indexId,
        IEnumerable<string> filePaths,
        CancellationToken ct = default)
    {
        return await _vectorStore.RemoveChunksForFilesAsync(indexId, filePaths, ct);
    }

    public async Task<IndexSyncResult> SyncIndexAsync(
        string indexId,
        IProgress<IndexingProgress>? progress = null,
        CancellationToken ct = default)
    {
        var index = await _vectorStore.GetIndexAsync(indexId, ct)
            ?? throw new ArgumentException($"Index not found: {indexId}");

        var options = index.Settings.ToIndexingOptions();
        var stopwatch = Stopwatch.StartNew();
        var errors = new ConcurrentBag<IndexingError>();

        // Scan current files
        var currentFiles = await ScanFilesAsync(
            index.WorkspacePath, options, null, ct);

        // Compute hashes
        var fileHashes = new Dictionary<string, string>();
        foreach (var file in currentFiles)
        {
            var relativePath = Path.GetRelativePath(index.WorkspacePath, file);
            var hash = await ComputeFileHashAsync(file, ct);
            fileHashes[relativePath] = hash;
        }

        // Detect changes
        var changes = await _vectorStore.DetectFileChangesAsync(indexId, fileHashes, ct);

        var chunksAdded = 0;
        var chunksRemoved = 0;

        // Remove deleted files
        if (changes.DeletedFiles.Count > 0)
        {
            chunksRemoved = await _vectorStore.RemoveChunksForFilesAsync(
                indexId, changes.DeletedFiles, ct);
        }

        // Process added and modified files
        var filesToProcess = changes.AddedFiles.Concat(changes.ModifiedFiles).ToList();
        if (filesToProcess.Count > 0)
        {
            // Remove chunks for modified files first
            foreach (var file in changes.ModifiedFiles)
            {
                chunksRemoved += await _vectorStore.RemoveChunksForFileAsync(indexId, file, ct);
            }

            var result = await ProcessFilesAsync(
                indexId,
                index.WorkspacePath,
                filesToProcess.Select(f => Path.Combine(index.WorkspacePath, f)).ToList(),
                options,
                errors,
                progress,
                ct);

            chunksAdded = result.ChunksCreated;
        }

        stopwatch.Stop();

        return new IndexSyncResult
        {
            Success = true,
            FilesAdded = changes.AddedFiles.Count,
            FilesUpdated = changes.ModifiedFiles.Count,
            FilesRemoved = changes.DeletedFiles.Count,
            FilesUnchanged = changes.UnchangedFiles.Count,
            ChunksDelta = chunksAdded - chunksRemoved,
            Duration = stopwatch.Elapsed,
            Errors = errors.ToList()
        };
    }

    public void CancelIndexing()
    {
        if (_currentJobCts != null && !_currentJobCts.IsCancellationRequested)
        {
            _logger.LogInformation("Cancelling current indexing operation");
            SetStatus(IndexingStatus.Cancelling);
            _currentJobCts.Cancel();
        }
    }

    public void PauseIndexing()
    {
        if (IsIndexing && !_isPaused)
        {
            _isPaused = true;
            SetStatus(IndexingStatus.Paused);
            _logger.LogInformation("Indexing paused");
        }
    }

    public void ResumeIndexing()
    {
        if (_isPaused)
        {
            _isPaused = false;
            SetStatus(IndexingStatus.Indexing);
            _logger.LogInformation("Indexing resumed");
        }
    }

    public IndexingJob? GetJob(string jobId)
    {
        if (_activeJobs.TryGetValue(jobId, out var job))
            return job;
        if (_jobHistory.TryGetValue(jobId, out job))
            return job;
        return null;
    }

    public IReadOnlyList<IndexingJob> GetActiveJobs()
    {
        return _activeJobs.Values.ToList();
    }

    public bool CancelJob(string jobId)
    {
        if (_currentJob?.Id == jobId)
        {
            CancelIndexing();
            return true;
        }

        return _jobQueue.Remove(jobId);
    }

    private void SetStatus(IndexingStatus status)
    {
        if (_status != status)
        {
            var oldStatus = _status;
            _status = status;
            StateChanged?.Invoke(this, new IndexingStateChangedEventArgs
            {
                OldStatus = oldStatus,
                NewStatus = status
            });
        }
    }

    private void TrimJobHistory()
    {
        while (_jobHistory.Count > _options.MaxJobHistory)
        {
            var oldest = _jobHistory.Values
                .OrderBy(j => j.CompletedAt ?? j.CreatedAt)
                .FirstOrDefault();

            if (oldest != null)
            {
                _jobHistory.TryRemove(oldest.Id, out _);
            }
        }
    }

    public async ValueTask DisposeAsync()
    {
        if (_isDisposed) return;
        _isDisposed = true;

        CancelIndexing();
        _currentJobCts?.Dispose();
        _indexingLock.Dispose();
    }
}
```

### File: `src/SeniorIntern.Services/Indexing/IndexingService.Processing.cs`

```csharp
using System;
using System.Collections.Concurrent;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Security.Cryptography;
using System.Text;
using System.Threading;
using System.Threading.Tasks;
using SeniorIntern.Core.Models;
using SeniorIntern.Services.VectorStore;

namespace SeniorIntern.Services.Indexing;

public sealed partial class IndexingService
{
    private async Task<List<string>> ScanFilesAsync(
        string workspacePath,
        IndexingOptions options,
        IProgress<IndexingProgress>? progress,
        CancellationToken ct)
    {
        var scanner = new FileScanner(_logger.CreateLogger<FileScanner>());
        var settings = options.ToVectorIndexSettings();

        var scanProgress = new Progress<FileScanProgress>(p =>
        {
            progress?.Report(new IndexingProgress
            {
                Phase = IndexingPhase.Scanning,
                Message = $"Scanning: {p.CurrentPath}",
                ProcessedFiles = p.ScannedCount
            });
        });

        var files = await scanner.ScanAsync(workspacePath, settings, scanProgress, ct);
        return files.Select(f => f.AbsolutePath).ToList();
    }

    private async Task<VectorIndex> GetOrCreateIndexAsync(
        string workspacePath,
        IndexingOptions options,
        CancellationToken ct)
    {
        var existing = await _vectorStore.GetIndexForWorkspaceAsync(workspacePath, ct);
        if (existing != null)
        {
            return existing;
        }

        if (!_embeddingService.IsModelLoaded)
        {
            throw new InvalidOperationException("No embedding model loaded");
        }

        return await _vectorStore.CreateIndexAsync(
            workspacePath,
            options.IndexName,
            _embeddingService.EmbeddingDimension,
            options.ToVectorIndexSettings(),
            ct);
    }

    private async Task<List<string>> FilterChangedFilesAsync(
        string indexId,
        List<string> files,
        CancellationToken ct)
    {
        var changedFiles = new List<string>();
        var index = await _vectorStore.GetIndexAsync(indexId, ct);
        if (index == null) return files;

        foreach (var file in files)
        {
            ct.ThrowIfCancellationRequested();

            var relativePath = Path.GetRelativePath(index.WorkspacePath, file);
            var indexed = await _vectorStore.GetIndexedFileAsync(indexId, relativePath, ct);

            if (indexed == null)
            {
                // New file
                changedFiles.Add(file);
            }
            else
            {
                // Check if modified
                var fileInfo = new FileInfo(file);
                if (fileInfo.LastWriteTimeUtc > indexed.IndexedAt)
                {
                    changedFiles.Add(file);
                }
            }
        }

        return changedFiles;
    }

    private async Task<ProcessingResult> ProcessFilesAsync(
        string indexId,
        string workspacePath,
        List<string> files,
        IndexingOptions options,
        ConcurrentBag<IndexingError> errors,
        IProgress<IndexingProgress>? progress,
        CancellationToken ct)
    {
        var processedFiles = 0;
        var totalChunks = 0;
        long bytesProcessed = 0;
        var totalFiles = files.Count;

        using var semaphore = new SemaphoreSlim(options.ParallelProcessing);
        var tasks = new List<Task<int>>();

        var progressTimer = new System.Timers.Timer(_options.ProgressReportIntervalMs);
        progressTimer.Elapsed += (s, e) =>
        {
            if (_isPaused) return;

            progress?.Report(new IndexingProgress
            {
                Phase = IndexingPhase.Embedding,
                TotalFiles = totalFiles,
                ProcessedFiles = processedFiles,
                FailedFiles = errors.Count,
                TotalChunks = totalChunks,
                ProcessedChunks = totalChunks,
                PercentComplete = totalFiles > 0 ? (double)processedFiles / totalFiles * 100 : 0,
                BytesProcessed = bytesProcessed
            });
        };
        progressTimer.Start();

        try
        {
            foreach (var file in files)
            {
                ct.ThrowIfCancellationRequested();

                // Wait if paused
                while (_isPaused && !ct.IsCancellationRequested)
                {
                    await Task.Delay(100, ct);
                }

                await semaphore.WaitAsync(ct);

                var task = Task.Run(async () =>
                {
                    try
                    {
                        var chunks = await ProcessSingleFileAsync(
                            indexId, workspacePath, file, options, ct);

                        var fileInfo = new FileInfo(file);
                        Interlocked.Add(ref bytesProcessed, fileInfo.Length);
                        Interlocked.Increment(ref processedFiles);
                        Interlocked.Add(ref totalChunks, chunks);

                        var relativePath = Path.GetRelativePath(workspacePath, file);
                        FileIndexed?.Invoke(this, new FileIndexedEventArgs
                        {
                            FilePath = relativePath,
                            ChunkCount = chunks,
                            Success = true
                        });

                        return chunks;
                    }
                    catch (Exception ex)
                    {
                        var relativePath = Path.GetRelativePath(workspacePath, file);
                        var error = IndexingError.FromException(relativePath, ex);
                        errors.Add(error);

                        FileError?.Invoke(this, new FileIndexingErrorEventArgs
                        {
                            FilePath = relativePath,
                            Error = error
                        });

                        _logger.LogWarning(ex, "Error processing file: {File}", relativePath);

                        if (!options.ContinueOnError)
                            throw;

                        if (options.MaxErrors > 0 && errors.Count >= options.MaxErrors)
                            throw new InvalidOperationException($"Maximum error count ({options.MaxErrors}) exceeded");

                        return 0;
                    }
                    finally
                    {
                        semaphore.Release();
                    }
                }, ct);

                tasks.Add(task);
            }

            await Task.WhenAll(tasks);
        }
        finally
        {
            progressTimer.Stop();
            progressTimer.Dispose();
        }

        return new ProcessingResult
        {
            FilesIndexed = processedFiles,
            ChunksCreated = totalChunks,
            BytesProcessed = bytesProcessed
        };
    }

    private async Task<int> ProcessSingleFileAsync(
        string indexId,
        string workspacePath,
        string filePath,
        IndexingOptions options,
        CancellationToken ct)
    {
        var relativePath = Path.GetRelativePath(workspacePath, filePath);

        // Read file with encoding detection
        var fileContent = await FileContentReader.ReadAsync(filePath, ct);
        var fileInfo = new FileInfo(filePath);
        var fileHash = FileHasher.ComputeHash(fileContent.Content);

        // Chunk the content
        var chunks = _chunkingService.ChunkDocument(
            fileContent.Content,
            relativePath,
            options.ChunkingOptions);

        if (chunks.Count == 0)
        {
            // Still track the file even if no chunks
            await _vectorStore.UpsertIndexedFileAsync(indexId, new IndexedFile
            {
                IndexId = indexId,
                FilePath = relativePath,
                FileHash = fileHash,
                FileSize = fileInfo.Length,
                LastModified = fileInfo.LastWriteTimeUtc,
                ChunkCount = 0,
                Language = _chunkingService.DetectLanguage(relativePath),
                Encoding = fileContent.Encoding,
                LineCount = fileContent.LineCount,
                Status = FileIndexStatus.Indexed
            }, ct);

            return 0;
        }

        // Generate embeddings in batches
        var chunkContents = chunks.Select(c => c.Content).ToList();
        var embeddings = await _embeddingService.EmbedBatchAsync(chunkContents, null, ct);

        // Create indexed file record
        var indexedFile = new IndexedFile
        {
            IndexId = indexId,
            FilePath = relativePath,
            FileHash = fileHash,
            FileSize = fileInfo.Length,
            LastModified = fileInfo.LastWriteTimeUtc,
            ChunkCount = chunks.Count,
            Language = chunks.FirstOrDefault()?.Language ?? _chunkingService.DetectLanguage(relativePath),
            Encoding = fileContent.Encoding,
            LineCount = fileContent.LineCount,
            Status = FileIndexStatus.Indexed
        };

        await _vectorStore.UpsertIndexedFileAsync(indexId, indexedFile, ct);

        // Prepare chunks with embeddings
        var chunksWithEmbeddings = chunks.Zip(embeddings, (chunk, embedding) =>
            new ChunkWithEmbedding
            {
                Chunk = chunk,
                Embedding = embedding,
                FileId = indexedFile.Id
            }).ToList();

        // Store in vector database
        await _vectorStore.AddChunksAsync(indexId, chunksWithEmbeddings, ct);

        return chunks.Count;
    }

    private async Task FinalizeIndexAsync(
        string indexId,
        ProcessingResult result,
        CancellationToken ct)
    {
        var index = await _vectorStore.GetIndexAsync(indexId, ct);
        if (index != null)
        {
            index.ChunkCount = result.ChunksCreated;
            index.FileCount = result.FilesIndexed;
            index.TotalFileSizeBytes = result.BytesProcessed;
            index.UpdatedAt = DateTime.UtcNow;
            index.LastFullIndexAt = DateTime.UtcNow;

            await _vectorStore.UpdateIndexAsync(index, ct);
        }
    }

    private static async Task<string> ComputeFileHashAsync(string filePath, CancellationToken ct)
    {
        await using var stream = File.OpenRead(filePath);
        var hashBytes = await SHA256.HashDataAsync(stream, ct);
        return Convert.ToHexString(hashBytes).ToLowerInvariant();
    }

    private sealed class ProcessingResult
    {
        public int FilesIndexed { get; init; }
        public int ChunksCreated { get; init; }
        public long BytesProcessed { get; init; }
    }
}
```

---

## v0.7.3d: File Processing Pipeline

### Objective
Implement the detailed file processing pipeline including reading, chunking, embedding, and storing.

### File: `src/SeniorIntern.Services/Indexing/FileProcessor.cs`

```csharp
using System;
using System.Collections.Generic;
using System.Diagnostics;
using System.IO;
using System.Linq;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.Extensions.Logging;
using SeniorIntern.Core.Interfaces;
using SeniorIntern.Core.Models;
using SeniorIntern.Services.VectorStore;

namespace SeniorIntern.Services.Indexing;

/// <summary>
/// Processes individual files through the indexing pipeline.
/// </summary>
public sealed class FileProcessor
{
    private readonly IChunkingService _chunkingService;
    private readonly IEmbeddingService _embeddingService;
    private readonly IVectorStore _vectorStore;
    private readonly ILogger<FileProcessor> _logger;

    public FileProcessor(
        IChunkingService chunkingService,
        IEmbeddingService embeddingService,
        IVectorStore vectorStore,
        ILogger<FileProcessor> logger)
    {
        _chunkingService = chunkingService;
        _embeddingService = embeddingService;
        _vectorStore = vectorStore;
        _logger = logger;
    }

    /// <summary>
    /// Process a single file through the complete pipeline.
    /// </summary>
    public async Task<FileProcessingResult> ProcessFileAsync(
        FileProcessingContext context,
        CancellationToken ct = default)
    {
        var stopwatch = Stopwatch.StartNew();
        var result = new FileProcessingResult
        {
            FilePath = context.RelativePath,
            StartedAt = DateTime.UtcNow
        };

        try
        {
            // Step 1: Read file
            _logger.LogDebug("Reading file: {Path}", context.RelativePath);
            var fileContent = await FileContentReader.ReadAsync(context.AbsolutePath, ct);

            result.FileSize = new FileInfo(context.AbsolutePath).Length;
            result.LineCount = fileContent.LineCount;
            result.Encoding = fileContent.Encoding;

            // Step 2: Compute hash for change detection
            var fileHash = FileHasher.ComputeHash(fileContent.Content);

            // Step 3: Chunk the content
            _logger.LogDebug("Chunking file: {Path}", context.RelativePath);
            var chunks = _chunkingService.ChunkDocument(
                fileContent.Content,
                context.RelativePath,
                context.Options.ChunkingOptions);

            result.ChunkCount = chunks.Count;
            result.Language = chunks.FirstOrDefault()?.Language
                ?? _chunkingService.DetectLanguage(context.RelativePath);

            if (chunks.Count == 0)
            {
                _logger.LogDebug("No chunks created for: {Path}", context.RelativePath);
                result.Status = FileProcessingStatus.Skipped;
                result.SkipReason = "No content to chunk";
                return result;
            }

            // Step 4: Generate embeddings
            _logger.LogDebug("Generating embeddings for {Count} chunks", chunks.Count);
            var embeddingStopwatch = Stopwatch.StartNew();

            var embeddings = await _embeddingService.EmbedBatchAsync(
                chunks.Select(c => c.Content),
                null,
                ct);

            embeddingStopwatch.Stop();
            result.EmbeddingDurationMs = (int)embeddingStopwatch.ElapsedMilliseconds;

            // Step 5: Create indexed file record
            var indexedFile = new IndexedFile
            {
                IndexId = context.IndexId,
                FilePath = context.RelativePath,
                FileHash = fileHash,
                FileSize = result.FileSize,
                LastModified = new FileInfo(context.AbsolutePath).LastWriteTimeUtc,
                ChunkCount = chunks.Count,
                Language = result.Language,
                Encoding = result.Encoding,
                LineCount = result.LineCount,
                Status = FileIndexStatus.Indexed,
                IndexingDurationMs = (int)stopwatch.ElapsedMilliseconds
            };

            // Step 6: Store in database
            _logger.LogDebug("Storing {Count} chunks", chunks.Count);
            await _vectorStore.UpsertIndexedFileAsync(context.IndexId, indexedFile, ct);

            var chunksWithEmbeddings = chunks.Zip(embeddings, (chunk, embedding) =>
                new ChunkWithEmbedding
                {
                    Chunk = chunk,
                    Embedding = embedding,
                    FileId = indexedFile.Id
                }).ToList();

            await _vectorStore.AddChunksAsync(context.IndexId, chunksWithEmbeddings, ct);

            stopwatch.Stop();
            result.DurationMs = (int)stopwatch.ElapsedMilliseconds;
            result.Status = FileProcessingStatus.Success;

            _logger.LogDebug(
                "Processed {Path}: {Chunks} chunks in {Duration}ms",
                context.RelativePath, chunks.Count, result.DurationMs);

            return result;
        }
        catch (Exception ex)
        {
            stopwatch.Stop();
            result.DurationMs = (int)stopwatch.ElapsedMilliseconds;
            result.Status = FileProcessingStatus.Failed;
            result.Error = IndexingError.FromException(context.RelativePath, ex);

            _logger.LogWarning(ex, "Failed to process file: {Path}", context.RelativePath);
            return result;
        }
    }

    /// <summary>
    /// Process multiple files in parallel.
    /// </summary>
    public async Task<BatchProcessingResult> ProcessFilesAsync(
        IEnumerable<FileProcessingContext> contexts,
        int parallelism,
        IProgress<BatchProgress>? progress = null,
        CancellationToken ct = default)
    {
        var contextList = contexts.ToList();
        var results = new List<FileProcessingResult>();
        var processed = 0;
        var total = contextList.Count;

        using var semaphore = new SemaphoreSlim(parallelism);
        var tasks = new List<Task<FileProcessingResult>>();

        foreach (var context in contextList)
        {
            ct.ThrowIfCancellationRequested();

            await semaphore.WaitAsync(ct);

            var task = Task.Run(async () =>
            {
                try
                {
                    var result = await ProcessFileAsync(context, ct);
                    var current = Interlocked.Increment(ref processed);

                    progress?.Report(new BatchProgress
                    {
                        Processed = current,
                        Total = total,
                        CurrentFile = context.RelativePath,
                        PercentComplete = (double)current / total * 100
                    });

                    return result;
                }
                finally
                {
                    semaphore.Release();
                }
            }, ct);

            tasks.Add(task);
        }

        var taskResults = await Task.WhenAll(tasks);
        results.AddRange(taskResults);

        var succeeded = results.Count(r => r.Status == FileProcessingStatus.Success);
        var failed = results.Count(r => r.Status == FileProcessingStatus.Failed);
        var skipped = results.Count(r => r.Status == FileProcessingStatus.Skipped);

        return new BatchProcessingResult
        {
            Results = results,
            TotalFiles = total,
            SuccessCount = succeeded,
            FailedCount = failed,
            SkippedCount = skipped,
            TotalChunks = results.Sum(r => r.ChunkCount),
            TotalBytes = results.Sum(r => r.FileSize),
            TotalDurationMs = results.Sum(r => r.DurationMs)
        };
    }
}

/// <summary>
/// Context for processing a single file.
/// </summary>
public sealed class FileProcessingContext
{
    public required string IndexId { get; init; }
    public required string AbsolutePath { get; init; }
    public required string RelativePath { get; init; }
    public required IndexingOptions Options { get; init; }
}

/// <summary>
/// Result of processing a single file.
/// </summary>
public sealed class FileProcessingResult
{
    public required string FilePath { get; init; }
    public FileProcessingStatus Status { get; set; }
    public int ChunkCount { get; set; }
    public long FileSize { get; set; }
    public int LineCount { get; set; }
    public string? Language { get; set; }
    public string? Encoding { get; set; }
    public int DurationMs { get; set; }
    public int EmbeddingDurationMs { get; set; }
    public DateTime StartedAt { get; init; }
    public IndexingError? Error { get; set; }
    public string? SkipReason { get; set; }
}

/// <summary>
/// Status of file processing.
/// </summary>
public enum FileProcessingStatus
{
    Success,
    Failed,
    Skipped
}

/// <summary>
/// Result of batch file processing.
/// </summary>
public sealed class BatchProcessingResult
{
    public IReadOnlyList<FileProcessingResult> Results { get; init; } = [];
    public int TotalFiles { get; init; }
    public int SuccessCount { get; init; }
    public int FailedCount { get; init; }
    public int SkippedCount { get; init; }
    public int TotalChunks { get; init; }
    public long TotalBytes { get; init; }
    public int TotalDurationMs { get; init; }
}

/// <summary>
/// Progress for batch processing.
/// </summary>
public sealed class BatchProgress
{
    public int Processed { get; init; }
    public int Total { get; init; }
    public string? CurrentFile { get; init; }
    public double PercentComplete { get; init; }
}
```

### File: `src/SeniorIntern.Services/Indexing/EmbeddingBatcher.cs`

```csharp
using System;
using System.Collections.Generic;
using System.Linq;
using System.Threading;
using System.Threading.Tasks;
using SeniorIntern.Core.Interfaces;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Services.Indexing;

/// <summary>
/// Batches embedding requests for efficient processing.
/// </summary>
public sealed class EmbeddingBatcher
{
    private readonly IEmbeddingService _embeddingService;
    private readonly int _batchSize;

    public EmbeddingBatcher(IEmbeddingService embeddingService, int batchSize = 32)
    {
        _embeddingService = embeddingService;
        _batchSize = batchSize;
    }

    /// <summary>
    /// Embed chunks in optimized batches.
    /// </summary>
    public async Task<IReadOnlyList<ChunkWithEmbedding>> EmbedChunksAsync(
        IEnumerable<(TextChunk Chunk, string FileId)> chunks,
        IProgress<EmbeddingBatchProgress>? progress = null,
        CancellationToken ct = default)
    {
        var chunkList = chunks.ToList();
        var results = new List<ChunkWithEmbedding>(chunkList.Count);
        var processedCount = 0;

        // Process in batches
        foreach (var batch in chunkList.Chunk(_batchSize))
        {
            ct.ThrowIfCancellationRequested();

            var batchChunks = batch.ToList();
            var texts = batchChunks.Select(c => c.Chunk.Content).ToList();

            // Get embeddings for batch
            var embeddings = await _embeddingService.EmbedBatchAsync(texts, null, ct);

            // Pair chunks with embeddings
            for (int i = 0; i < batchChunks.Count; i++)
            {
                results.Add(new ChunkWithEmbedding
                {
                    Chunk = batchChunks[i].Chunk,
                    Embedding = embeddings[i],
                    FileId = batchChunks[i].FileId
                });
            }

            processedCount += batchChunks.Count;

            progress?.Report(new EmbeddingBatchProgress
            {
                ProcessedChunks = processedCount,
                TotalChunks = chunkList.Count,
                PercentComplete = (double)processedCount / chunkList.Count * 100
            });
        }

        return results;
    }

    /// <summary>
    /// Embed text content in optimized batches.
    /// </summary>
    public async Task<IReadOnlyList<float[]>> EmbedTextsAsync(
        IEnumerable<string> texts,
        IProgress<EmbeddingBatchProgress>? progress = null,
        CancellationToken ct = default)
    {
        var textList = texts.ToList();
        var results = new List<float[]>(textList.Count);
        var processedCount = 0;

        foreach (var batch in textList.Chunk(_batchSize))
        {
            ct.ThrowIfCancellationRequested();

            var batchTexts = batch.ToList();
            var embeddings = await _embeddingService.EmbedBatchAsync(batchTexts, null, ct);

            results.AddRange(embeddings);
            processedCount += batchTexts.Count;

            progress?.Report(new EmbeddingBatchProgress
            {
                ProcessedChunks = processedCount,
                TotalChunks = textList.Count,
                PercentComplete = (double)processedCount / textList.Count * 100
            });
        }

        return results;
    }
}

/// <summary>
/// Progress for embedding batch operations.
/// </summary>
public sealed class EmbeddingBatchProgress
{
    public int ProcessedChunks { get; init; }
    public int TotalChunks { get; init; }
    public double PercentComplete { get; init; }
}
```

### File: `src/SeniorIntern.Services/Indexing/IndexingExtensions.cs`

```csharp
using SeniorIntern.Core.Models;

namespace SeniorIntern.Services.Indexing;

/// <summary>
/// Extension methods for indexing types.
/// </summary>
public static class IndexingExtensions
{
    /// <summary>
    /// Convert IndexingOptions to VectorIndexSettings.
    /// </summary>
    public static VectorIndexSettings ToVectorIndexSettings(this IndexingOptions options)
    {
        return new VectorIndexSettings
        {
            ChunkingOptions = options.ChunkingOptions,
            IncludePatterns = options.IncludePatterns,
            ExcludePatterns = options.ExcludePatterns,
            MaxFileSizeKb = options.MaxFileSizeKb,
            MinFileSizeBytes = options.MinFileSizeBytes,
            IndexHiddenFiles = options.IndexHiddenFiles,
            RespectGitignore = options.RespectGitignore,
            FollowSymlinks = options.FollowSymlinks,
            MaxDepth = options.MaxDepth,
            AutoReindex = options.EnableFileWatching,
            AutoReindexDelayMs = options.FileWatchDebounceMs,
            ParallelProcessing = options.ParallelProcessing
        };
    }

    /// <summary>
    /// Convert VectorIndexSettings to IndexingOptions.
    /// </summary>
    public static IndexingOptions ToIndexingOptions(this VectorIndexSettings settings)
    {
        return new IndexingOptions
        {
            ChunkingOptions = settings.ChunkingOptions,
            IncludePatterns = settings.IncludePatterns,
            ExcludePatterns = settings.ExcludePatterns,
            MaxFileSizeKb = settings.MaxFileSizeKb,
            MinFileSizeBytes = settings.MinFileSizeBytes,
            IndexHiddenFiles = settings.IndexHiddenFiles,
            RespectGitignore = settings.RespectGitignore,
            FollowSymlinks = settings.FollowSymlinks,
            MaxDepth = settings.MaxDepth,
            EnableFileWatching = settings.AutoReindex,
            FileWatchDebounceMs = settings.AutoReindexDelayMs,
            ParallelProcessing = settings.ParallelProcessing
        };
    }
}
```

---

## v0.7.3e: Gitignore Pattern Matching

### Objective
Implement gitignore parsing and pattern matching to respect .gitignore rules during indexing.

### File: `src/SeniorIntern.Services/Indexing/GitignoreParser.cs`

```csharp
using System;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Text.RegularExpressions;
using SeniorIntern.Core.Interfaces;

namespace SeniorIntern.Services.Indexing;

/// <summary>
/// Parser for .gitignore files.
/// </summary>
public sealed class GitignoreParser : IGitignoreParser
{
    public IGitignoreMatcher LoadFromFile(string gitignorePath)
    {
        if (!File.Exists(gitignorePath))
            return new GitignoreMatcher([], Path.GetDirectoryName(gitignorePath) ?? "");

        var content = File.ReadAllText(gitignorePath);
        var basePath = Path.GetDirectoryName(gitignorePath) ?? "";
        return LoadFromContent(content, basePath);
    }

    public IGitignoreMatcher LoadFromContent(string content, string basePath)
    {
        var patterns = ParsePatterns(content);
        return new GitignoreMatcher(patterns, basePath);
    }

    public IGitignoreMatcher LoadFromDirectory(string rootPath)
    {
        var allPatterns = new List<GitignorePattern>();

        // Find all .gitignore files
        var gitignoreFiles = Directory.EnumerateFiles(
            rootPath,
            ".gitignore",
            SearchOption.AllDirectories);

        foreach (var file in gitignoreFiles)
        {
            var relativePath = Path.GetRelativePath(rootPath, Path.GetDirectoryName(file) ?? "");
            var content = File.ReadAllText(file);
            var patterns = ParsePatterns(content);

            // Adjust patterns to be relative to root
            foreach (var pattern in patterns)
            {
                var adjustedPattern = relativePath == "."
                    ? pattern
                    : pattern with
                    {
                        Pattern = Path.Combine(relativePath, pattern.Pattern).Replace('\\', '/')
                    };
                allPatterns.Add(adjustedPattern);
            }
        }

        return new GitignoreMatcher(allPatterns, rootPath);
    }

    private static List<GitignorePattern> ParsePatterns(string content)
    {
        var patterns = new List<GitignorePattern>();
        var lines = content.Split('\n');

        foreach (var line in lines)
        {
            var trimmed = line.Trim();

            // Skip empty lines and comments
            if (string.IsNullOrEmpty(trimmed) || trimmed.StartsWith('#'))
                continue;

            var isNegation = trimmed.StartsWith('!');
            var pattern = isNegation ? trimmed[1..] : trimmed;

            // Check if directory-only pattern
            var isDirectoryOnly = pattern.EndsWith('/');
            pattern = pattern.TrimEnd('/');

            // Remove leading slash (anchored to root)
            var isAnchored = pattern.StartsWith('/');
            pattern = pattern.TrimStart('/');

            patterns.Add(new GitignorePattern
            {
                Pattern = pattern,
                IsNegation = isNegation,
                IsDirectoryOnly = isDirectoryOnly,
                IsAnchored = isAnchored,
                Regex = CompilePattern(pattern, isAnchored)
            });
        }

        return patterns;
    }

    private static Regex CompilePattern(string pattern, bool isAnchored)
    {
        var regexPattern = new System.Text.StringBuilder();

        if (isAnchored)
            regexPattern.Append('^');
        else if (!pattern.Contains('/'))
            regexPattern.Append("(^|/)");
        else
            regexPattern.Append('^');

        for (int i = 0; i < pattern.Length; i++)
        {
            var c = pattern[i];

            if (c == '*')
            {
                // Check for **
                if (i + 1 < pattern.Length && pattern[i + 1] == '*')
                {
                    // Check for /**/
                    if (i + 2 < pattern.Length && pattern[i + 2] == '/')
                    {
                        regexPattern.Append("(.*/)?");
                        i += 2;
                    }
                    else
                    {
                        regexPattern.Append(".*");
                        i++;
                    }
                }
                else
                {
                    regexPattern.Append("[^/]*");
                }
            }
            else if (c == '?')
            {
                regexPattern.Append("[^/]");
            }
            else if (c == '/')
            {
                regexPattern.Append('/');
            }
            else if (c == '[')
            {
                // Character class - find matching ]
                var end = pattern.IndexOf(']', i + 1);
                if (end > i)
                {
                    regexPattern.Append(pattern.AsSpan(i, end - i + 1));
                    i = end;
                }
                else
                {
                    regexPattern.Append(Regex.Escape(c.ToString()));
                }
            }
            else
            {
                regexPattern.Append(Regex.Escape(c.ToString()));
            }
        }

        regexPattern.Append("(/|$)");

        return new Regex(regexPattern.ToString(), RegexOptions.Compiled | RegexOptions.IgnoreCase);
    }
}

/// <summary>
/// Matcher for gitignore patterns.
/// </summary>
public sealed class GitignoreMatcher : IGitignoreMatcher
{
    private readonly List<GitignorePattern> _patterns;
    private readonly string _basePath;

    public int PatternCount => _patterns.Count;

    public GitignoreMatcher(IEnumerable<GitignorePattern> patterns, string basePath)
    {
        _patterns = patterns.ToList();
        _basePath = basePath;
    }

    public bool IsIgnored(string relativePath, bool isDirectory = false)
    {
        // Normalize path
        relativePath = relativePath.Replace('\\', '/').TrimStart('/');

        var isIgnored = false;

        foreach (var pattern in _patterns)
        {
            // Skip directory-only patterns for files
            if (pattern.IsDirectoryOnly && !isDirectory)
                continue;

            var matches = pattern.Regex.IsMatch(relativePath);
            if (matches)
            {
                isIgnored = !pattern.IsNegation;
            }
        }

        return isIgnored;
    }

    public IEnumerable<string> Filter(IEnumerable<string> relativePaths)
    {
        return relativePaths.Where(p => !IsIgnored(p));
    }

    public IReadOnlyList<string> GetPatterns()
    {
        return _patterns.Select(p =>
        {
            var prefix = p.IsNegation ? "!" : "";
            var suffix = p.IsDirectoryOnly ? "/" : "";
            var anchor = p.IsAnchored ? "/" : "";
            return $"{prefix}{anchor}{p.Pattern}{suffix}";
        }).ToList();
    }
}

/// <summary>
/// Represents a single gitignore pattern.
/// </summary>
public sealed record GitignorePattern
{
    public required string Pattern { get; init; }
    public bool IsNegation { get; init; }
    public bool IsDirectoryOnly { get; init; }
    public bool IsAnchored { get; init; }
    public required Regex Regex { get; init; }
}
```

### File: `src/SeniorIntern.Services/Indexing/GitignorePatternMatcher.cs`

```csharp
using System;
using System.Collections.Generic;
using System.IO;
using System.Linq;

namespace SeniorIntern.Services.Indexing;

/// <summary>
/// Efficient gitignore pattern matching with caching.
/// </summary>
public sealed class GitignorePatternMatcher
{
    private readonly Dictionary<string, GitignoreMatcher> _matchers = new();
    private readonly GitignoreParser _parser = new();
    private readonly object _lock = new();

    /// <summary>
    /// Get or create a matcher for a directory.
    /// </summary>
    public GitignoreMatcher GetMatcher(string directoryPath)
    {
        lock (_lock)
        {
            if (_matchers.TryGetValue(directoryPath, out var cached))
                return cached;

            var gitignorePath = Path.Combine(directoryPath, ".gitignore");
            var matcher = (GitignoreMatcher)_parser.LoadFromFile(gitignorePath);

            _matchers[directoryPath] = matcher;
            return matcher;
        }
    }

    /// <summary>
    /// Check if a file should be ignored, considering all parent .gitignore files.
    /// </summary>
    public bool IsIgnored(string rootPath, string relativePath)
    {
        var pathParts = relativePath.Replace('\\', '/').Split('/');
        var currentPath = rootPath;
        var isIgnored = false;

        // Check each directory level from root to file
        for (int i = 0; i < pathParts.Length; i++)
        {
            var matcher = GetMatcher(currentPath);
            var partialPath = string.Join('/', pathParts.Take(i + 1));
            var isDir = i < pathParts.Length - 1;

            if (matcher.IsIgnored(partialPath, isDir))
            {
                isIgnored = true;
            }

            if (i < pathParts.Length - 1)
            {
                currentPath = Path.Combine(currentPath, pathParts[i]);
            }
        }

        return isIgnored;
    }

    /// <summary>
    /// Filter paths, removing those that are gitignored.
    /// </summary>
    public IEnumerable<string> FilterPaths(string rootPath, IEnumerable<string> relativePaths)
    {
        return relativePaths.Where(p => !IsIgnored(rootPath, p));
    }

    /// <summary>
    /// Clear the matcher cache.
    /// </summary>
    public void ClearCache()
    {
        lock (_lock)
        {
            _matchers.Clear();
        }
    }

    /// <summary>
    /// Remove cached matcher for a specific directory.
    /// </summary>
    public void InvalidateCache(string directoryPath)
    {
        lock (_lock)
        {
            _matchers.Remove(directoryPath);
        }
    }
}
```

---

## v0.7.3f: Incremental Indexing Logic

### Objective
Implement intelligent change detection and incremental indexing to minimize re-processing of unchanged files.

### File: `src/SeniorIntern.Services/Indexing/IncrementalIndexer.cs`

```csharp
using System;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.Extensions.Logging;
using SeniorIntern.Core.Interfaces;
using SeniorIntern.Core.Models;
using SeniorIntern.Services.VectorStore;

namespace SeniorIntern.Services.Indexing;

/// <summary>
/// Handles incremental indexing of changed files.
/// </summary>
public sealed class IncrementalIndexer
{
    private readonly IVectorStore _vectorStore;
    private readonly FileScanner _fileScanner;
    private readonly FileProcessor _fileProcessor;
    private readonly ILogger<IncrementalIndexer> _logger;

    public IncrementalIndexer(
        IVectorStore vectorStore,
        FileScanner fileScanner,
        FileProcessor fileProcessor,
        ILogger<IncrementalIndexer> logger)
    {
        _vectorStore = vectorStore;
        _fileScanner = fileScanner;
        _fileProcessor = fileProcessor;
        _logger = logger;
    }

    /// <summary>
    /// Detect and process only changed files.
    /// </summary>
    public async Task<IncrementalIndexResult> IndexChangesAsync(
        string indexId,
        IProgress<IncrementalProgress>? progress = null,
        CancellationToken ct = default)
    {
        var index = await _vectorStore.GetIndexAsync(indexId, ct)
            ?? throw new ArgumentException($"Index not found: {indexId}");

        _logger.LogInformation("Starting incremental index for {IndexId}", indexId);

        // Step 1: Scan current files
        progress?.Report(new IncrementalProgress
        {
            Phase = IncrementalPhase.Scanning,
            Message = "Scanning for file changes..."
        });

        var settings = index.Settings;
        var currentFiles = await _fileScanner.ScanAsync(
            index.WorkspacePath, settings, null, ct);

        // Step 2: Compute hashes
        progress?.Report(new IncrementalProgress
        {
            Phase = IncrementalPhase.ComputingHashes,
            Message = "Computing file hashes..."
        });

        var fileHashes = new Dictionary<string, string>();
        foreach (var file in currentFiles)
        {
            ct.ThrowIfCancellationRequested();
            fileHashes[file.RelativePath] = file.ContentHash;
        }

        // Step 3: Detect changes
        progress?.Report(new IncrementalProgress
        {
            Phase = IncrementalPhase.DetectingChanges,
            Message = "Detecting changes..."
        });

        var changes = await _vectorStore.DetectFileChangesAsync(indexId, fileHashes, ct);

        _logger.LogInformation(
            "Detected changes: {Added} added, {Modified} modified, {Deleted} deleted, {Unchanged} unchanged",
            changes.AddedFiles.Count, changes.ModifiedFiles.Count,
            changes.DeletedFiles.Count, changes.UnchangedFiles.Count);

        if (!changes.HasChanges)
        {
            _logger.LogInformation("No changes detected");
            return new IncrementalIndexResult
            {
                Success = true,
                NoChanges = true
            };
        }

        // Step 4: Process changes
        var result = new IncrementalIndexResult
        {
            Success = true,
            FilesAdded = changes.AddedFiles.Count,
            FilesModified = changes.ModifiedFiles.Count,
            FilesDeleted = changes.DeletedFiles.Count,
            FilesUnchanged = changes.UnchangedFiles.Count
        };

        // Remove deleted files
        if (changes.DeletedFiles.Count > 0)
        {
            progress?.Report(new IncrementalProgress
            {
                Phase = IncrementalPhase.RemovingDeleted,
                Message = $"Removing {changes.DeletedFiles.Count} deleted files..."
            });

            result.ChunksRemoved = await _vectorStore.RemoveChunksForFilesAsync(
                indexId, changes.DeletedFiles, ct);
        }

        // Process added and modified files
        var filesToProcess = changes.AddedFiles.Concat(changes.ModifiedFiles).ToList();
        if (filesToProcess.Count > 0)
        {
            // Remove old chunks for modified files
            foreach (var file in changes.ModifiedFiles)
            {
                result.ChunksRemoved += await _vectorStore.RemoveChunksForFileAsync(indexId, file, ct);
            }

            // Process files
            var options = settings.ToIndexingOptions();
            var contexts = filesToProcess.Select(f => new FileProcessingContext
            {
                IndexId = indexId,
                AbsolutePath = Path.Combine(index.WorkspacePath, f),
                RelativePath = f,
                Options = options
            }).ToList();

            var processed = 0;
            var batchProgress = new Progress<BatchProgress>(p =>
            {
                progress?.Report(new IncrementalProgress
                {
                    Phase = IncrementalPhase.Processing,
                    ProcessedFiles = p.Processed,
                    TotalFiles = p.Total,
                    CurrentFile = p.CurrentFile,
                    PercentComplete = p.PercentComplete
                });
            });

            var processingResult = await _fileProcessor.ProcessFilesAsync(
                contexts,
                options.ParallelProcessing,
                batchProgress,
                ct);

            result.ChunksAdded = processingResult.TotalChunks;
            result.Errors = processingResult.Results
                .Where(r => r.Error != null)
                .Select(r => r.Error!)
                .ToList();
        }

        // Update index statistics
        await UpdateIndexStatisticsAsync(indexId, ct);

        _logger.LogInformation(
            "Incremental index complete: +{Added} -{Removed} chunks",
            result.ChunksAdded, result.ChunksRemoved);

        return result;
    }

    /// <summary>
    /// Check if an index needs updating.
    /// </summary>
    public async Task<IndexChangesSummary> CheckForChangesAsync(
        string indexId,
        CancellationToken ct = default)
    {
        var index = await _vectorStore.GetIndexAsync(indexId, ct)
            ?? throw new ArgumentException($"Index not found: {indexId}");

        var currentFiles = await _fileScanner.ScanAsync(
            index.WorkspacePath, index.Settings, null, ct);

        var fileHashes = currentFiles.ToDictionary(f => f.RelativePath, f => f.ContentHash);
        var changes = await _vectorStore.DetectFileChangesAsync(indexId, fileHashes, ct);

        return new IndexChangesSummary
        {
            HasChanges = changes.HasChanges,
            AddedCount = changes.AddedFiles.Count,
            ModifiedCount = changes.ModifiedFiles.Count,
            DeletedCount = changes.DeletedFiles.Count,
            UnchangedCount = changes.UnchangedFiles.Count,
            AddedFiles = changes.AddedFiles,
            ModifiedFiles = changes.ModifiedFiles,
            DeletedFiles = changes.DeletedFiles
        };
    }

    private async Task UpdateIndexStatisticsAsync(string indexId, CancellationToken ct)
    {
        var stats = await _vectorStore.GetStatisticsAsync(indexId, ct);
        var index = await _vectorStore.GetIndexAsync(indexId, ct);

        if (index != null)
        {
            index.ChunkCount = stats.TotalChunks;
            index.FileCount = stats.TotalFiles;
            index.TotalFileSizeBytes = stats.TotalFileSizeBytes;
            index.UpdatedAt = DateTime.UtcNow;
            index.LastIncrementalUpdateAt = DateTime.UtcNow;

            await _vectorStore.UpdateIndexAsync(index, ct);
        }
    }
}

/// <summary>
/// Result of incremental indexing.
/// </summary>
public sealed class IncrementalIndexResult
{
    public bool Success { get; init; }
    public bool NoChanges { get; init; }
    public int FilesAdded { get; init; }
    public int FilesModified { get; init; }
    public int FilesDeleted { get; init; }
    public int FilesUnchanged { get; init; }
    public int ChunksAdded { get; init; }
    public int ChunksRemoved { get; init; }
    public int ChunksDelta => ChunksAdded - ChunksRemoved;
    public IReadOnlyList<IndexingError> Errors { get; init; } = [];
}

/// <summary>
/// Progress for incremental indexing.
/// </summary>
public sealed class IncrementalProgress
{
    public IncrementalPhase Phase { get; init; }
    public string? Message { get; init; }
    public int ProcessedFiles { get; init; }
    public int TotalFiles { get; init; }
    public string? CurrentFile { get; init; }
    public double PercentComplete { get; init; }
}

/// <summary>
/// Phases of incremental indexing.
/// </summary>
public enum IncrementalPhase
{
    Scanning,
    ComputingHashes,
    DetectingChanges,
    RemovingDeleted,
    Processing,
    Complete
}

/// <summary>
/// Summary of detected changes.
/// </summary>
public sealed class IndexChangesSummary
{
    public bool HasChanges { get; init; }
    public int AddedCount { get; init; }
    public int ModifiedCount { get; init; }
    public int DeletedCount { get; init; }
    public int UnchangedCount { get; init; }
    public IReadOnlyList<string> AddedFiles { get; init; } = [];
    public IReadOnlyList<string> ModifiedFiles { get; init; } = [];
    public IReadOnlyList<string> DeletedFiles { get; init; } = [];
}
```

### File: `src/SeniorIntern.Services/Indexing/ChangeDetector.cs`

```csharp
using System;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Threading;
using System.Threading.Tasks;
using SeniorIntern.Core.Models;
using SeniorIntern.Services.VectorStore;

namespace SeniorIntern.Services.Indexing;

/// <summary>
/// Detects file changes for incremental indexing.
/// </summary>
public sealed class ChangeDetector
{
    /// <summary>
    /// Detect changes between indexed files and current filesystem state.
    /// </summary>
    public async Task<DetectedChanges> DetectChangesAsync(
        IReadOnlyList<IndexedFile> indexedFiles,
        IReadOnlyList<ScannedFile> currentFiles,
        ChangeDetectionOptions options,
        CancellationToken ct = default)
    {
        var indexedByPath = indexedFiles.ToDictionary(f => f.FilePath);
        var currentByPath = currentFiles.ToDictionary(f => f.RelativePath);

        var added = new List<ScannedFile>();
        var modified = new List<(IndexedFile Old, ScannedFile New)>();
        var deleted = new List<IndexedFile>();
        var unchanged = new List<IndexedFile>();

        // Check each current file
        foreach (var current in currentFiles)
        {
            ct.ThrowIfCancellationRequested();

            if (!indexedByPath.TryGetValue(current.RelativePath, out var indexed))
            {
                // New file
                added.Add(current);
            }
            else
            {
                // Check if modified
                var isModified = options.UseHashComparison
                    ? current.ContentHash != indexed.FileHash
                    : current.LastModified > indexed.IndexedAt;

                if (isModified)
                {
                    modified.Add((indexed, current));
                }
                else
                {
                    unchanged.Add(indexed);
                }
            }
        }

        // Find deleted files
        foreach (var indexed in indexedFiles)
        {
            if (!currentByPath.ContainsKey(indexed.FilePath))
            {
                deleted.Add(indexed);
            }
        }

        return new DetectedChanges
        {
            Added = added,
            Modified = modified,
            Deleted = deleted,
            Unchanged = unchanged
        };
    }

    /// <summary>
    /// Quick check if any files have changed.
    /// </summary>
    public async Task<bool> HasAnyChangesAsync(
        string workspacePath,
        IReadOnlyList<IndexedFile> indexedFiles,
        CancellationToken ct = default)
    {
        var indexedByPath = indexedFiles.ToDictionary(f => f.FilePath);

        // Check for new or modified files
        foreach (var indexed in indexedFiles)
        {
            ct.ThrowIfCancellationRequested();

            var fullPath = Path.Combine(workspacePath, indexed.FilePath);

            if (!File.Exists(fullPath))
            {
                // File deleted
                return true;
            }

            var fileInfo = new FileInfo(fullPath);
            if (fileInfo.LastWriteTimeUtc > indexed.IndexedAt)
            {
                // File modified
                return true;
            }
        }

        // This doesn't check for new files - would need full scan
        return false;
    }
}

/// <summary>
/// Options for change detection.
/// </summary>
public sealed class ChangeDetectionOptions
{
    /// <summary>
    /// Whether to use content hash comparison (more accurate but slower).
    /// </summary>
    public bool UseHashComparison { get; init; } = true;

    /// <summary>
    /// Whether to check for renamed files.
    /// </summary>
    public bool DetectRenames { get; init; } = false;

    /// <summary>
    /// Minimum file age in seconds to consider for indexing.
    /// Helps avoid indexing files that are still being written.
    /// </summary>
    public int MinFileAgeSeconds { get; init; } = 1;
}

/// <summary>
/// Result of change detection.
/// </summary>
public sealed class DetectedChanges
{
    public IReadOnlyList<ScannedFile> Added { get; init; } = [];
    public IReadOnlyList<(IndexedFile Old, ScannedFile New)> Modified { get; init; } = [];
    public IReadOnlyList<IndexedFile> Deleted { get; init; } = [];
    public IReadOnlyList<IndexedFile> Unchanged { get; init; } = [];

    public bool HasChanges => Added.Count > 0 || Modified.Count > 0 || Deleted.Count > 0;
    public int TotalChanges => Added.Count + Modified.Count + Deleted.Count;
}
```

---

## v0.7.3g: Background Job Queue

### Objective
Implement a background job queue for processing indexing operations asynchronously.

### File: `src/SeniorIntern.Services/Indexing/IndexingJobQueue.cs`

```csharp
using System;
using System.Collections.Concurrent;
using System.Collections.Generic;
using System.Linq;
using System.Threading;
using System.Threading.Channels;
using System.Threading.Tasks;
using SeniorIntern.Core.Interfaces;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Services.Indexing;

/// <summary>
/// Priority queue for indexing jobs.
/// </summary>
public sealed class IndexingJobQueue : IIndexingJobQueue
{
    private readonly Channel<IndexingJob> _channel;
    private readonly ConcurrentDictionary<string, IndexingJob> _jobs = new();
    private readonly object _lock = new();

    public int QueueLength => _jobs.Count(j => j.Value.Status == IndexingJobStatus.Queued);
    public bool IsProcessing { get; private set; }

    public event EventHandler<IndexingJobEventArgs>? JobEnqueued;
    public event EventHandler<IndexingJobEventArgs>? JobDequeued;

    public IndexingJobQueue()
    {
        _channel = Channel.CreateUnbounded<IndexingJob>(new UnboundedChannelOptions
        {
            SingleReader = true,
            SingleWriter = false
        });
    }

    public void Enqueue(IndexingJob job)
    {
        if (_jobs.TryAdd(job.Id, job))
        {
            // Insert in priority order
            _channel.Writer.TryWrite(job);
            JobEnqueued?.Invoke(this, new IndexingJobEventArgs { Job = job });
        }
    }

    public bool TryDequeue(out IndexingJob? job)
    {
        if (_channel.Reader.TryRead(out job))
        {
            if (job != null)
            {
                job.Status = IndexingJobStatus.Running;
                job.StartedAt = DateTime.UtcNow;
                IsProcessing = true;
                JobDequeued?.Invoke(this, new IndexingJobEventArgs { Job = job });
            }
            return true;
        }
        return false;
    }

    public async Task<IndexingJob> DequeueAsync(CancellationToken ct = default)
    {
        var job = await _channel.Reader.ReadAsync(ct);
        job.Status = IndexingJobStatus.Running;
        job.StartedAt = DateTime.UtcNow;
        IsProcessing = true;
        JobDequeued?.Invoke(this, new IndexingJobEventArgs { Job = job });
        return job;
    }

    public IReadOnlyList<IndexingJob> GetQueuedJobs()
    {
        return _jobs.Values
            .Where(j => j.Status == IndexingJobStatus.Queued)
            .OrderByDescending(j => j.Priority)
            .ThenBy(j => j.CreatedAt)
            .ToList();
    }

    public bool Remove(string jobId)
    {
        if (_jobs.TryRemove(jobId, out var job))
        {
            job.Status = IndexingJobStatus.Cancelled;
            return true;
        }
        return false;
    }

    public void Clear()
    {
        lock (_lock)
        {
            foreach (var job in _jobs.Values.Where(j => j.Status == IndexingJobStatus.Queued))
            {
                job.Status = IndexingJobStatus.Cancelled;
            }
            _jobs.Clear();

            // Drain the channel
            while (_channel.Reader.TryRead(out _)) { }
        }
    }

    public bool Prioritize(string jobId)
    {
        if (_jobs.TryGetValue(jobId, out var job) && job.Status == IndexingJobStatus.Queued)
        {
            // Can't truly re-prioritize in a channel, but we can update the priority
            // for sorting purposes when listing
            var updatedJob = job with { Priority = int.MaxValue };
            _jobs.TryUpdate(jobId, updatedJob, job);
            return true;
        }
        return false;
    }

    public void MarkComplete(string jobId)
    {
        if (_jobs.TryGetValue(jobId, out var job))
        {
            job.Status = IndexingJobStatus.Completed;
            job.CompletedAt = DateTime.UtcNow;
        }
        IsProcessing = false;
    }

    public void MarkFailed(string jobId, string error)
    {
        if (_jobs.TryGetValue(jobId, out var job))
        {
            job.Status = IndexingJobStatus.Failed;
            job.CompletedAt = DateTime.UtcNow;
            job.ErrorMessage = error;
        }
        IsProcessing = false;
    }
}
```

### File: `src/SeniorIntern.Services/Indexing/IndexingJobProcessor.cs`

```csharp
using System;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.Extensions.Hosting;
using Microsoft.Extensions.Logging;
using SeniorIntern.Core.Interfaces;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Services.Indexing;

/// <summary>
/// Background service that processes indexing jobs from the queue.
/// </summary>
public sealed class IndexingJobProcessor : BackgroundService
{
    private readonly IIndexingService _indexingService;
    private readonly IndexingJobQueue _jobQueue;
    private readonly ILogger<IndexingJobProcessor> _logger;

    public IndexingJobProcessor(
        IIndexingService indexingService,
        IIndexingJobQueue jobQueue,
        ILogger<IndexingJobProcessor> logger)
    {
        _indexingService = indexingService;
        _jobQueue = (IndexingJobQueue)jobQueue;
        _logger = logger;
    }

    protected override async Task ExecuteAsync(CancellationToken stoppingToken)
    {
        _logger.LogInformation("Indexing job processor started");

        while (!stoppingToken.IsCancellationRequested)
        {
            try
            {
                var job = await _jobQueue.DequeueAsync(stoppingToken);
                await ProcessJobAsync(job, stoppingToken);
            }
            catch (OperationCanceledException) when (stoppingToken.IsCancellationRequested)
            {
                break;
            }
            catch (Exception ex)
            {
                _logger.LogError(ex, "Error processing job from queue");
                await Task.Delay(1000, stoppingToken); // Brief delay before retrying
            }
        }

        _logger.LogInformation("Indexing job processor stopped");
    }

    private async Task ProcessJobAsync(IndexingJob job, CancellationToken ct)
    {
        _logger.LogInformation(
            "Processing job {JobId}: {Type} for {Path}",
            job.Id, job.Type, job.WorkspacePath);

        try
        {
            IndexingResult result;

            switch (job.Type)
            {
                case IndexingJobType.FullIndex:
                    result = await _indexingService.IndexWorkspaceAsync(
                        job.WorkspacePath,
                        job.Options,
                        new Progress<IndexingProgress>(p => job.Progress = p),
                        ct);
                    break;

                case IndexingJobType.IncrementalUpdate:
                    result = await _indexingService.UpdateIndexAsync(
                        job.IndexId!,
                        new Progress<IndexingProgress>(p => job.Progress = p),
                        ct);
                    break;

                case IndexingJobType.Sync:
                    var syncResult = await _indexingService.SyncIndexAsync(
                        job.IndexId!,
                        new Progress<IndexingProgress>(p => job.Progress = p),
                        ct);
                    result = new IndexingResult
                    {
                        Success = syncResult.Success,
                        FilesIndexed = syncResult.FilesAdded + syncResult.FilesUpdated,
                        ChunksCreated = syncResult.ChunksDelta > 0 ? syncResult.ChunksDelta : 0,
                        Duration = syncResult.Duration,
                        Errors = syncResult.Errors
                    };
                    break;

                default:
                    throw new NotSupportedException($"Job type not supported: {job.Type}");
            }

            job.Result = result;

            if (result.Success)
            {
                _jobQueue.MarkComplete(job.Id);
                _logger.LogInformation(
                    "Job {JobId} completed: {Files} files, {Chunks} chunks",
                    job.Id, result.FilesIndexed, result.ChunksCreated);
            }
            else
            {
                _jobQueue.MarkFailed(job.Id, result.ErrorMessage ?? "Unknown error");
                _logger.LogWarning("Job {JobId} failed: {Error}", job.Id, result.ErrorMessage);
            }
        }
        catch (OperationCanceledException)
        {
            _jobQueue.MarkFailed(job.Id, "Job was cancelled");
            throw;
        }
        catch (Exception ex)
        {
            _jobQueue.MarkFailed(job.Id, ex.Message);
            _logger.LogError(ex, "Job {JobId} threw exception", job.Id);

            // Check if should retry
            if (job.CanRetry)
            {
                job.RetryCount++;
                job.Status = IndexingJobStatus.Queued;
                _jobQueue.Enqueue(job);
                _logger.LogInformation("Re-queued job {JobId} (attempt {Attempt})", job.Id, job.RetryCount);
            }
        }
    }
}
```

### File: `src/SeniorIntern.Services/Indexing/IndexingJobEventArgs.cs`

```csharp
using System;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Services.Indexing;

/// <summary>
/// Event args for indexing job events.
/// </summary>
public sealed class IndexingJobEventArgs : EventArgs
{
    public required IndexingJob Job { get; init; }
}

/// <summary>
/// Event args for job completion.
/// </summary>
public sealed class IndexingJobCompletedEventArgs : EventArgs
{
    public required IndexingJob Job { get; init; }
    public required IndexingResult Result { get; init; }
}

/// <summary>
/// Event args for state changes.
/// </summary>
public sealed class IndexingStateChangedEventArgs : EventArgs
{
    public IndexingStatus OldStatus { get; init; }
    public IndexingStatus NewStatus { get; init; }
}

/// <summary>
/// Event args for file indexed.
/// </summary>
public sealed class FileIndexedEventArgs : EventArgs
{
    public required string FilePath { get; init; }
    public int ChunkCount { get; init; }
    public bool Success { get; init; }
    public string? Error { get; init; }
}

/// <summary>
/// Event args for file error.
/// </summary>
public sealed class FileIndexingErrorEventArgs : EventArgs
{
    public required string FilePath { get; init; }
    public required IndexingError Error { get; init; }
}

/// <summary>
/// Event args for progress updates.
/// </summary>
public sealed class IndexingProgressEventArgs : EventArgs
{
    public required IndexingProgress Progress { get; init; }
}

/// <summary>
/// Status of the indexing service.
/// </summary>
public enum IndexingStatus
{
    /// <summary>
    /// No indexing in progress.
    /// </summary>
    Idle,

    /// <summary>
    /// Scanning filesystem for files.
    /// </summary>
    Scanning,

    /// <summary>
    /// Processing files.
    /// </summary>
    Indexing,

    /// <summary>
    /// Updating existing index.
    /// </summary>
    Updating,

    /// <summary>
    /// Indexing is paused.
    /// </summary>
    Paused,

    /// <summary>
    /// Cancellation in progress.
    /// </summary>
    Cancelling
}
```

---

## v0.7.3h: File Watcher Service

### Objective
Implement file system watching to detect changes and trigger automatic re-indexing.

### File: `src/SeniorIntern.Services/Indexing/FileWatcherService.cs`

```csharp
using System;
using System.Collections.Concurrent;
using System.Collections.Generic;
using System.IO;
using System.Linq;
using System.Threading;
using System.Threading.Tasks;
using System.Timers;
using Microsoft.Extensions.Logging;
using SeniorIntern.Core.Interfaces;
using SeniorIntern.Core.Models;
using Timer = System.Timers.Timer;

namespace SeniorIntern.Services.Indexing;

/// <summary>
/// Watches file system changes and triggers re-indexing.
/// </summary>
public sealed class FileWatcherService : IFileWatcherService, IAsyncDisposable
{
    private readonly IIndexingJobQueue _jobQueue;
    private readonly ILogger<FileWatcherService> _logger;

    private readonly ConcurrentDictionary<string, WatcherState> _watchers = new();
    private bool _isEnabled = true;
    private bool _isDisposed;

    public bool IsEnabled => _isEnabled;
    public IReadOnlyList<string> WatchedWorkspaces => _watchers.Keys.ToList();

    public event EventHandler<FileChangesDetectedEventArgs>? ChangesDetected;

    public FileWatcherService(
        IIndexingJobQueue jobQueue,
        ILogger<FileWatcherService> logger)
    {
        _jobQueue = jobQueue;
        _logger = logger;
    }

    public void StartWatching(
        string indexId,
        string workspacePath,
        FileWatchOptions? options = null)
    {
        if (_isDisposed)
            throw new ObjectDisposedException(nameof(FileWatcherService));

        options ??= new FileWatchOptions();

        if (_watchers.ContainsKey(workspacePath))
        {
            _logger.LogWarning("Already watching {Path}", workspacePath);
            return;
        }

        _logger.LogInformation("Starting file watcher for {Path}", workspacePath);

        var watcher = new FileSystemWatcher(workspacePath)
        {
            IncludeSubdirectories = options.IncludeSubdirectories,
            NotifyFilter = NotifyFilters.FileName |
                          NotifyFilters.DirectoryName |
                          NotifyFilters.LastWrite |
                          NotifyFilters.Size,
            EnableRaisingEvents = _isEnabled
        };

        var state = new WatcherState
        {
            IndexId = indexId,
            WorkspacePath = workspacePath,
            Watcher = watcher,
            Options = options,
            PendingChanges = new ConcurrentBag<FileChange>(),
            DebounceTimer = new Timer(options.DebounceDelayMs)
        };

        state.DebounceTimer.AutoReset = false;
        state.DebounceTimer.Elapsed += (s, e) => OnDebounceElapsed(state);

        // Set up event handlers
        watcher.Created += (s, e) => OnFileChanged(state, e.FullPath, FileChangeType.Created);
        watcher.Changed += (s, e) => OnFileChanged(state, e.FullPath, FileChangeType.Modified);
        watcher.Deleted += (s, e) => OnFileChanged(state, e.FullPath, FileChangeType.Deleted);
        watcher.Renamed += (s, e) => OnFileRenamed(state, e.OldFullPath, e.FullPath);
        watcher.Error += (s, e) => OnWatcherError(state, e.GetException());

        _watchers.TryAdd(workspacePath, state);
    }

    public void StopWatching(string workspacePath)
    {
        if (_watchers.TryRemove(workspacePath, out var state))
        {
            _logger.LogInformation("Stopping file watcher for {Path}", workspacePath);
            DisposeWatcher(state);
        }
    }

    public void StopAll()
    {
        foreach (var path in _watchers.Keys.ToList())
        {
            StopWatching(path);
        }
    }

    public void SetEnabled(bool enabled)
    {
        _isEnabled = enabled;

        foreach (var state in _watchers.Values)
        {
            state.Watcher.EnableRaisingEvents = enabled;
        }

        _logger.LogInformation("File watching {Status}", enabled ? "enabled" : "disabled");
    }

    public IReadOnlyList<string> GetPendingChanges(string workspacePath)
    {
        if (_watchers.TryGetValue(workspacePath, out var state))
        {
            return state.PendingChanges
                .Select(c => c.FilePath)
                .Distinct()
                .ToList();
        }
        return [];
    }

    private void OnFileChanged(WatcherState state, string fullPath, FileChangeType changeType)
    {
        if (!_isEnabled) return;

        var relativePath = Path.GetRelativePath(state.WorkspacePath, fullPath);

        // Check if should ignore
        if (ShouldIgnore(relativePath, state.Options))
            return;

        _logger.LogDebug("File {Type}: {Path}", changeType, relativePath);

        state.PendingChanges.Add(new FileChange
        {
            FilePath = relativePath,
            ChangeType = changeType
        });

        // Reset debounce timer
        state.DebounceTimer.Stop();
        state.DebounceTimer.Start();

        // Check if batch size exceeded
        if (state.PendingChanges.Count >= state.Options.MaxBatchSize)
        {
            state.DebounceTimer.Stop();
            OnDebounceElapsed(state);
        }
    }

    private void OnFileRenamed(WatcherState state, string oldPath, string newPath)
    {
        if (!_isEnabled) return;

        var oldRelative = Path.GetRelativePath(state.WorkspacePath, oldPath);
        var newRelative = Path.GetRelativePath(state.WorkspacePath, newPath);

        if (ShouldIgnore(newRelative, state.Options))
            return;

        _logger.LogDebug("File renamed: {Old} -> {New}", oldRelative, newRelative);

        state.PendingChanges.Add(new FileChange
        {
            FilePath = newRelative,
            ChangeType = FileChangeType.Renamed,
            OldPath = oldRelative
        });

        state.DebounceTimer.Stop();
        state.DebounceTimer.Start();
    }

    private void OnWatcherError(WatcherState state, Exception error)
    {
        _logger.LogError(error, "File watcher error for {Path}", state.WorkspacePath);

        // Try to restart watcher
        try
        {
            state.Watcher.EnableRaisingEvents = false;
            state.Watcher.EnableRaisingEvents = _isEnabled;
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Failed to restart watcher for {Path}", state.WorkspacePath);
        }
    }

    private void OnDebounceElapsed(WatcherState state)
    {
        if (state.PendingChanges.IsEmpty)
            return;

        // Collect all pending changes
        var changes = new List<FileChange>();
        while (state.PendingChanges.TryTake(out var change))
        {
            changes.Add(change);
        }

        // Deduplicate
        var deduped = DeduplicateChanges(changes);

        _logger.LogInformation(
            "Detected {Count} file changes in {Path}",
            deduped.Count, state.WorkspacePath);

        // Raise event
        ChangesDetected?.Invoke(this, new FileChangesDetectedEventArgs
        {
            IndexId = state.IndexId,
            WorkspacePath = state.WorkspacePath,
            Changes = deduped
        });

        // Queue re-indexing if enabled
        if (state.Options.AutoReindex)
        {
            var job = new IndexingJob
            {
                Type = IndexingJobType.FileWatchUpdate,
                IndexId = state.IndexId,
                WorkspacePath = state.WorkspacePath,
                Options = new IndexingOptions
                {
                    ForceReindex = false
                }
            };

            _jobQueue.Enqueue(job);
            _logger.LogDebug("Queued file watch update job {JobId}", job.Id);
        }
    }

    private static IReadOnlyList<FileChange> DeduplicateChanges(List<FileChange> changes)
    {
        var byPath = new Dictionary<string, FileChange>();

        foreach (var change in changes)
        {
            if (byPath.TryGetValue(change.FilePath, out var existing))
            {
                // Merge changes
                if (change.ChangeType == FileChangeType.Deleted)
                {
                    if (existing.ChangeType == FileChangeType.Created)
                    {
                        // Created then deleted = no change
                        byPath.Remove(change.FilePath);
                    }
                    else
                    {
                        byPath[change.FilePath] = change;
                    }
                }
                else if (change.ChangeType == FileChangeType.Created)
                {
                    // Keep as created
                    byPath[change.FilePath] = change;
                }
                // Multiple modifications = keep latest
            }
            else
            {
                byPath[change.FilePath] = change;
            }
        }

        return byPath.Values.ToList();
    }

    private bool ShouldIgnore(string relativePath, FileWatchOptions options)
    {
        // Check exclude patterns
        if (options.ExcludePatterns != null)
        {
            foreach (var pattern in options.ExcludePatterns)
            {
                if (MatchesPattern(relativePath, pattern))
                    return true;
            }
        }

        // Check include patterns if specified
        if (options.FilePatterns != null && options.FilePatterns.Count > 0)
        {
            foreach (var pattern in options.FilePatterns)
            {
                if (MatchesPattern(relativePath, pattern))
                    return false;
            }
            return true; // Not in include list
        }

        return false;
    }

    private static bool MatchesPattern(string path, string pattern)
    {
        // Simple glob matching
        var regexPattern = "^" + System.Text.RegularExpressions.Regex.Escape(pattern)
            .Replace(@"\*\*", ".*")
            .Replace(@"\*", "[^/\\\\]*")
            .Replace(@"\?", ".") + "$";

        return System.Text.RegularExpressions.Regex.IsMatch(
            path,
            regexPattern,
            System.Text.RegularExpressions.RegexOptions.IgnoreCase);
    }

    private static void DisposeWatcher(WatcherState state)
    {
        state.DebounceTimer.Stop();
        state.DebounceTimer.Dispose();
        state.Watcher.EnableRaisingEvents = false;
        state.Watcher.Dispose();
    }

    public async ValueTask DisposeAsync()
    {
        if (_isDisposed) return;
        _isDisposed = true;

        foreach (var state in _watchers.Values)
        {
            DisposeWatcher(state);
        }
        _watchers.Clear();

        await Task.CompletedTask;
    }

    private sealed class WatcherState
    {
        public required string IndexId { get; init; }
        public required string WorkspacePath { get; init; }
        public required FileSystemWatcher Watcher { get; init; }
        public required FileWatchOptions Options { get; init; }
        public required ConcurrentBag<FileChange> PendingChanges { get; init; }
        public required Timer DebounceTimer { get; init; }
    }
}
```

### File: `src/SeniorIntern.Services/Indexing/FileWatcherCoordinator.cs`

```csharp
using System;
using System.Collections.Generic;
using System.Linq;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.Extensions.Hosting;
using Microsoft.Extensions.Logging;
using SeniorIntern.Core.Interfaces;

namespace SeniorIntern.Services.Indexing;

/// <summary>
/// Coordinates file watching with indexing operations.
/// </summary>
public sealed class FileWatcherCoordinator : IHostedService
{
    private readonly IFileWatcherService _fileWatcher;
    private readonly IVectorStore _vectorStore;
    private readonly ILogger<FileWatcherCoordinator> _logger;

    public FileWatcherCoordinator(
        IFileWatcherService fileWatcher,
        IVectorStore vectorStore,
        ILogger<FileWatcherCoordinator> logger)
    {
        _fileWatcher = fileWatcher;
        _vectorStore = vectorStore;
        _logger = logger;
    }

    public async Task StartAsync(CancellationToken cancellationToken)
    {
        _logger.LogInformation("Starting file watcher coordinator");

        // Load existing indexes and start watching them
        var indexes = await _vectorStore.ListIndexesAsync(cancellationToken);

        foreach (var index in indexes.Where(i => i.Settings.AutoReindex))
        {
            try
            {
                _fileWatcher.StartWatching(
                    index.Id,
                    index.WorkspacePath,
                    new FileWatchOptions
                    {
                        DebounceDelayMs = index.Settings.AutoReindexDelayMs,
                        ExcludePatterns = index.Settings.ExcludePatterns
                    });
            }
            catch (Exception ex)
            {
                _logger.LogWarning(ex,
                    "Failed to start file watcher for index {Id}",
                    index.Id);
            }
        }
    }

    public Task StopAsync(CancellationToken cancellationToken)
    {
        _logger.LogInformation("Stopping file watcher coordinator");
        _fileWatcher.StopAll();
        return Task.CompletedTask;
    }
}
```

### File: `src/SeniorIntern.Services/Indexing/FileWatcherBackgroundService.cs`

```csharp
using System;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.Extensions.Hosting;
using Microsoft.Extensions.Logging;
using SeniorIntern.Core.Interfaces;

namespace SeniorIntern.Services.Indexing;

/// <summary>
/// Background service that handles file change events.
/// </summary>
public sealed class FileWatcherBackgroundService : BackgroundService
{
    private readonly IFileWatcherService _fileWatcher;
    private readonly IIndexingService _indexingService;
    private readonly ILogger<FileWatcherBackgroundService> _logger;

    public FileWatcherBackgroundService(
        IFileWatcherService fileWatcher,
        IIndexingService indexingService,
        ILogger<FileWatcherBackgroundService> logger)
    {
        _fileWatcher = fileWatcher;
        _indexingService = indexingService;
        _logger = logger;
    }

    protected override async Task ExecuteAsync(CancellationToken stoppingToken)
    {
        _fileWatcher.ChangesDetected += async (sender, args) =>
        {
            if (stoppingToken.IsCancellationRequested) return;

            _logger.LogDebug(
                "Processing {Count} file changes for index {IndexId}",
                args.Changes.Count, args.IndexId);

            try
            {
                // Get files that need processing
                var addedOrModified = args.Changes
                    .Where(c => c.ChangeType != FileChangeType.Deleted)
                    .Select(c => c.FilePath)
                    .ToList();

                var deleted = args.Changes
                    .Where(c => c.ChangeType == FileChangeType.Deleted)
                    .Select(c => c.FilePath)
                    .ToList();

                // Remove deleted files
                if (deleted.Count > 0)
                {
                    await _indexingService.RemoveFilesAsync(
                        args.IndexId, deleted, stoppingToken);
                }

                // Index added/modified files
                if (addedOrModified.Count > 0)
                {
                    await _indexingService.IndexFilesAsync(
                        args.IndexId, addedOrModified, null, stoppingToken);
                }
            }
            catch (Exception ex)
            {
                _logger.LogError(ex,
                    "Error processing file changes for index {IndexId}",
                    args.IndexId);
            }
        };

        // Keep running until cancelled
        await Task.Delay(Timeout.Infinite, stoppingToken);
    }
}
```

---

## v0.7.3i: Progress Tracking & Events

### Objective
Implement detailed progress tracking and event notifications for the indexing pipeline.

### File: `src/SeniorIntern.Services/Indexing/ProgressTracker.cs`

```csharp
using System;
using System.Diagnostics;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Services.Indexing;

/// <summary>
/// Tracks and calculates indexing progress.
/// </summary>
public sealed class ProgressTracker
{
    private readonly Stopwatch _stopwatch = new();
    private readonly int _totalFiles;
    private readonly long _totalBytes;

    private int _processedFiles;
    private int _skippedFiles;
    private int _failedFiles;
    private int _totalChunks;
    private int _processedChunks;
    private long _processedBytes;
    private string? _currentFile;
    private IndexingPhase _phase = IndexingPhase.Initializing;

    public ProgressTracker(int totalFiles, long totalBytes = 0)
    {
        _totalFiles = totalFiles;
        _totalBytes = totalBytes;
    }

    public void Start()
    {
        _stopwatch.Start();
    }

    public void Stop()
    {
        _stopwatch.Stop();
    }

    public void SetPhase(IndexingPhase phase)
    {
        _phase = phase;
    }

    public void SetCurrentFile(string? file)
    {
        _currentFile = file;
    }

    public void FileProcessed(int chunkCount, long bytes)
    {
        _processedFiles++;
        _totalChunks += chunkCount;
        _processedChunks += chunkCount;
        _processedBytes += bytes;
    }

    public void FileSkipped()
    {
        _skippedFiles++;
    }

    public void FileFailed()
    {
        _failedFiles++;
    }

    public IndexingProgress GetProgress(string? message = null)
    {
        var elapsed = _stopwatch.Elapsed;
        var percentComplete = _totalFiles > 0
            ? (double)(_processedFiles + _skippedFiles + _failedFiles) / _totalFiles * 100
            : 0;

        TimeSpan? estimatedRemaining = null;
        var processedCount = _processedFiles + _skippedFiles + _failedFiles;
        if (processedCount > 0 && elapsed.TotalSeconds > 0)
        {
            var rate = processedCount / elapsed.TotalSeconds;
            var remaining = _totalFiles - processedCount;
            estimatedRemaining = TimeSpan.FromSeconds(remaining / rate);
        }

        return new IndexingProgress
        {
            Phase = _phase,
            TotalFiles = _totalFiles,
            ProcessedFiles = _processedFiles,
            SkippedFiles = _skippedFiles,
            FailedFiles = _failedFiles,
            TotalChunks = _totalChunks,
            ProcessedChunks = _processedChunks,
            CurrentFile = _currentFile,
            Message = message,
            PercentComplete = percentComplete,
            Elapsed = elapsed,
            EstimatedRemaining = estimatedRemaining,
            FilesPerSecond = elapsed.TotalSeconds > 0
                ? _processedFiles / elapsed.TotalSeconds
                : 0,
            ChunksPerSecond = elapsed.TotalSeconds > 0
                ? _processedChunks / elapsed.TotalSeconds
                : 0,
            BytesProcessed = _processedBytes,
            TotalBytes = _totalBytes
        };
    }

    public IndexingResult GetResult(string indexId, bool success, string? errorMessage = null)
    {
        _stopwatch.Stop();

        return new IndexingResult
        {
            Success = success,
            IndexId = indexId,
            FilesIndexed = _processedFiles,
            FilesSkipped = _skippedFiles,
            FilesErrored = _failedFiles,
            ChunksCreated = _totalChunks,
            BytesProcessed = _processedBytes,
            Duration = _stopwatch.Elapsed,
            StartedAt = DateTime.UtcNow - _stopwatch.Elapsed,
            CompletedAt = DateTime.UtcNow,
            ErrorMessage = errorMessage
        };
    }
}
```

### File: `src/SeniorIntern.Services/Indexing/IndexingEventAggregator.cs`

```csharp
using System;
using System.Collections.Concurrent;
using System.Collections.Generic;
using System.Linq;
using SeniorIntern.Core.Models;

namespace SeniorIntern.Services.Indexing;

/// <summary>
/// Aggregates and manages indexing events.
/// </summary>
public sealed class IndexingEventAggregator
{
    private readonly ConcurrentQueue<IndexingEvent> _events = new();
    private readonly int _maxEvents;

    public IndexingEventAggregator(int maxEvents = 1000)
    {
        _maxEvents = maxEvents;
    }

    public void AddEvent(IndexingEventType type, string message, object? data = null)
    {
        var evt = new IndexingEvent
        {
            Type = type,
            Message = message,
            Data = data,
            Timestamp = DateTime.UtcNow
        };

        _events.Enqueue(evt);

        // Trim old events
        while (_events.Count > _maxEvents)
        {
            _events.TryDequeue(out _);
        }
    }

    public void FileStarted(string filePath)
    {
        AddEvent(IndexingEventType.FileStarted, $"Processing: {filePath}", filePath);
    }

    public void FileCompleted(string filePath, int chunkCount)
    {
        AddEvent(IndexingEventType.FileCompleted,
            $"Completed: {filePath} ({chunkCount} chunks)",
            new { FilePath = filePath, ChunkCount = chunkCount });
    }

    public void FileFailed(string filePath, string error)
    {
        AddEvent(IndexingEventType.FileFailed,
            $"Failed: {filePath} - {error}",
            new { FilePath = filePath, Error = error });
    }

    public void PhaseChanged(IndexingPhase phase)
    {
        AddEvent(IndexingEventType.PhaseChanged, $"Phase: {phase}", phase);
    }

    public void ProgressUpdate(IndexingProgress progress)
    {
        AddEvent(IndexingEventType.ProgressUpdate,
            $"Progress: {progress.PercentComplete:F1}%",
            progress);
    }

    public IReadOnlyList<IndexingEvent> GetRecentEvents(int count = 50)
    {
        return _events.Reverse().Take(count).Reverse().ToList();
    }

    public IReadOnlyList<IndexingEvent> GetEventsSince(DateTime since)
    {
        return _events.Where(e => e.Timestamp > since).ToList();
    }

    public void Clear()
    {
        while (_events.TryDequeue(out _)) { }
    }
}

/// <summary>
/// An indexing event.
/// </summary>
public sealed class IndexingEvent
{
    public required IndexingEventType Type { get; init; }
    public required string Message { get; init; }
    public object? Data { get; init; }
    public DateTime Timestamp { get; init; }
}

/// <summary>
/// Types of indexing events.
/// </summary>
public enum IndexingEventType
{
    FileStarted,
    FileCompleted,
    FileFailed,
    FileSkipped,
    PhaseChanged,
    ProgressUpdate,
    Warning,
    Error,
    Info
}
```

### File: `src/SeniorIntern.Services/Indexing/IndexingMetrics.cs`

```csharp
using System;
using System.Collections.Concurrent;
using System.Collections.Generic;
using System.Linq;

namespace SeniorIntern.Services.Indexing;

/// <summary>
/// Collects metrics about indexing performance.
/// </summary>
public sealed class IndexingMetrics
{
    private readonly ConcurrentDictionary<string, LanguageMetrics> _byLanguage = new();
    private readonly ConcurrentBag<FileMetrics> _fileMetrics = new();

    private long _totalFilesProcessed;
    private long _totalChunksCreated;
    private long _totalBytesProcessed;
    private long _totalEmbeddingTimeMs;
    private long _totalProcessingTimeMs;

    public void RecordFile(FileMetrics metrics)
    {
        _fileMetrics.Add(metrics);

        Interlocked.Increment(ref _totalFilesProcessed);
        Interlocked.Add(ref _totalChunksCreated, metrics.ChunkCount);
        Interlocked.Add(ref _totalBytesProcessed, metrics.FileSize);
        Interlocked.Add(ref _totalEmbeddingTimeMs, metrics.EmbeddingTimeMs);
        Interlocked.Add(ref _totalProcessingTimeMs, metrics.TotalTimeMs);

        if (!string.IsNullOrEmpty(metrics.Language))
        {
            var langMetrics = _byLanguage.GetOrAdd(metrics.Language, _ => new LanguageMetrics());
            langMetrics.Add(metrics);
        }
    }

    public IndexingMetricsSummary GetSummary()
    {
        var files = _fileMetrics.ToList();

        return new IndexingMetricsSummary
        {
            TotalFilesProcessed = _totalFilesProcessed,
            TotalChunksCreated = _totalChunksCreated,
            TotalBytesProcessed = _totalBytesProcessed,
            TotalEmbeddingTimeMs = _totalEmbeddingTimeMs,
            TotalProcessingTimeMs = _totalProcessingTimeMs,

            AverageChunksPerFile = _totalFilesProcessed > 0
                ? (double)_totalChunksCreated / _totalFilesProcessed
                : 0,
            AverageFileSizeBytes = _totalFilesProcessed > 0
                ? (double)_totalBytesProcessed / _totalFilesProcessed
                : 0,
            AverageEmbeddingTimeMs = _totalFilesProcessed > 0
                ? (double)_totalEmbeddingTimeMs / _totalFilesProcessed
                : 0,
            AverageProcessingTimeMs = _totalFilesProcessed > 0
                ? (double)_totalProcessingTimeMs / _totalFilesProcessed
                : 0,

            FilesPerSecond = _totalProcessingTimeMs > 0
                ? _totalFilesProcessed / (_totalProcessingTimeMs / 1000.0)
                : 0,
            ChunksPerSecond = _totalProcessingTimeMs > 0
                ? _totalChunksCreated / (_totalProcessingTimeMs / 1000.0)
                : 0,
            BytesPerSecond = _totalProcessingTimeMs > 0
                ? _totalBytesProcessed / (_totalProcessingTimeMs / 1000.0)
                : 0,

            ByLanguage = _byLanguage.ToDictionary(
                kv => kv.Key,
                kv => kv.Value.GetSummary())
        };
    }

    public void Reset()
    {
        _byLanguage.Clear();
        while (_fileMetrics.TryTake(out _)) { }
        _totalFilesProcessed = 0;
        _totalChunksCreated = 0;
        _totalBytesProcessed = 0;
        _totalEmbeddingTimeMs = 0;
        _totalProcessingTimeMs = 0;
    }
}

/// <summary>
/// Metrics for a single file.
/// </summary>
public sealed class FileMetrics
{
    public required string FilePath { get; init; }
    public string? Language { get; init; }
    public long FileSize { get; init; }
    public int LineCount { get; init; }
    public int ChunkCount { get; init; }
    public int EmbeddingTimeMs { get; init; }
    public int TotalTimeMs { get; init; }
}

/// <summary>
/// Aggregated metrics by language.
/// </summary>
public sealed class LanguageMetrics
{
    private long _fileCount;
    private long _totalChunks;
    private long _totalBytes;
    private long _totalTimeMs;

    public void Add(FileMetrics metrics)
    {
        Interlocked.Increment(ref _fileCount);
        Interlocked.Add(ref _totalChunks, metrics.ChunkCount);
        Interlocked.Add(ref _totalBytes, metrics.FileSize);
        Interlocked.Add(ref _totalTimeMs, metrics.TotalTimeMs);
    }

    public LanguageMetricsSummary GetSummary()
    {
        return new LanguageMetricsSummary
        {
            FileCount = _fileCount,
            TotalChunks = _totalChunks,
            TotalBytes = _totalBytes,
            AverageChunksPerFile = _fileCount > 0 ? (double)_totalChunks / _fileCount : 0
        };
    }
}

/// <summary>
/// Summary of indexing metrics.
/// </summary>
public sealed class IndexingMetricsSummary
{
    public long TotalFilesProcessed { get; init; }
    public long TotalChunksCreated { get; init; }
    public long TotalBytesProcessed { get; init; }
    public long TotalEmbeddingTimeMs { get; init; }
    public long TotalProcessingTimeMs { get; init; }

    public double AverageChunksPerFile { get; init; }
    public double AverageFileSizeBytes { get; init; }
    public double AverageEmbeddingTimeMs { get; init; }
    public double AverageProcessingTimeMs { get; init; }

    public double FilesPerSecond { get; init; }
    public double ChunksPerSecond { get; init; }
    public double BytesPerSecond { get; init; }

    public IReadOnlyDictionary<string, LanguageMetricsSummary> ByLanguage { get; init; } =
        new Dictionary<string, LanguageMetricsSummary>();
}

/// <summary>
/// Summary for a single language.
/// </summary>
public sealed class LanguageMetricsSummary
{
    public long FileCount { get; init; }
    public long TotalChunks { get; init; }
    public long TotalBytes { get; init; }
    public double AverageChunksPerFile { get; init; }
}
```

---

## v0.7.3j: Unit Testing & Integration

### Objective
Implement comprehensive tests for the indexing pipeline components.

### Files to Create

| File | Purpose |
|------|---------|
| `tests/SeniorIntern.Tests/Indexing/IndexingServiceTests.cs` | Core service tests |
| `tests/SeniorIntern.Tests/Indexing/FileProcessorTests.cs` | File processor tests |
| `tests/SeniorIntern.Tests/Indexing/GitignoreParserTests.cs` | Gitignore tests |
| `tests/SeniorIntern.Tests/Indexing/IncrementalIndexerTests.cs` | Incremental tests |
| `tests/SeniorIntern.Tests/Indexing/IndexingJobQueueTests.cs` | Job queue tests |
| `tests/SeniorIntern.Tests/Indexing/FileWatcherServiceTests.cs` | File watcher tests |
| `tests/SeniorIntern.Tests/Indexing/ProgressTrackerTests.cs` | Progress tests |
| `tests/SeniorIntern.Tests/Indexing/ChangeDetectorTests.cs` | Change detection tests |
| `tests/SeniorIntern.Tests/Indexing/IndexingOptionsTests.cs` | Options tests |
| `tests/SeniorIntern.Tests/Indexing/IndexingIntegrationTests.cs` | E2E tests |

(Full test implementations would follow the same patterns as v0.7.2j tests)

---

## Dependencies

### No New NuGet Packages Required
This version builds on existing infrastructure from v0.7.1 and v0.7.2.

### Internal Dependencies
- `IEmbeddingService` from v0.7.1
- `IChunkingService` from v0.7.1
- `IVectorStore` from v0.7.2
- `FileScanner` from v0.7.2
- `FileHasher` from v0.7.2

---

## Implementation Sequence

```
┌─────────────────────────────────────────────────────────────────────┐
│  v0.7.3a: Indexing Service Interface                               │
│  └─ IIndexingService, IIndexingJobQueue, IFileWatcherService       │
└──────────────────────────┬──────────────────────────────────────────┘
                           │
                           ▼
┌─────────────────────────────────────────────────────────────────────┐
│  v0.7.3b: Indexing Models & Options                                │
│  └─ IndexingOptions, IndexingProgress, IndexingResult, IndexingJob │
└──────────────────────────┬──────────────────────────────────────────┘
                           │
                           ▼
┌─────────────────────────────────────────────────────────────────────┐
│  v0.7.3c: Indexing Service Implementation                          │
│  └─ IndexingService main class + processing partial                │
└──────────────────────────┬──────────────────────────────────────────┘
                           │
                           ▼
┌─────────────────────────────────────────────────────────────────────┐
│  v0.7.3d: File Processing Pipeline                                 │
│  └─ FileProcessor, EmbeddingBatcher, Extensions                    │
└──────────────────────────┬──────────────────────────────────────────┘
                           │
                           ▼
┌─────────────────────────────────────────────────────────────────────┐
│  v0.7.3e: Gitignore Pattern Matching                               │
│  └─ GitignoreParser, GitignorePatternMatcher                       │
└──────────────────────────┬──────────────────────────────────────────┘
                           │
                           ▼
┌─────────────────────────────────────────────────────────────────────┐
│  v0.7.3f: Incremental Indexing Logic                               │
│  └─ IncrementalIndexer, ChangeDetector                             │
└──────────────────────────┬──────────────────────────────────────────┘
                           │
                           ▼
┌─────────────────────────────────────────────────────────────────────┐
│  v0.7.3g: Background Job Queue                                     │
│  └─ IndexingJobQueue, IndexingJobProcessor, EventArgs              │
└──────────────────────────┬──────────────────────────────────────────┘
                           │
                           ▼
┌─────────────────────────────────────────────────────────────────────┐
│  v0.7.3h: File Watcher Service                                     │
│  └─ FileWatcherService, Coordinator, BackgroundService             │
└──────────────────────────┬──────────────────────────────────────────┘
                           │
                           ▼
┌─────────────────────────────────────────────────────────────────────┐
│  v0.7.3i: Progress Tracking & Events                               │
│  └─ ProgressTracker, EventAggregator, Metrics                      │
└──────────────────────────┬──────────────────────────────────────────┘
                           │
                           ▼
┌─────────────────────────────────────────────────────────────────────┐
│  v0.7.3j: Unit Testing & Integration                               │
│  └─ All test files for indexing components                         │
└─────────────────────────────────────────────────────────────────────┘
```

---

## Summary

This design specification covers v0.7.3 Indexing Pipeline with 10 sub-parts:

| Sub-version | Focus | Key Deliverables |
|-------------|-------|------------------|
| **v0.7.3a** | Service Interface | IIndexingService, IIndexingJobQueue, IFileWatcherService |
| **v0.7.3b** | Models & Options | IndexingOptions, IndexingProgress, IndexingResult, IndexingJob |
| **v0.7.3c** | Service Implementation | IndexingService with processing partial |
| **v0.7.3d** | File Processing | FileProcessor, EmbeddingBatcher |
| **v0.7.3e** | Gitignore Matching | GitignoreParser, GitignoreMatcher |
| **v0.7.3f** | Incremental Indexing | IncrementalIndexer, ChangeDetector |
| **v0.7.3g** | Background Jobs | IndexingJobQueue, IndexingJobProcessor |
| **v0.7.3h** | File Watching | FileWatcherService, Coordinator |
| **v0.7.3i** | Progress & Events | ProgressTracker, EventAggregator, Metrics |
| **v0.7.3j** | Testing | Comprehensive unit and integration tests |

**Totals:**
- **37 files to create**
- **1 file to modify** (DI registration)
- **No new NuGet packages** (builds on v0.7.1 and v0.7.2)
