# Design Specification: The Senior Intern v0.8.0 "The Training Lab"

## Executive Summary

This document provides a comprehensive design specification for v0.8.0, which introduces local fine-tuning capabilities to The Senior Intern. This phase enables users to train LoRA (Low-Rank Adaptation) adapters on their own code, allowing the model to learn their specific coding style, project conventions, and domain knowledge—all running locally on consumer hardware.

### v0.8.0 Scope (from ROADMAP.md)
- Introduce the **ML.NET / TorchSharp** fine-tuning module
- UI for **LoRA (Low-Rank Adaptation)** training: The user provides a folder of high-quality code, and the Intern "learns" the user's specific coding style overnight
- Hardware Monitoring: Real-time graphs showing VRAM and CPU usage during training

---

## Sub-Version Breakdown

| Version | Name | Focus |
|---------|------|-------|
| v0.8.1 | Training Infrastructure | TorchSharp integration, CUDA/Metal detection, memory management |
| v0.8.2 | Dataset Pipeline | Code dataset preparation, tokenization, instruction formatting |
| v0.8.3 | LoRA Training Engine | LoRA implementation, training loop, checkpointing |
| v0.8.4 | Hardware Monitoring | GPU/CPU metrics, memory tracking, real-time graphs |
| v0.8.5 | Training UI & Polish | Training wizard, progress UI, adapter management |

---

## Architecture Overview

### Training System Architecture

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                              User Interface                                  │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐             │
│  │ Training Wizard │  │ Hardware Monitor│  │ Adapter Manager │             │
│  └────────┬────────┘  └────────┬────────┘  └────────┬────────┘             │
└───────────┼────────────────────┼────────────────────┼───────────────────────┘
            │                    │                    │
            ▼                    ▼                    ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                           Training Orchestrator                              │
│  ┌──────────────────────────────────────────────────────────────────────┐  │
│  │                        ITrainingService                               │  │
│  │  ┌────────────────┐  ┌────────────────┐  ┌────────────────┐         │  │
│  │  │ Dataset Manager│  │ Training Engine│  │Checkpoint Mgr  │         │  │
│  │  └────────────────┘  └────────────────┘  └────────────────┘         │  │
│  └──────────────────────────────────────────────────────────────────────┘  │
│                                    │                                        │
│                                    ▼                                        │
│  ┌──────────────────────────────────────────────────────────────────────┐  │
│  │                        Dataset Pipeline                               │  │
│  │  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐  ┌─────────┐   │  │
│  │  │  Code   │  │ Instruct│  │Tokenizer│  │ Batching│  │  Data   │   │  │
│  │  │ Scanner │→ │ Format  │→ │         │→ │         │→ │ Loader  │   │  │
│  │  └─────────┘  └─────────┘  └─────────┘  └─────────┘  └─────────┘   │  │
│  └──────────────────────────────────────────────────────────────────────┘  │
│                                    │                                        │
│                                    ▼                                        │
│  ┌──────────────────────────────────────────────────────────────────────┐  │
│  │                         LoRA Training Core                            │  │
│  │  ┌────────────────────────┐  ┌────────────────────────┐             │  │
│  │  │     TorchSharp         │  │    LLamaSharp Model    │             │  │
│  │  │  (Training Backend)    │  │    (Base Weights)      │             │  │
│  │  └────────────────────────┘  └────────────────────────┘             │  │
│  └──────────────────────────────────────────────────────────────────────┘  │
│                                    │                                        │
│                                    ▼                                        │
│  ┌──────────────────────────────────────────────────────────────────────┐  │
│  │                       Hardware Monitor                                │  │
│  │  ┌────────────────┐  ┌────────────────┐  ┌────────────────┐         │  │
│  │  │  GPU Metrics   │  │  CPU Metrics   │  │ Memory Tracker │         │  │
│  │  └────────────────┘  └────────────────┘  └────────────────┘         │  │
│  └──────────────────────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────────────────────┘
```

### LoRA Training Flow

```
User selects code folder
         │
         ▼
┌─────────────────┐
│  Scan & Filter  │ ← Find code files, apply quality filters
│   Code Files    │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│ Generate Train  │ ← Create instruction-response pairs
│    Examples     │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│   Tokenize &    │ ← Convert to token IDs, create batches
│     Batch       │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  Load Base      │ ← Load GGUF model weights
│     Model       │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│ Initialize LoRA │ ← Create low-rank adapter matrices
│    Adapters     │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  Training Loop  │ ← Forward pass, compute loss, backprop
│   (Epochs)      │ ← Update only LoRA weights
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│ Save Checkpoint │ ← Save LoRA weights (small file)
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│ Merge or Load   │ ← Use adapter with base model
│   Adapter       │
└─────────────────┘
```

---

## v0.8.1: Training Infrastructure

### Objective
Establish the TorchSharp-based training infrastructure with proper CUDA/Metal detection, memory management, and integration with the existing LLamaSharp model loading.

### Hardware Detection

```csharp
namespace SeniorIntern.Core.Training;

/// <summary>
/// Hardware capabilities for training
/// </summary>
public sealed class TrainingHardwareInfo
{
    public bool CudaAvailable { get; init; }
    public bool MetalAvailable { get; init; }
    public int CudaDeviceCount { get; init; }
    public IReadOnlyList<GpuDeviceInfo> GpuDevices { get; init; } = Array.Empty<GpuDeviceInfo>();
    public long TotalSystemMemory { get; init; }
    public long AvailableSystemMemory { get; init; }
    public int CpuCoreCount { get; init; }
    public string RecommendedDevice { get; init; } = "cpu";
    public int RecommendedBatchSize { get; init; } = 1;
}

public sealed class GpuDeviceInfo
{
    public int DeviceIndex { get; init; }
    public string Name { get; init; } = string.Empty;
    public long TotalMemory { get; init; }
    public long FreeMemory { get; init; }
    public GpuBackend Backend { get; init; }
    public int ComputeCapabilityMajor { get; init; }
    public int ComputeCapabilityMinor { get; init; }
}

public enum GpuBackend
{
    None,
    Cuda,
    Metal,
    Vulkan
}
```

### IHardwareDetectionService

```csharp
namespace SeniorIntern.Core.Interfaces;

public interface IHardwareDetectionService
{
    /// <summary>
    /// Detect available training hardware
    /// </summary>
    Task<TrainingHardwareInfo> DetectHardwareAsync(CancellationToken ct = default);

    /// <summary>
    /// Check if training is possible on this system
    /// </summary>
    Task<TrainingCapabilityResult> CheckTrainingCapabilityAsync(
        TrainingRequirements requirements,
        CancellationToken ct = default);

    /// <summary>
    /// Get recommended training configuration
    /// </summary>
    TrainingConfiguration GetRecommendedConfiguration(
        TrainingHardwareInfo hardware,
        long modelSizeBytes,
        int datasetSize);

    /// <summary>
    /// Event when hardware state changes (GPU memory, etc.)
    /// </summary>
    event EventHandler<HardwareStateChangedEventArgs>? HardwareStateChanged;
}

public sealed class TrainingRequirements
{
    public long MinimumVramBytes { get; init; }
    public long MinimumRamBytes { get; init; }
    public bool RequiresGpu { get; init; }
    public int MinimumComputeCapability { get; init; }
}

public sealed class TrainingCapabilityResult
{
    public bool CanTrain { get; init; }
    public string? Reason { get; init; }
    public IReadOnlyList<string> Warnings { get; init; } = Array.Empty<string>();
    public TrainingHardwareInfo Hardware { get; init; } = null!;
}
```

### Hardware Detection Implementation

```csharp
namespace SeniorIntern.Services.Training;

public sealed class HardwareDetectionService : IHardwareDetectionService
{
    private readonly ILogger<HardwareDetectionService> _logger;

    public async Task<TrainingHardwareInfo> DetectHardwareAsync(CancellationToken ct = default)
    {
        return await Task.Run(() =>
        {
            var info = new TrainingHardwareInfo
            {
                CudaAvailable = torch.cuda.is_available(),
                MetalAvailable = OperatingSystem.IsMacOS() && torch.mps.is_available(),
                CudaDeviceCount = torch.cuda.is_available() ? torch.cuda.device_count() : 0,
                TotalSystemMemory = GC.GetGCMemoryInfo().TotalAvailableMemoryBytes,
                AvailableSystemMemory = GC.GetGCMemoryInfo().TotalAvailableMemoryBytes -
                                        Process.GetCurrentProcess().WorkingSet64,
                CpuCoreCount = Environment.ProcessorCount
            };

            var gpuDevices = new List<GpuDeviceInfo>();

            // Detect CUDA devices
            if (info.CudaAvailable)
            {
                for (int i = 0; i < info.CudaDeviceCount; i++)
                {
                    var props = torch.cuda.get_device_properties(i);
                    gpuDevices.Add(new GpuDeviceInfo
                    {
                        DeviceIndex = i,
                        Name = props.name,
                        TotalMemory = (long)props.total_memory,
                        FreeMemory = GetCudaFreeMemory(i),
                        Backend = GpuBackend.Cuda,
                        ComputeCapabilityMajor = props.major,
                        ComputeCapabilityMinor = props.minor
                    });
                }
            }

            // Detect Metal (macOS)
            if (info.MetalAvailable)
            {
                gpuDevices.Add(new GpuDeviceInfo
                {
                    DeviceIndex = 0,
                    Name = GetMetalDeviceName(),
                    TotalMemory = GetMetalTotalMemory(),
                    FreeMemory = GetMetalFreeMemory(),
                    Backend = GpuBackend.Metal
                });
            }

            return info with
            {
                GpuDevices = gpuDevices,
                RecommendedDevice = DetermineRecommendedDevice(info, gpuDevices),
                RecommendedBatchSize = CalculateRecommendedBatchSize(gpuDevices)
            };
        }, ct);
    }

    public TrainingConfiguration GetRecommendedConfiguration(
        TrainingHardwareInfo hardware,
        long modelSizeBytes,
        int datasetSize)
    {
        // LoRA typically needs ~2-4x model size in VRAM for training
        var estimatedVramNeeded = modelSizeBytes * 3;
        var availableVram = hardware.GpuDevices.Sum(g => g.FreeMemory);

        var config = new TrainingConfiguration();

        if (availableVram >= estimatedVramNeeded)
        {
            // Can fit in GPU memory
            config = config with
            {
                Device = hardware.RecommendedDevice,
                BatchSize = CalculateBatchSizeForVram(availableVram, modelSizeBytes),
                GradientAccumulationSteps = 1,
                UseGradientCheckpointing = false
            };
        }
        else if (availableVram >= modelSizeBytes)
        {
            // Need gradient checkpointing and accumulation
            config = config with
            {
                Device = hardware.RecommendedDevice,
                BatchSize = 1,
                GradientAccumulationSteps = 8,
                UseGradientCheckpointing = true
            };
        }
        else
        {
            // CPU-only training
            config = config with
            {
                Device = "cpu",
                BatchSize = 1,
                GradientAccumulationSteps = 16,
                UseGradientCheckpointing = true
            };
        }

        // Adjust epochs based on dataset size
        config = config with
        {
            Epochs = datasetSize < 100 ? 10 : datasetSize < 1000 ? 5 : 3,
            WarmupSteps = Math.Max(10, datasetSize / 10)
        };

        return config;
    }

    private static long GetCudaFreeMemory(int deviceIndex)
    {
        // Use nvidia-smi or CUDA runtime API
        torch.cuda.set_device(deviceIndex);
        var (free, total) = torch.cuda.mem_get_info();
        return (long)free;
    }

    private static string DetermineRecommendedDevice(
        TrainingHardwareInfo info,
        List<GpuDeviceInfo> gpus)
    {
        if (info.MetalAvailable)
            return "mps";
        if (info.CudaAvailable && gpus.Count > 0)
            return $"cuda:{gpus.OrderByDescending(g => g.FreeMemory).First().DeviceIndex}";
        return "cpu";
    }

    private static int CalculateRecommendedBatchSize(List<GpuDeviceInfo> gpus)
    {
        if (gpus.Count == 0) return 1;

        var maxVram = gpus.Max(g => g.FreeMemory);
        // Rough heuristic: 8GB = batch 4, 16GB = batch 8, etc.
        return Math.Max(1, (int)(maxVram / (2L * 1024 * 1024 * 1024)));
    }
}
```

### Training Configuration

```csharp
namespace SeniorIntern.Core.Training;

public sealed record TrainingConfiguration
{
    // Hardware
    public string Device { get; init; } = "cpu";
    public bool UseMixedPrecision { get; init; } = true;
    public bool UseGradientCheckpointing { get; init; } = false;

    // Training hyperparameters
    public int Epochs { get; init; } = 3;
    public int BatchSize { get; init; } = 4;
    public int GradientAccumulationSteps { get; init; } = 1;
    public float LearningRate { get; init; } = 2e-4f;
    public float WeightDecay { get; init; } = 0.01f;
    public int WarmupSteps { get; init; } = 100;
    public string LrScheduler { get; init; } = "cosine";

    // LoRA specific
    public int LoraRank { get; init; } = 16;
    public float LoraAlpha { get; init; } = 32f;
    public float LoraDropout { get; init; } = 0.05f;
    public IReadOnlyList<string> TargetModules { get; init; } = new[]
    {
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    };

    // Checkpointing
    public int SaveEveryNSteps { get; init; } = 500;
    public int MaxCheckpoints { get; init; } = 3;
    public string CheckpointDir { get; init; } = string.Empty;

    // Data
    public int MaxSequenceLength { get; init; } = 2048;
    public bool PackSequences { get; init; } = true;

    // Validation
    public float ValidationSplit { get; init; } = 0.1f;
    public int EvalEveryNSteps { get; init; } = 100;
}
```

### v0.8.1 Files to Create

| File | Purpose |
|------|---------|
| `Core/Training/TrainingHardwareInfo.cs` | Hardware info models |
| `Core/Training/TrainingConfiguration.cs` | Training config model |
| `Core/Interfaces/IHardwareDetectionService.cs` | Hardware detection interface |
| `Services/Training/HardwareDetectionService.cs` | Hardware detection implementation |
| `Services/Training/TorchSharpInitializer.cs` | TorchSharp setup and initialization |

---

## v0.8.2: Dataset Pipeline

### Objective
Implement the dataset preparation pipeline for code-based fine-tuning, including instruction formatting, tokenization, and efficient batching.

### Dataset Models

```csharp
namespace SeniorIntern.Core.Training;

/// <summary>
/// A training example for instruction fine-tuning
/// </summary>
public sealed class TrainingExample
{
    public Guid Id { get; init; } = Guid.NewGuid();
    public string Instruction { get; init; } = string.Empty;
    public string Input { get; init; } = string.Empty;
    public string Output { get; init; } = string.Empty;
    public string? SystemPrompt { get; init; }
    public ExampleSource Source { get; init; }
    public IReadOnlyDictionary<string, string>? Metadata { get; init; }
}

public sealed class ExampleSource
{
    public string FilePath { get; init; } = string.Empty;
    public int StartLine { get; init; }
    public int EndLine { get; init; }
    public string? Language { get; init; }
    public string? SymbolName { get; init; }
}

/// <summary>
/// Tokenized training example ready for training
/// </summary>
public sealed class TokenizedExample
{
    public int[] InputIds { get; init; } = Array.Empty<int>();
    public int[] AttentionMask { get; init; } = Array.Empty<int>();
    public int[] Labels { get; init; } = Array.Empty<int>();
    public int SequenceLength { get; init; }
}

/// <summary>
/// A batch of tokenized examples
/// </summary>
public sealed class TrainingBatch
{
    public torch.Tensor InputIds { get; init; } = null!;
    public torch.Tensor AttentionMask { get; init; } = null!;
    public torch.Tensor Labels { get; init; } = null!;
    public int BatchSize { get; init; }
    public int SequenceLength { get; init; }
}

/// <summary>
/// Dataset statistics
/// </summary>
public sealed class DatasetStatistics
{
    public int TotalExamples { get; init; }
    public int TotalTokens { get; init; }
    public int TrainExamples { get; init; }
    public int ValidationExamples { get; init; }
    public double AverageTokensPerExample { get; init; }
    public int MaxSequenceLength { get; init; }
    public Dictionary<string, int> ExamplesByLanguage { get; init; } = new();
    public Dictionary<string, int> ExamplesByType { get; init; } = new();
}
```

### IDatasetService Interface

```csharp
namespace SeniorIntern.Core.Interfaces;

public interface IDatasetService
{
    /// <summary>
    /// Create a dataset from a code folder
    /// </summary>
    Task<TrainingDataset> CreateDatasetFromCodeAsync(
        string folderPath,
        DatasetCreationOptions options,
        IProgress<DatasetCreationProgress>? progress = null,
        CancellationToken ct = default);

    /// <summary>
    /// Load an existing dataset
    /// </summary>
    Task<TrainingDataset> LoadDatasetAsync(
        string datasetPath,
        CancellationToken ct = default);

    /// <summary>
    /// Save a dataset to disk
    /// </summary>
    Task SaveDatasetAsync(
        TrainingDataset dataset,
        string outputPath,
        CancellationToken ct = default);

    /// <summary>
    /// Tokenize a dataset for training
    /// </summary>
    Task<TokenizedDataset> TokenizeDatasetAsync(
        TrainingDataset dataset,
        TokenizationOptions options,
        IProgress<TokenizationProgress>? progress = null,
        CancellationToken ct = default);

    /// <summary>
    /// Create data loaders for training
    /// </summary>
    (IDataLoader Train, IDataLoader Validation) CreateDataLoaders(
        TokenizedDataset dataset,
        DataLoaderOptions options);

    /// <summary>
    /// Get dataset statistics
    /// </summary>
    DatasetStatistics GetStatistics(TrainingDataset dataset);
}

public interface IDataLoader : IEnumerable<TrainingBatch>, IDisposable
{
    int TotalBatches { get; }
    int CurrentBatch { get; }
    void Reset();
    void Shuffle();
}
```

### Dataset Creation Options

```csharp
namespace SeniorIntern.Core.Training;

public sealed class DatasetCreationOptions
{
    /// <summary>
    /// Types of examples to generate
    /// </summary>
    public ExampleGenerationType GenerationType { get; init; } =
        ExampleGenerationType.All;

    /// <summary>
    /// File patterns to include
    /// </summary>
    public IReadOnlyList<string> IncludePatterns { get; init; } = new[]
    {
        "**/*.cs", "**/*.ts", "**/*.js", "**/*.py"
    };

    /// <summary>
    /// File patterns to exclude
    /// </summary>
    public IReadOnlyList<string> ExcludePatterns { get; init; } = new[]
    {
        "**/node_modules/**", "**/bin/**", "**/obj/**", "**/*.min.js"
    };

    /// <summary>
    /// Minimum lines of code for a valid example
    /// </summary>
    public int MinCodeLines { get; init; } = 5;

    /// <summary>
    /// Maximum lines of code for a single example
    /// </summary>
    public int MaxCodeLines { get; init; } = 100;

    /// <summary>
    /// Whether to include comments as training data
    /// </summary>
    public bool IncludeComments { get; init; } = true;

    /// <summary>
    /// Whether to deduplicate similar examples
    /// </summary>
    public bool Deduplicate { get; init; } = true;

    /// <summary>
    /// Similarity threshold for deduplication (0-1)
    /// </summary>
    public float DeduplicationThreshold { get; init; } = 0.9f;

    /// <summary>
    /// Custom instruction templates
    /// </summary>
    public IReadOnlyDictionary<string, string>? CustomTemplates { get; init; }
}

[Flags]
public enum ExampleGenerationType
{
    None = 0,
    CodeCompletion = 1,      // Complete partial code
    CodeExplanation = 2,     // Explain what code does
    CodeRefactoring = 4,     // Suggest improvements
    DocstringGeneration = 8, // Generate documentation
    BugFinding = 16,         // Identify potential bugs
    TestGeneration = 32,     // Generate unit tests
    All = CodeCompletion | CodeExplanation | CodeRefactoring |
          DocstringGeneration | BugFinding | TestGeneration
}
```

### Instruction Template Generator

```csharp
namespace SeniorIntern.Services.Training;

public sealed class InstructionTemplateGenerator
{
    private readonly Random _random = new();

    private static readonly Dictionary<ExampleGenerationType, string[]> Templates = new()
    {
        [ExampleGenerationType.CodeCompletion] = new[]
        {
            "Complete the following {language} code:\n\n{input}",
            "Finish implementing this {language} function:\n\n{input}",
            "Continue the code below:\n\n{input}",
            "Write the rest of this {language} code:\n\n{input}"
        },
        [ExampleGenerationType.CodeExplanation] = new[]
        {
            "Explain what this {language} code does:\n\n{input}",
            "Describe the purpose of this code:\n\n{input}",
            "What does the following {language} function do?\n\n{input}",
            "Analyze this code and explain its behavior:\n\n{input}"
        },
        [ExampleGenerationType.CodeRefactoring] = new[]
        {
            "Refactor this {language} code to improve readability:\n\n{input}",
            "Suggest improvements for this code:\n\n{input}",
            "Optimize the following {language} code:\n\n{input}",
            "Clean up this code following best practices:\n\n{input}"
        },
        [ExampleGenerationType.DocstringGeneration] = new[]
        {
            "Write documentation for this {language} function:\n\n{input}",
            "Generate a docstring for the following code:\n\n{input}",
            "Add XML documentation comments to this code:\n\n{input}",
            "Document this {language} class/method:\n\n{input}"
        },
        [ExampleGenerationType.BugFinding] = new[]
        {
            "Find potential bugs in this {language} code:\n\n{input}",
            "Review this code for issues:\n\n{input}",
            "What problems might this code have?\n\n{input}",
            "Identify any errors or edge cases in:\n\n{input}"
        },
        [ExampleGenerationType.TestGeneration] = new[]
        {
            "Write unit tests for this {language} code:\n\n{input}",
            "Generate test cases for the following function:\n\n{input}",
            "Create comprehensive tests for this code:\n\n{input}",
            "Write test methods to verify this code works:\n\n{input}"
        }
    };

    public IEnumerable<TrainingExample> GenerateExamples(
        CodeSymbol symbol,
        string fileContent,
        ExampleGenerationType types)
    {
        foreach (ExampleGenerationType type in Enum.GetValues<ExampleGenerationType>())
        {
            if (type == ExampleGenerationType.None ||
                type == ExampleGenerationType.All ||
                !types.HasFlag(type))
                continue;

            var example = GenerateExample(symbol, fileContent, type);
            if (example != null)
                yield return example;
        }
    }

    private TrainingExample? GenerateExample(
        CodeSymbol symbol,
        string fileContent,
        ExampleGenerationType type)
    {
        var templates = Templates[type];
        var template = templates[_random.Next(templates.Length)];

        var instruction = template
            .Replace("{language}", symbol.Language)
            .Replace("{input}", "");

        var (input, output) = type switch
        {
            ExampleGenerationType.CodeCompletion =>
                GenerateCompletionPair(symbol, fileContent),
            ExampleGenerationType.CodeExplanation =>
                (symbol.Content, GenerateExplanation(symbol)),
            ExampleGenerationType.CodeRefactoring =>
                (symbol.Content, symbol.Content), // Same for now, can enhance
            ExampleGenerationType.DocstringGeneration =>
                GenerateDocstringPair(symbol),
            ExampleGenerationType.BugFinding =>
                (symbol.Content, "No obvious bugs found."),
            ExampleGenerationType.TestGeneration =>
                (symbol.Content, GenerateTestSkeleton(symbol)),
            _ => (string.Empty, string.Empty)
        };

        if (string.IsNullOrEmpty(input) || string.IsNullOrEmpty(output))
            return null;

        return new TrainingExample
        {
            Instruction = instruction,
            Input = input,
            Output = output,
            Source = new ExampleSource
            {
                FilePath = symbol.FilePath,
                StartLine = symbol.StartLine,
                EndLine = symbol.EndLine,
                Language = symbol.Language,
                SymbolName = symbol.Name
            }
        };
    }

    private (string Input, string Output) GenerateCompletionPair(
        CodeSymbol symbol,
        string content)
    {
        var lines = content.Split('\n');
        var symbolLines = lines
            .Skip(symbol.StartLine - 1)
            .Take(symbol.EndLine - symbol.StartLine + 1)
            .ToArray();

        if (symbolLines.Length < 4)
            return (string.Empty, string.Empty);

        // Split roughly in half
        var splitPoint = symbolLines.Length / 2;
        var input = string.Join('\n', symbolLines.Take(splitPoint));
        var output = string.Join('\n', symbolLines.Skip(splitPoint));

        return (input, output);
    }

    private string GenerateExplanation(CodeSymbol symbol)
    {
        // Basic explanation template - in production, could use the LLM itself
        return symbol.SymbolType switch
        {
            SymbolType.Class =>
                $"This is a {symbol.Language} class named '{symbol.Name}' that {InferPurpose(symbol)}.",
            SymbolType.Method or SymbolType.Function =>
                $"This {symbol.Language} {symbol.SymbolType.ToString().ToLower()} '{symbol.Name}' {InferPurpose(symbol)}.",
            SymbolType.Interface =>
                $"This interface '{symbol.Name}' defines a contract for {InferPurpose(symbol)}.",
            _ => $"This code defines '{symbol.Name}' in {symbol.Language}."
        };
    }

    private (string Input, string Output) GenerateDocstringPair(CodeSymbol symbol)
    {
        var input = symbol.Content;
        var docstring = symbol.Language switch
        {
            "csharp" => GenerateCSharpDocstring(symbol),
            "python" => GeneratePythonDocstring(symbol),
            "typescript" or "javascript" => GenerateJsDocstring(symbol),
            _ => $"/// <summary>\n/// {symbol.Name}\n/// </summary>"
        };

        return (input, docstring + "\n" + input);
    }

    private string GenerateTestSkeleton(CodeSymbol symbol)
    {
        return symbol.Language switch
        {
            "csharp" => $@"[Fact]
public void {symbol.Name}_ShouldWorkCorrectly()
{{
    // Arrange

    // Act

    // Assert
    Assert.True(false, ""Test not implemented"");
}}",
            "python" => $@"def test_{ToSnakeCase(symbol.Name)}():
    # Arrange

    # Act

    # Assert
    assert False, ""Test not implemented""",
            _ => "// TODO: Implement test"
        };
    }

    private static string InferPurpose(CodeSymbol symbol)
    {
        // Simple heuristic based on naming
        var name = symbol.Name.ToLower();
        if (name.Contains("get")) return "retrieves data";
        if (name.Contains("set")) return "sets a value";
        if (name.Contains("create")) return "creates a new instance";
        if (name.Contains("delete") || name.Contains("remove")) return "removes an item";
        if (name.Contains("update")) return "updates existing data";
        if (name.Contains("validate")) return "validates input";
        if (name.Contains("parse")) return "parses input data";
        if (name.Contains("save")) return "persists data";
        if (name.Contains("load")) return "loads data";
        return "performs an operation";
    }

    private static string ToSnakeCase(string name) =>
        string.Concat(name.Select((c, i) =>
            i > 0 && char.IsUpper(c) ? "_" + char.ToLower(c) : char.ToLower(c).ToString()));
}
```

### Tokenization Service

```csharp
namespace SeniorIntern.Services.Training;

public sealed class TokenizationService
{
    private readonly ILogger<TokenizationService> _logger;
    private LLamaContext? _tokenizerContext;

    /// <summary>
    /// Initialize tokenizer from model
    /// </summary>
    public async Task InitializeAsync(string modelPath, CancellationToken ct = default)
    {
        var modelParams = new ModelParams(modelPath)
        {
            ContextSize = 512, // Small context for tokenization only
            GpuLayerCount = 0  // CPU only for tokenizer
        };

        var model = await LLamaWeights.LoadFromFileAsync(modelParams, ct);
        _tokenizerContext = model.CreateContext(modelParams);
    }

    /// <summary>
    /// Tokenize a training example using chat template
    /// </summary>
    public TokenizedExample TokenizeExample(
        TrainingExample example,
        TokenizationOptions options)
    {
        // Format as chat conversation
        var formattedText = FormatAsChatTemplate(example, options);

        // Tokenize
        var tokens = _tokenizerContext!.Tokenize(formattedText, addBos: true);
        var tokenArray = tokens.ToArray();

        // Truncate if needed
        if (tokenArray.Length > options.MaxLength)
        {
            tokenArray = tokenArray.Take(options.MaxLength).ToArray();
        }

        // Create labels (same as input for causal LM, -100 for padding)
        var labels = tokenArray.ToArray();

        // Mask instruction tokens if using instruction masking
        if (options.MaskInstructionTokens)
        {
            var instructionLength = GetInstructionLength(example, options);
            for (int i = 0; i < Math.Min(instructionLength, labels.Length); i++)
            {
                labels[i] = -100; // Ignore in loss computation
            }
        }

        // Create attention mask
        var attentionMask = Enumerable.Repeat(1, tokenArray.Length).ToArray();

        return new TokenizedExample
        {
            InputIds = tokenArray,
            Labels = labels,
            AttentionMask = attentionMask,
            SequenceLength = tokenArray.Length
        };
    }

    private string FormatAsChatTemplate(
        TrainingExample example,
        TokenizationOptions options)
    {
        // ChatML format (common for fine-tuning)
        var sb = new StringBuilder();

        if (!string.IsNullOrEmpty(example.SystemPrompt))
        {
            sb.AppendLine("<|im_start|>system");
            sb.AppendLine(example.SystemPrompt);
            sb.AppendLine("<|im_end|>");
        }

        sb.AppendLine("<|im_start|>user");
        sb.AppendLine(example.Instruction);
        if (!string.IsNullOrEmpty(example.Input))
        {
            sb.AppendLine();
            sb.AppendLine(example.Input);
        }
        sb.AppendLine("<|im_end|>");

        sb.AppendLine("<|im_start|>assistant");
        sb.AppendLine(example.Output);
        sb.Append("<|im_end|>");

        return sb.ToString();
    }
}

public sealed class TokenizationOptions
{
    public int MaxLength { get; init; } = 2048;
    public bool MaskInstructionTokens { get; init; } = true;
    public string ChatTemplate { get; init; } = "chatml";
    public bool AddEosToken { get; init; } = true;
    public int PadTokenId { get; init; } = 0;
}
```

### v0.8.2 Files to Create

| File | Purpose |
|------|---------|
| `Core/Training/TrainingExample.cs` | Training example models |
| `Core/Training/DatasetModels.cs` | Dataset and batch models |
| `Core/Training/DatasetCreationOptions.cs` | Dataset creation options |
| `Core/Interfaces/IDatasetService.cs` | Dataset service interface |
| `Services/Training/DatasetService.cs` | Dataset preparation implementation |
| `Services/Training/InstructionTemplateGenerator.cs` | Instruction generation |
| `Services/Training/TokenizationService.cs` | Tokenization implementation |
| `Services/Training/CodeSymbolExtractor.cs` | Extract code symbols for examples |

---

## v0.8.3: LoRA Training Engine

### Objective
Implement the LoRA training algorithm using TorchSharp, including the training loop, optimizer, scheduler, loss computation, and checkpoint management.

### LoRA Layer Implementation

```csharp
namespace SeniorIntern.Services.Training;

/// <summary>
/// Low-Rank Adaptation layer that wraps a linear layer
/// </summary>
public sealed class LoraLayer : torch.nn.Module<torch.Tensor, torch.Tensor>
{
    private readonly torch.nn.Linear _originalLayer;
    private readonly torch.nn.Linear _loraA;
    private readonly torch.nn.Linear _loraB;
    private readonly float _scaling;
    private readonly torch.nn.Dropout _dropout;
    private readonly bool _fanInFanOut;

    public int Rank { get; }
    public float Alpha { get; }
    public bool Merged { get; private set; }

    public LoraLayer(
        torch.nn.Linear originalLayer,
        int rank,
        float alpha,
        float dropout = 0.0f,
        bool fanInFanOut = false)
        : base(nameof(LoraLayer))
    {
        _originalLayer = originalLayer;
        Rank = rank;
        Alpha = alpha;
        _scaling = alpha / rank;
        _fanInFanOut = fanInFanOut;

        var inFeatures = originalLayer.weight!.shape[1];
        var outFeatures = originalLayer.weight!.shape[0];

        // LoRA matrices: W = W0 + BA where B is out x r and A is r x in
        _loraA = torch.nn.Linear(inFeatures, rank, hasBias: false);
        _loraB = torch.nn.Linear(rank, outFeatures, hasBias: false);
        _dropout = torch.nn.Dropout(dropout);

        // Initialize A with Kaiming uniform, B with zeros
        torch.nn.init.kaiming_uniform_(_loraA.weight!, a: Math.Sqrt(5));
        torch.nn.init.zeros_(_loraB.weight!);

        // Freeze original weights
        _originalLayer.weight!.requires_grad = false;
        if (_originalLayer.bias is not null)
            _originalLayer.bias.requires_grad = false;

        RegisterComponents();
    }

    public override torch.Tensor forward(torch.Tensor input)
    {
        if (Merged)
        {
            return _originalLayer.forward(input);
        }

        var originalOutput = _originalLayer.forward(input);
        var loraOutput = _loraB.forward(_loraA.forward(_dropout.forward(input)));

        return originalOutput + (loraOutput * _scaling);
    }

    /// <summary>
    /// Merge LoRA weights into original layer (for inference)
    /// </summary>
    public void Merge()
    {
        if (Merged) return;

        using (torch.no_grad())
        {
            var delta = torch.mm(_loraB.weight!, _loraA.weight!) * _scaling;
            _originalLayer.weight!.add_(delta);
        }

        Merged = true;
    }

    /// <summary>
    /// Unmerge LoRA weights (for continued training)
    /// </summary>
    public void Unmerge()
    {
        if (!Merged) return;

        using (torch.no_grad())
        {
            var delta = torch.mm(_loraB.weight!, _loraA.weight!) * _scaling;
            _originalLayer.weight!.sub_(delta);
        }

        Merged = false;
    }

    /// <summary>
    /// Get LoRA weights for saving
    /// </summary>
    public Dictionary<string, torch.Tensor> GetLoraWeights()
    {
        return new Dictionary<string, torch.Tensor>
        {
            ["lora_A"] = _loraA.weight!.clone(),
            ["lora_B"] = _loraB.weight!.clone()
        };
    }

    /// <summary>
    /// Load LoRA weights
    /// </summary>
    public void LoadLoraWeights(Dictionary<string, torch.Tensor> weights)
    {
        _loraA.weight!.copy_(weights["lora_A"]);
        _loraB.weight!.copy_(weights["lora_B"]);
    }
}
```

### ITrainingService Interface

```csharp
namespace SeniorIntern.Core.Interfaces;

public interface ITrainingService
{
    /// <summary>
    /// Current training state
    /// </summary>
    TrainingState State { get; }

    /// <summary>
    /// Start a training run
    /// </summary>
    Task<TrainingResult> TrainAsync(
        TrainingJob job,
        IProgress<TrainingProgress>? progress = null,
        CancellationToken ct = default);

    /// <summary>
    /// Resume training from checkpoint
    /// </summary>
    Task<TrainingResult> ResumeTrainingAsync(
        string checkpointPath,
        IProgress<TrainingProgress>? progress = null,
        CancellationToken ct = default);

    /// <summary>
    /// Pause current training
    /// </summary>
    Task PauseTrainingAsync();

    /// <summary>
    /// Cancel current training
    /// </summary>
    void CancelTraining();

    /// <summary>
    /// Load a trained adapter
    /// </summary>
    Task<LoraAdapter> LoadAdapterAsync(
        string adapterPath,
        CancellationToken ct = default);

    /// <summary>
    /// List available adapters
    /// </summary>
    Task<IReadOnlyList<LoraAdapterInfo>> ListAdaptersAsync(
        CancellationToken ct = default);

    /// <summary>
    /// Delete an adapter
    /// </summary>
    Task DeleteAdapterAsync(string adapterId, CancellationToken ct = default);

    /// <summary>
    /// Event for training progress updates
    /// </summary>
    event EventHandler<TrainingProgressEventArgs>? ProgressChanged;

    /// <summary>
    /// Event for training state changes
    /// </summary>
    event EventHandler<TrainingStateChangedEventArgs>? StateChanged;
}

public enum TrainingState
{
    Idle,
    Preparing,
    Training,
    Paused,
    Evaluating,
    Saving,
    Completed,
    Failed,
    Cancelled
}
```

### Training Job & Result Models

```csharp
namespace SeniorIntern.Core.Training;

public sealed class TrainingJob
{
    public Guid Id { get; init; } = Guid.NewGuid();
    public string Name { get; init; } = string.Empty;
    public string BaseModelPath { get; init; } = string.Empty;
    public string DatasetPath { get; init; } = string.Empty;
    public string OutputDir { get; init; } = string.Empty;
    public TrainingConfiguration Configuration { get; init; } = new();
    public DateTime CreatedAt { get; init; } = DateTime.UtcNow;
    public Dictionary<string, string>? Tags { get; init; }
}

public sealed class TrainingResult
{
    public Guid JobId { get; init; }
    public bool Success { get; init; }
    public string? AdapterPath { get; init; }
    public TrainingMetrics FinalMetrics { get; init; } = new();
    public TimeSpan TotalDuration { get; init; }
    public int TotalSteps { get; init; }
    public int TotalEpochs { get; init; }
    public string? ErrorMessage { get; init; }
    public IReadOnlyList<string> Checkpoints { get; init; } = Array.Empty<string>();
    public IReadOnlyList<TrainingMetrics> MetricsHistory { get; init; } = Array.Empty<TrainingMetrics>();
}

public sealed class TrainingProgress
{
    public int CurrentEpoch { get; init; }
    public int TotalEpochs { get; init; }
    public int CurrentStep { get; init; }
    public int TotalSteps { get; init; }
    public int SamplesProcessed { get; init; }
    public int TotalSamples { get; init; }
    public double PercentComplete { get; init; }
    public TrainingMetrics CurrentMetrics { get; init; } = new();
    public TimeSpan Elapsed { get; init; }
    public TimeSpan? EstimatedRemaining { get; init; }
    public string? CurrentOperation { get; init; }
    public HardwareMetrics HardwareMetrics { get; init; } = new();
}

public sealed class TrainingMetrics
{
    public int Step { get; init; }
    public int Epoch { get; init; }
    public float Loss { get; init; }
    public float? ValidationLoss { get; init; }
    public float LearningRate { get; init; }
    public float GradientNorm { get; init; }
    public float Perplexity => MathF.Exp(Loss);
    public DateTime Timestamp { get; init; } = DateTime.UtcNow;
}

public sealed class HardwareMetrics
{
    public float GpuUtilization { get; init; }
    public long GpuMemoryUsed { get; init; }
    public long GpuMemoryTotal { get; init; }
    public float CpuUtilization { get; init; }
    public long RamUsed { get; init; }
    public long RamTotal { get; init; }
    public float GpuTemperature { get; init; }
}
```

### Training Engine Implementation

```csharp
namespace SeniorIntern.Services.Training;

public sealed class LoraTrainingEngine : ITrainingService
{
    private readonly IHardwareDetectionService _hardwareService;
    private readonly IDatasetService _datasetService;
    private readonly ILogger<LoraTrainingEngine> _logger;

    private CancellationTokenSource? _trainCts;
    private TrainingState _state = TrainingState.Idle;
    private TrainingJob? _currentJob;

    public TrainingState State => _state;

    public event EventHandler<TrainingProgressEventArgs>? ProgressChanged;
    public event EventHandler<TrainingStateChangedEventArgs>? StateChanged;

    public async Task<TrainingResult> TrainAsync(
        TrainingJob job,
        IProgress<TrainingProgress>? progress = null,
        CancellationToken ct = default)
    {
        if (_state != TrainingState.Idle)
            throw new InvalidOperationException($"Cannot start training in state: {_state}");

        _currentJob = job;
        _trainCts = CancellationTokenSource.CreateLinkedTokenSource(ct);
        var stopwatch = Stopwatch.StartNew();
        var metricsHistory = new List<TrainingMetrics>();

        try
        {
            SetState(TrainingState.Preparing);

            // 1. Detect and configure hardware
            var hardware = await _hardwareService.DetectHardwareAsync(_trainCts.Token);
            var device = torch.device(job.Configuration.Device);

            _logger.LogInformation(
                "Training on device: {Device}, GPU Memory: {Memory}GB",
                device, hardware.GpuDevices.FirstOrDefault()?.TotalMemory / 1e9);

            // 2. Load base model
            progress?.Report(new TrainingProgress
            {
                CurrentOperation = "Loading base model..."
            });

            var baseModel = await LoadBaseModelAsync(
                job.BaseModelPath, device, _trainCts.Token);

            // 3. Apply LoRA adapters
            progress?.Report(new TrainingProgress
            {
                CurrentOperation = "Initializing LoRA adapters..."
            });

            var loraModel = ApplyLoraAdapters(baseModel, job.Configuration);

            // 4. Load and prepare dataset
            progress?.Report(new TrainingProgress
            {
                CurrentOperation = "Loading dataset..."
            });

            var dataset = await _datasetService.LoadDatasetAsync(
                job.DatasetPath, _trainCts.Token);
            var tokenizedDataset = await _datasetService.TokenizeDatasetAsync(
                dataset,
                new TokenizationOptions { MaxLength = job.Configuration.MaxSequenceLength },
                null,
                _trainCts.Token);

            var (trainLoader, validLoader) = _datasetService.CreateDataLoaders(
                tokenizedDataset,
                new DataLoaderOptions
                {
                    BatchSize = job.Configuration.BatchSize,
                    Shuffle = true,
                    DropLast = true
                });

            // 5. Setup optimizer and scheduler
            var optimizer = CreateOptimizer(loraModel, job.Configuration);
            var scheduler = CreateScheduler(
                optimizer, trainLoader.TotalBatches, job.Configuration);

            // 6. Training loop
            SetState(TrainingState.Training);

            var totalSteps = 0;
            var bestValLoss = float.MaxValue;

            for (int epoch = 0; epoch < job.Configuration.Epochs; epoch++)
            {
                _trainCts.Token.ThrowIfCancellationRequested();

                // Training epoch
                var epochMetrics = await TrainEpochAsync(
                    loraModel,
                    trainLoader,
                    optimizer,
                    scheduler,
                    device,
                    job.Configuration,
                    epoch,
                    totalSteps,
                    progress,
                    metricsHistory,
                    _trainCts.Token);

                totalSteps += trainLoader.TotalBatches;

                // Validation
                if (validLoader != null)
                {
                    SetState(TrainingState.Evaluating);
                    var valLoss = await EvaluateAsync(
                        loraModel, validLoader, device, _trainCts.Token);

                    epochMetrics = epochMetrics with { ValidationLoss = valLoss };

                    if (valLoss < bestValLoss)
                    {
                        bestValLoss = valLoss;
                        await SaveCheckpointAsync(
                            loraModel, optimizer, epoch, totalSteps,
                            job.OutputDir, "best", _trainCts.Token);
                    }

                    SetState(TrainingState.Training);
                }

                // Save epoch checkpoint
                if (epoch % job.Configuration.SaveEveryNSteps == 0 ||
                    epoch == job.Configuration.Epochs - 1)
                {
                    await SaveCheckpointAsync(
                        loraModel, optimizer, epoch, totalSteps,
                        job.OutputDir, $"epoch-{epoch}", _trainCts.Token);
                }

                trainLoader.Shuffle();
            }

            // 7. Save final adapter
            SetState(TrainingState.Saving);
            var adapterPath = await SaveFinalAdapterAsync(
                loraModel, job, _trainCts.Token);

            stopwatch.Stop();
            SetState(TrainingState.Completed);

            return new TrainingResult
            {
                JobId = job.Id,
                Success = true,
                AdapterPath = adapterPath,
                FinalMetrics = metricsHistory.LastOrDefault() ?? new(),
                TotalDuration = stopwatch.Elapsed,
                TotalSteps = totalSteps,
                TotalEpochs = job.Configuration.Epochs,
                MetricsHistory = metricsHistory
            };
        }
        catch (OperationCanceledException)
        {
            SetState(TrainingState.Cancelled);
            return new TrainingResult
            {
                JobId = job.Id,
                Success = false,
                ErrorMessage = "Training was cancelled",
                TotalDuration = stopwatch.Elapsed,
                MetricsHistory = metricsHistory
            };
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Training failed");
            SetState(TrainingState.Failed);
            return new TrainingResult
            {
                JobId = job.Id,
                Success = false,
                ErrorMessage = ex.Message,
                TotalDuration = stopwatch.Elapsed,
                MetricsHistory = metricsHistory
            };
        }
        finally
        {
            _trainCts?.Dispose();
            _trainCts = null;
            _currentJob = null;
        }
    }

    private async Task<TrainingMetrics> TrainEpochAsync(
        LoraModel model,
        IDataLoader trainLoader,
        torch.optim.Optimizer optimizer,
        torch.optim.lr_scheduler.LRScheduler scheduler,
        torch.Device device,
        TrainingConfiguration config,
        int epoch,
        int globalStep,
        IProgress<TrainingProgress>? progress,
        List<TrainingMetrics> metricsHistory,
        CancellationToken ct)
    {
        model.train();
        var totalLoss = 0.0f;
        var batchCount = 0;
        var accumSteps = 0;

        optimizer.zero_grad();

        foreach (var batch in trainLoader)
        {
            ct.ThrowIfCancellationRequested();

            // Move batch to device
            var inputIds = batch.InputIds.to(device);
            var attentionMask = batch.AttentionMask.to(device);
            var labels = batch.Labels.to(device);

            // Forward pass
            using var disposeScope = torch.NewDisposeScope();

            var outputs = model.forward(inputIds, attentionMask);
            var loss = ComputeLoss(outputs, labels);

            // Scale loss for gradient accumulation
            var scaledLoss = loss / config.GradientAccumulationSteps;
            scaledLoss.backward();

            totalLoss += loss.item<float>();
            accumSteps++;

            // Update weights after accumulation
            if (accumSteps >= config.GradientAccumulationSteps)
            {
                // Gradient clipping
                var gradNorm = torch.nn.utils.clip_grad_norm_(
                    model.parameters(), max_norm: 1.0f);

                optimizer.step();
                scheduler.step();
                optimizer.zero_grad();

                globalStep++;
                accumSteps = 0;

                // Log metrics
                var metrics = new TrainingMetrics
                {
                    Step = globalStep,
                    Epoch = epoch,
                    Loss = totalLoss / (batchCount + 1),
                    LearningRate = (float)scheduler.get_last_lr()[0],
                    GradientNorm = gradNorm.item<float>()
                };
                metricsHistory.Add(metrics);

                // Report progress
                var hwMetrics = await GetHardwareMetricsAsync();
                progress?.Report(new TrainingProgress
                {
                    CurrentEpoch = epoch + 1,
                    TotalEpochs = config.Epochs,
                    CurrentStep = globalStep,
                    TotalSteps = trainLoader.TotalBatches * config.Epochs,
                    SamplesProcessed = (batchCount + 1) * config.BatchSize,
                    TotalSamples = trainLoader.TotalBatches * config.BatchSize,
                    PercentComplete = (double)globalStep /
                        (trainLoader.TotalBatches * config.Epochs) * 100,
                    CurrentMetrics = metrics,
                    HardwareMetrics = hwMetrics,
                    CurrentOperation = $"Epoch {epoch + 1}/{config.Epochs}, " +
                        $"Step {batchCount + 1}/{trainLoader.TotalBatches}"
                });

                ProgressChanged?.Invoke(this, new TrainingProgressEventArgs
                {
                    Progress = new TrainingProgress
                    {
                        CurrentStep = globalStep,
                        CurrentMetrics = metrics
                    }
                });
            }

            batchCount++;
        }

        return new TrainingMetrics
        {
            Step = globalStep,
            Epoch = epoch,
            Loss = totalLoss / batchCount,
            LearningRate = (float)scheduler.get_last_lr()[0]
        };
    }

    private async Task<float> EvaluateAsync(
        LoraModel model,
        IDataLoader validLoader,
        torch.Device device,
        CancellationToken ct)
    {
        model.eval();
        var totalLoss = 0.0f;
        var batchCount = 0;

        using (torch.no_grad())
        {
            foreach (var batch in validLoader)
            {
                ct.ThrowIfCancellationRequested();

                var inputIds = batch.InputIds.to(device);
                var attentionMask = batch.AttentionMask.to(device);
                var labels = batch.Labels.to(device);

                var outputs = model.forward(inputIds, attentionMask);
                var loss = ComputeLoss(outputs, labels);

                totalLoss += loss.item<float>();
                batchCount++;
            }
        }

        return totalLoss / batchCount;
    }

    private torch.Tensor ComputeLoss(torch.Tensor logits, torch.Tensor labels)
    {
        // Shift for causal LM
        var shiftLogits = logits[.., ..^1, ..].contiguous();
        var shiftLabels = labels[.., 1..].contiguous();

        // Cross entropy loss, ignoring -100 labels
        return torch.nn.functional.cross_entropy(
            shiftLogits.view(-1, shiftLogits.shape[^1]),
            shiftLabels.view(-1),
            ignore_index: -100);
    }

    private LoraModel ApplyLoraAdapters(
        torch.nn.Module baseModel,
        TrainingConfiguration config)
    {
        var loraLayers = new Dictionary<string, LoraLayer>();

        // Find and wrap target linear layers
        foreach (var (name, module) in baseModel.named_modules())
        {
            if (module is torch.nn.Linear linear &&
                config.TargetModules.Any(t => name.Contains(t)))
            {
                var loraLayer = new LoraLayer(
                    linear,
                    config.LoraRank,
                    config.LoraAlpha,
                    config.LoraDropout);

                loraLayers[name] = loraLayer;
            }
        }

        return new LoraModel(baseModel, loraLayers);
    }

    private torch.optim.Optimizer CreateOptimizer(
        LoraModel model,
        TrainingConfiguration config)
    {
        // Only optimize LoRA parameters
        var loraParams = model.GetLoraParameters();

        return torch.optim.AdamW(
            loraParams,
            lr: config.LearningRate,
            weight_decay: config.WeightDecay,
            betas: (0.9, 0.999));
    }

    private torch.optim.lr_scheduler.LRScheduler CreateScheduler(
        torch.optim.Optimizer optimizer,
        int stepsPerEpoch,
        TrainingConfiguration config)
    {
        var totalSteps = stepsPerEpoch * config.Epochs;

        return config.LrScheduler switch
        {
            "cosine" => torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer, T_max: totalSteps),
            "linear" => torch.optim.lr_scheduler.LinearLR(
                optimizer,
                start_factor: 1.0,
                end_factor: 0.0,
                total_iters: totalSteps),
            _ => torch.optim.lr_scheduler.ConstantLR(optimizer, factor: 1.0)
        };
    }

    private void SetState(TrainingState state)
    {
        _state = state;
        StateChanged?.Invoke(this, new TrainingStateChangedEventArgs { State = state });
    }
}
```

### LoRA Adapter Model

```csharp
namespace SeniorIntern.Core.Training;

/// <summary>
/// Saved LoRA adapter metadata and weights
/// </summary>
public sealed class LoraAdapter
{
    public string Id { get; init; } = Guid.NewGuid().ToString();
    public string Name { get; init; } = string.Empty;
    public string Description { get; init; } = string.Empty;
    public string BaseModelHash { get; init; } = string.Empty;
    public int Rank { get; init; }
    public float Alpha { get; init; }
    public IReadOnlyList<string> TargetModules { get; init; } = Array.Empty<string>();
    public DateTime CreatedAt { get; init; }
    public TrainingMetrics FinalMetrics { get; init; } = new();
    public Dictionary<string, string> Tags { get; init; } = new();
    public string WeightsPath { get; init; } = string.Empty;
}

public sealed class LoraAdapterInfo
{
    public string Id { get; init; } = string.Empty;
    public string Name { get; init; } = string.Empty;
    public string Description { get; init; } = string.Empty;
    public DateTime CreatedAt { get; init; }
    public long SizeBytes { get; init; }
    public int Rank { get; init; }
    public float FinalLoss { get; init; }
    public string Path { get; init; } = string.Empty;
}
```

### v0.8.3 Files to Create

| File | Purpose |
|------|---------|
| `Core/Training/LoraAdapter.cs` | Adapter models |
| `Core/Training/TrainingJob.cs` | Job and result models |
| `Core/Training/TrainingProgress.cs` | Progress models |
| `Core/Interfaces/ITrainingService.cs` | Training service interface |
| `Services/Training/LoraLayer.cs` | LoRA layer implementation |
| `Services/Training/LoraModel.cs` | LoRA-wrapped model |
| `Services/Training/LoraTrainingEngine.cs` | Training loop implementation |
| `Services/Training/CheckpointManager.cs` | Checkpoint save/load |

---

## v0.8.4: Hardware Monitoring

### Objective
Implement real-time hardware monitoring with GPU/CPU metrics, memory tracking, temperature monitoring, and streaming to the UI for live graphs.

### IHardwareMonitorService Interface

```csharp
namespace SeniorIntern.Core.Interfaces;

public interface IHardwareMonitorService : IDisposable
{
    /// <summary>
    /// Start monitoring hardware
    /// </summary>
    void StartMonitoring(TimeSpan interval);

    /// <summary>
    /// Stop monitoring
    /// </summary>
    void StopMonitoring();

    /// <summary>
    /// Get current metrics snapshot
    /// </summary>
    HardwareMetrics GetCurrentMetrics();

    /// <summary>
    /// Get metrics history
    /// </summary>
    IReadOnlyList<TimestampedMetrics> GetHistory(TimeSpan duration);

    /// <summary>
    /// Clear metrics history
    /// </summary>
    void ClearHistory();

    /// <summary>
    /// Event when new metrics are available
    /// </summary>
    event EventHandler<HardwareMetricsEventArgs>? MetricsUpdated;
}

public sealed class TimestampedMetrics
{
    public DateTime Timestamp { get; init; }
    public HardwareMetrics Metrics { get; init; } = null!;
}

public sealed class HardwareMetricsEventArgs : EventArgs
{
    public HardwareMetrics Metrics { get; init; } = null!;
}
```

### Hardware Monitor Implementation

```csharp
namespace SeniorIntern.Services.Training;

public sealed class HardwareMonitorService : IHardwareMonitorService
{
    private readonly ILogger<HardwareMonitorService> _logger;
    private readonly ConcurrentQueue<TimestampedMetrics> _history = new();
    private readonly int _maxHistorySize = 3600; // 1 hour at 1 second intervals

    private Timer? _timer;
    private bool _isMonitoring;

    public event EventHandler<HardwareMetricsEventArgs>? MetricsUpdated;

    public void StartMonitoring(TimeSpan interval)
    {
        if (_isMonitoring) return;

        _timer = new Timer(
            _ => CollectMetrics(),
            null,
            TimeSpan.Zero,
            interval);

        _isMonitoring = true;
        _logger.LogInformation("Hardware monitoring started with interval: {Interval}", interval);
    }

    public void StopMonitoring()
    {
        _timer?.Dispose();
        _timer = null;
        _isMonitoring = false;
        _logger.LogInformation("Hardware monitoring stopped");
    }

    public HardwareMetrics GetCurrentMetrics()
    {
        return CollectMetricsInternal();
    }

    public IReadOnlyList<TimestampedMetrics> GetHistory(TimeSpan duration)
    {
        var cutoff = DateTime.UtcNow - duration;
        return _history.Where(m => m.Timestamp >= cutoff).ToList();
    }

    public void ClearHistory()
    {
        while (_history.TryDequeue(out _)) { }
    }

    private void CollectMetrics()
    {
        try
        {
            var metrics = CollectMetricsInternal();
            var timestamped = new TimestampedMetrics
            {
                Timestamp = DateTime.UtcNow,
                Metrics = metrics
            };

            _history.Enqueue(timestamped);

            // Trim history
            while (_history.Count > _maxHistorySize)
                _history.TryDequeue(out _);

            MetricsUpdated?.Invoke(this, new HardwareMetricsEventArgs
            {
                Metrics = metrics
            });
        }
        catch (Exception ex)
        {
            _logger.LogWarning(ex, "Failed to collect hardware metrics");
        }
    }

    private HardwareMetrics CollectMetricsInternal()
    {
        var metrics = new HardwareMetrics();

        // GPU metrics
        if (torch.cuda.is_available())
        {
            metrics = metrics with
            {
                GpuUtilization = GetGpuUtilization(),
                GpuMemoryUsed = GetGpuMemoryUsed(),
                GpuMemoryTotal = GetGpuMemoryTotal(),
                GpuTemperature = GetGpuTemperature()
            };
        }
        else if (OperatingSystem.IsMacOS() && torch.mps.is_available())
        {
            metrics = metrics with
            {
                GpuMemoryUsed = GetMetalMemoryUsed(),
                GpuMemoryTotal = GetMetalMemoryTotal()
            };
        }

        // CPU metrics
        metrics = metrics with
        {
            CpuUtilization = GetCpuUtilization(),
            RamUsed = Process.GetCurrentProcess().WorkingSet64,
            RamTotal = GC.GetGCMemoryInfo().TotalAvailableMemoryBytes
        };

        return metrics;
    }

    private float GetGpuUtilization()
    {
        if (OperatingSystem.IsWindows() || OperatingSystem.IsLinux())
        {
            return QueryNvidiaSmi("utilization.gpu");
        }
        return 0;
    }

    private long GetGpuMemoryUsed()
    {
        if (torch.cuda.is_available())
        {
            return (long)torch.cuda.memory_allocated();
        }
        return 0;
    }

    private long GetGpuMemoryTotal()
    {
        if (torch.cuda.is_available())
        {
            var (_, total) = torch.cuda.mem_get_info();
            return (long)total;
        }
        return 0;
    }

    private float GetGpuTemperature()
    {
        if (OperatingSystem.IsWindows() || OperatingSystem.IsLinux())
        {
            return QueryNvidiaSmi("temperature.gpu");
        }
        return 0;
    }

    private float GetCpuUtilization()
    {
        // Platform-specific CPU usage
        if (OperatingSystem.IsWindows())
        {
            return GetWindowsCpuUsage();
        }
        else if (OperatingSystem.IsLinux() || OperatingSystem.IsMacOS())
        {
            return GetUnixCpuUsage();
        }
        return 0;
    }

    private static float QueryNvidiaSmi(string query)
    {
        try
        {
            var startInfo = new ProcessStartInfo
            {
                FileName = "nvidia-smi",
                Arguments = $"--query-gpu={query} --format=csv,noheader,nounits",
                RedirectStandardOutput = true,
                UseShellExecute = false,
                CreateNoWindow = true
            };

            using var process = Process.Start(startInfo);
            if (process == null) return 0;

            var output = process.StandardOutput.ReadToEnd().Trim();
            process.WaitForExit();

            if (float.TryParse(output, out var value))
                return value;
        }
        catch { }

        return 0;
    }

    public void Dispose()
    {
        StopMonitoring();
    }
}
```

### v0.8.4 Files to Create

| File | Purpose |
|------|---------|
| `Core/Interfaces/IHardwareMonitorService.cs` | Monitor service interface |
| `Services/Training/HardwareMonitorService.cs` | Monitor implementation |
| `Services/Training/GpuMetricsProvider.cs` | GPU-specific metrics (CUDA/Metal) |
| `Services/Training/CpuMetricsProvider.cs` | CPU metrics (cross-platform) |

---

## v0.8.5: Training UI & Polish

### Objective
Create the training wizard UI, real-time progress monitoring with graphs, adapter management interface, and polish the user experience.

### Training Wizard Layout

```
┌──────────────────────────────────────────────────────────────────────────────┐
│  Training Lab - Create New Adapter                                      [X]  │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Step 2 of 4: Configure Training                                            │
│  ═══════════════════════════════════════════════════════════════════════════│
│                                                                              │
│  ┌─ Base Model ──────────────────────────────────────────────────────────┐  │
│  │ Model: codellama-7b-instruct.Q4_K_M.gguf                              │  │
│  │ Size: 4.1 GB  |  Parameters: 7B                                       │  │
│  └───────────────────────────────────────────────────────────────────────┘  │
│                                                                              │
│  ┌─ Dataset ─────────────────────────────────────────────────────────────┐  │
│  │ Source: ~/Projects/MyApp                                              │  │
│  │ Examples: 1,234  |  Tokens: ~2.5M  |  Languages: C#, TypeScript       │  │
│  └───────────────────────────────────────────────────────────────────────┘  │
│                                                                              │
│  ┌─ LoRA Configuration ──────────────────────────────────────────────────┐  │
│  │ Rank (r):        [16 ▼]     Alpha:    [32 ▼]     Dropout: [0.05▼]    │  │
│  │                                                                        │  │
│  │ Target Modules:                                                        │  │
│  │ ☑ q_proj  ☑ k_proj  ☑ v_proj  ☑ o_proj                              │  │
│  │ ☑ gate_proj  ☑ up_proj  ☑ down_proj                                  │  │
│  └───────────────────────────────────────────────────────────────────────┘  │
│                                                                              │
│  ┌─ Training Parameters ─────────────────────────────────────────────────┐  │
│  │ Epochs:          [3  ▼]     Learning Rate:  [2e-4▼]                   │  │
│  │ Batch Size:      [4  ▼]     Warmup Steps:   [100 ▼]                   │  │
│  │ Grad Accum:      [4  ▼]     Max Seq Length: [2048▼]                   │  │
│  │                                                                        │  │
│  │ ☑ Mixed Precision (FP16)  ☐ Gradient Checkpointing                   │  │
│  └───────────────────────────────────────────────────────────────────────┘  │
│                                                                              │
│  ┌─ Hardware ────────────────────────────────────────────────────────────┐  │
│  │ Device: NVIDIA RTX 4090 (24GB)                                        │  │
│  │ Estimated VRAM: 12.4 GB  |  Estimated Time: ~2 hours                  │  │
│  │ ⚠ Consider gradient checkpointing for lower memory usage             │  │
│  └───────────────────────────────────────────────────────────────────────┘  │
│                                                                              │
│  ─────────────────────────────────────────────────────────────────────────  │
│  [< Back]                                          [Next >] [Start Training] │
└──────────────────────────────────────────────────────────────────────────────┘
```

### Training Progress View

```
┌──────────────────────────────────────────────────────────────────────────────┐
│  Training Lab - Training in Progress                                    [X]  │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  Adapter: my-coding-style  |  Epoch 2/3  |  Step 1,234/3,000                │
│                                                                              │
│  ████████████████████████████░░░░░░░░░░  68%                                │
│                                                                              │
│  Elapsed: 1h 24m  |  Remaining: ~42m  |  Speed: 14.2 samples/sec            │
│                                                                              │
│  ┌─ Training Metrics ────────────────────────────────────────────────────┐  │
│  │          Loss                              Learning Rate              │  │
│  │     2.5┤                                0.0002┤──────╮                │  │
│  │        │╲                                     │      ╲               │  │
│  │     2.0┤ ╲                                    │       ╲              │  │
│  │        │  ╲___                                │        ╲____         │  │
│  │     1.5┤      ╲___                            │             ╲___     │  │
│  │        │          ╲__                         │                 ╲    │  │
│  │     1.0┤             ╲_                  0.0000┤                  ╲   │  │
│  │        ├──────────────────                    ├───────────────────── │  │
│  │        0    500   1000  1500                  0    500   1000  1500  │  │
│  └───────────────────────────────────────────────────────────────────────┘  │
│                                                                              │
│  ┌─ Hardware Utilization ────────────────────────────────────────────────┐  │
│  │  GPU Memory                    GPU Utilization        Temperature     │  │
│  │  ████████████░░  12.4/24 GB    ████████████████ 98%   72°C           │  │
│  │                                                                        │  │
│  │  CPU Usage                     System RAM                             │  │
│  │  ████████░░░░░░  52%           ████████████░░░  24/32 GB              │  │
│  └───────────────────────────────────────────────────────────────────────┘  │
│                                                                              │
│  ┌─ Log ─────────────────────────────────────────────────────────────────┐  │
│  │ [14:32:01] Epoch 2 started                                            │  │
│  │ [14:32:45] Step 1200: loss=1.234, lr=0.00015                          │  │
│  │ [14:33:30] Step 1234: loss=1.198, lr=0.00014                          │  │
│  │ [14:33:30] Checkpoint saved: epoch-2-step-1234                        │  │
│  └───────────────────────────────────────────────────────────────────────┘  │
│                                                                              │
│  [Pause]  [Stop]                                           [Run in Background]│
└──────────────────────────────────────────────────────────────────────────────┘
```

### Adapter Manager View

```
┌──────────────────────────────────────────────────────────────────────────────┐
│  Training Lab - Adapter Manager                                         [X]  │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  [+ New Adapter]  [Import]                                    [Search: ____] │
│                                                                              │
│  ┌────────────────────────────────────────────────────────────────────────┐ │
│  │ ● my-coding-style                                              Active │ │
│  │   Base: codellama-7b  |  Rank: 16  |  Loss: 1.12                      │ │
│  │   Created: 2 days ago  |  Size: 24 MB                                 │ │
│  │   [Load] [Merge] [Export] [Delete]                                    │ │
│  ├────────────────────────────────────────────────────────────────────────┤ │
│  │ ○ react-typescript-expert                                             │ │
│  │   Base: codellama-7b  |  Rank: 32  |  Loss: 0.98                      │ │
│  │   Created: 1 week ago  |  Size: 48 MB                                 │ │
│  │   [Load] [Merge] [Export] [Delete]                                    │ │
│  ├────────────────────────────────────────────────────────────────────────┤ │
│  │ ○ dotnet-best-practices                                               │ │
│  │   Base: deepseek-coder-6.7b  |  Rank: 16  |  Loss: 1.34              │ │
│  │   Created: 2 weeks ago  |  Size: 24 MB                                │ │
│  │   [Load] [Merge] [Export] [Delete]                                    │ │
│  └────────────────────────────────────────────────────────────────────────┘ │
│                                                                              │
│  ─────────────────────────────────────────────────────────────────────────  │
│                                                                   [Close]    │
└──────────────────────────────────────────────────────────────────────────────┘
```

### TrainingLabViewModel

```csharp
namespace SeniorIntern.Desktop.ViewModels;

public partial class TrainingLabViewModel : ViewModelBase
{
    private readonly ITrainingService _trainingService;
    private readonly IDatasetService _datasetService;
    private readonly IHardwareDetectionService _hardwareService;
    private readonly IHardwareMonitorService _monitorService;

    // Wizard state
    [ObservableProperty]
    private int _currentStep = 1;

    [ObservableProperty]
    private bool _canGoBack;

    [ObservableProperty]
    private bool _canGoNext = true;

    // Step 1: Dataset
    [ObservableProperty]
    private string _datasetPath = string.Empty;

    [ObservableProperty]
    private DatasetStatistics? _datasetStats;

    [ObservableProperty]
    private bool _isAnalyzingDataset;

    // Step 2: Configuration
    [ObservableProperty]
    private string _adapterName = string.Empty;

    [ObservableProperty]
    private int _loraRank = 16;

    [ObservableProperty]
    private float _loraAlpha = 32f;

    [ObservableProperty]
    private float _loraDropout = 0.05f;

    [ObservableProperty]
    private int _epochs = 3;

    [ObservableProperty]
    private float _learningRate = 2e-4f;

    [ObservableProperty]
    private int _batchSize = 4;

    [ObservableProperty]
    private bool _useMixedPrecision = true;

    [ObservableProperty]
    private bool _useGradientCheckpointing;

    // Step 3: Hardware
    [ObservableProperty]
    private TrainingHardwareInfo? _hardwareInfo;

    [ObservableProperty]
    private string _selectedDevice = "auto";

    [ObservableProperty]
    private long _estimatedVramUsage;

    [ObservableProperty]
    private TimeSpan _estimatedTrainingTime;

    // Training state
    [ObservableProperty]
    private bool _isTraining;

    [ObservableProperty]
    private TrainingProgress? _trainingProgress;

    [ObservableProperty]
    private ObservableCollection<TrainingMetrics> _metricsHistory = new();

    [ObservableProperty]
    private ObservableCollection<TimestampedMetrics> _hardwareHistory = new();

    // Adapters
    [ObservableProperty]
    private ObservableCollection<LoraAdapterInfo> _adapters = new();

    [ObservableProperty]
    private LoraAdapterInfo? _selectedAdapter;

    [ObservableProperty]
    private LoraAdapterInfo? _activeAdapter;

    public TrainingLabViewModel(
        ITrainingService trainingService,
        IDatasetService datasetService,
        IHardwareDetectionService hardwareService,
        IHardwareMonitorService monitorService)
    {
        _trainingService = trainingService;
        _datasetService = datasetService;
        _hardwareService = hardwareService;
        _monitorService = monitorService;

        _trainingService.ProgressChanged += OnTrainingProgress;
        _trainingService.StateChanged += OnTrainingStateChanged;
        _monitorService.MetricsUpdated += OnHardwareMetrics;
    }

    [RelayCommand]
    private async Task BrowseDatasetAsync()
    {
        var dialog = new OpenFolderDialog
        {
            Title = "Select Code Folder for Training"
        };

        var result = await dialog.ShowAsync(GetWindow());
        if (!string.IsNullOrEmpty(result))
        {
            DatasetPath = result;
            await AnalyzeDatasetAsync();
        }
    }

    [RelayCommand]
    private async Task AnalyzeDatasetAsync()
    {
        if (string.IsNullOrEmpty(DatasetPath)) return;

        IsAnalyzingDataset = true;
        try
        {
            var dataset = await _datasetService.CreateDatasetFromCodeAsync(
                DatasetPath,
                new DatasetCreationOptions
                {
                    GenerationType = ExampleGenerationType.All
                });

            DatasetStats = _datasetService.GetStatistics(dataset);
        }
        catch (Exception ex)
        {
            await ShowErrorAsync("Dataset Analysis Failed", ex.Message);
        }
        finally
        {
            IsAnalyzingDataset = false;
        }
    }

    [RelayCommand]
    private async Task DetectHardwareAsync()
    {
        HardwareInfo = await _hardwareService.DetectHardwareAsync();
        UpdateEstimates();
    }

    [RelayCommand]
    private async Task StartTrainingAsync()
    {
        if (string.IsNullOrEmpty(AdapterName))
        {
            await ShowErrorAsync("Missing Name", "Please enter a name for the adapter.");
            return;
        }

        var job = new TrainingJob
        {
            Name = AdapterName,
            BaseModelPath = GetCurrentModelPath(),
            DatasetPath = DatasetPath,
            OutputDir = GetAdaptersDirectory(),
            Configuration = new TrainingConfiguration
            {
                Device = SelectedDevice == "auto" ? HardwareInfo?.RecommendedDevice ?? "cpu" : SelectedDevice,
                LoraRank = LoraRank,
                LoraAlpha = LoraAlpha,
                LoraDropout = LoraDropout,
                Epochs = Epochs,
                LearningRate = LearningRate,
                BatchSize = BatchSize,
                UseMixedPrecision = UseMixedPrecision,
                UseGradientCheckpointing = UseGradientCheckpointing
            }
        };

        IsTraining = true;
        MetricsHistory.Clear();

        _monitorService.StartMonitoring(TimeSpan.FromSeconds(1));

        var progress = new Progress<TrainingProgress>(p =>
        {
            Dispatcher.UIThread.InvokeAsync(() =>
            {
                TrainingProgress = p;
                if (p.CurrentMetrics != null)
                    MetricsHistory.Add(p.CurrentMetrics);
            });
        });

        try
        {
            var result = await _trainingService.TrainAsync(job, progress);

            if (result.Success)
            {
                await ShowSuccessAsync(
                    "Training Complete",
                    $"Adapter '{AdapterName}' trained successfully.\n" +
                    $"Final loss: {result.FinalMetrics.Loss:F4}");
                await LoadAdaptersAsync();
            }
            else
            {
                await ShowErrorAsync("Training Failed", result.ErrorMessage ?? "Unknown error");
            }
        }
        finally
        {
            IsTraining = false;
            _monitorService.StopMonitoring();
        }
    }

    [RelayCommand]
    private void PauseTraining()
    {
        _trainingService.PauseTrainingAsync();
    }

    [RelayCommand]
    private void StopTraining()
    {
        _trainingService.CancelTraining();
    }

    [RelayCommand]
    private async Task LoadAdaptersAsync()
    {
        var adapters = await _trainingService.ListAdaptersAsync();
        Adapters = new ObservableCollection<LoraAdapterInfo>(adapters);
    }

    [RelayCommand]
    private async Task LoadAdapterAsync(LoraAdapterInfo adapter)
    {
        try
        {
            var loaded = await _trainingService.LoadAdapterAsync(adapter.Path);
            ActiveAdapter = adapter;
            await ShowSuccessAsync("Adapter Loaded", $"'{adapter.Name}' is now active.");
        }
        catch (Exception ex)
        {
            await ShowErrorAsync("Load Failed", ex.Message);
        }
    }

    [RelayCommand]
    private async Task DeleteAdapterAsync(LoraAdapterInfo adapter)
    {
        var confirmed = await ShowConfirmAsync(
            "Delete Adapter",
            $"Are you sure you want to delete '{adapter.Name}'?");

        if (confirmed)
        {
            await _trainingService.DeleteAdapterAsync(adapter.Id);
            await LoadAdaptersAsync();
        }
    }

    private void UpdateEstimates()
    {
        if (HardwareInfo == null || DatasetStats == null) return;

        // Estimate VRAM: ~2 bytes per param for FP16, LoRA adds ~2-5%
        var baseVram = 7_000_000_000L * 2; // 7B params * 2 bytes
        var loraOverhead = baseVram * 0.05 * (LoraRank / 16.0);
        var batchOverhead = BatchSize * 2048 * 4096 * 2L; // Activations

        EstimatedVramUsage = (long)(baseVram + loraOverhead + batchOverhead);

        // Estimate time: ~1 second per step on good GPU
        var stepsPerEpoch = DatasetStats.TrainExamples / BatchSize;
        var totalSteps = stepsPerEpoch * Epochs;
        EstimatedTrainingTime = TimeSpan.FromSeconds(totalSteps * 1.5);
    }

    private void OnTrainingProgress(object? sender, TrainingProgressEventArgs e)
    {
        Dispatcher.UIThread.InvokeAsync(() =>
        {
            TrainingProgress = e.Progress;
        });
    }

    private void OnHardwareMetrics(object? sender, HardwareMetricsEventArgs e)
    {
        Dispatcher.UIThread.InvokeAsync(() =>
        {
            HardwareHistory.Add(new TimestampedMetrics
            {
                Timestamp = DateTime.UtcNow,
                Metrics = e.Metrics
            });

            // Keep last 5 minutes
            while (HardwareHistory.Count > 300)
                HardwareHistory.RemoveAt(0);
        });
    }
}
```

### Training Settings Model

```csharp
namespace SeniorIntern.Core.Models;

public sealed class TrainingSettings
{
    /// <summary>
    /// Directory for storing adapters
    /// </summary>
    public string AdaptersDirectory { get; set; } = Path.Combine(
        Environment.GetFolderPath(Environment.SpecialFolder.ApplicationData),
        "SeniorIntern", "adapters");

    /// <summary>
    /// Default LoRA rank
    /// </summary>
    public int DefaultLoraRank { get; set; } = 16;

    /// <summary>
    /// Default LoRA alpha
    /// </summary>
    public float DefaultLoraAlpha { get; set; } = 32f;

    /// <summary>
    /// Default learning rate
    /// </summary>
    public float DefaultLearningRate { get; set; } = 2e-4f;

    /// <summary>
    /// Default epochs
    /// </summary>
    public int DefaultEpochs { get; set; } = 3;

    /// <summary>
    /// Auto-load last used adapter on startup
    /// </summary>
    public bool AutoLoadLastAdapter { get; set; } = true;

    /// <summary>
    /// Last used adapter ID
    /// </summary>
    public string? LastAdapterId { get; set; }

    /// <summary>
    /// Maximum checkpoints to keep per training run
    /// </summary>
    public int MaxCheckpoints { get; set; } = 3;

    /// <summary>
    /// Enable TensorBoard logging
    /// </summary>
    public bool EnableTensorBoard { get; set; } = false;
}
```

### v0.8.5 Files to Create

| File | Purpose |
|------|---------|
| `Desktop/Views/TrainingLabView.axaml` | Main training lab view |
| `Desktop/Views/TrainingLabView.axaml.cs` | Code-behind |
| `Desktop/ViewModels/TrainingLabViewModel.cs` | Training lab logic |
| `Desktop/Views/TrainingWizardDialog.axaml` | Training wizard |
| `Desktop/Views/TrainingProgressView.axaml` | Progress monitoring |
| `Desktop/Views/AdapterManagerView.axaml` | Adapter management |
| `Desktop/Controls/MetricsChart.axaml` | Real-time chart control |
| `Desktop/Controls/HardwareGauge.axaml` | Hardware utilization gauge |
| `Core/Models/TrainingSettings.cs` | Training settings model |

---

## NuGet Packages Summary

| Package | Version | Purpose |
|---------|---------|---------|
| `TorchSharp` | 0.103.x | PyTorch bindings for .NET |
| `TorchSharp-cuda-windows` | 0.103.x | CUDA backend (Windows) |
| `TorchSharp-cuda-linux` | 0.103.x | CUDA backend (Linux) |
| `libtorch-cpu` | 2.3.x | CPU backend |
| `LiveChartsCore.SkiaSharpView.Avalonia` | 2.0.x | Real-time charts |
| `Hardware.Info` | 100.x | Hardware detection |

---

## Testing Strategy

### Unit Tests

```csharp
public class LoraLayerTests
{
    [Fact]
    public void Forward_WithoutMerge_AddsLoraOutput()
    {
        var linear = torch.nn.Linear(64, 128);
        var lora = new LoraLayer(linear, rank: 8, alpha: 16);

        var input = torch.randn(2, 64);
        var output = lora.forward(input);

        Assert.Equal(new long[] { 2, 128 }, output.shape);
    }

    [Fact]
    public void Merge_ModifiesOriginalWeights()
    {
        var linear = torch.nn.Linear(64, 128);
        var originalWeight = linear.weight!.clone();
        var lora = new LoraLayer(linear, rank: 8, alpha: 16);

        lora.Merge();

        Assert.False(torch.equal(linear.weight!, originalWeight));
        Assert.True(lora.Merged);
    }
}

public class DatasetServiceTests
{
    [Fact]
    public async Task CreateDatasetFromCode_GeneratesExamples()
    {
        var service = new DatasetService(...);
        var dataset = await service.CreateDatasetFromCodeAsync(
            "TestData/SampleProject",
            new DatasetCreationOptions
            {
                GenerationType = ExampleGenerationType.CodeCompletion
            });

        Assert.NotEmpty(dataset.Examples);
        Assert.All(dataset.Examples, e =>
        {
            Assert.NotEmpty(e.Instruction);
            Assert.NotEmpty(e.Output);
        });
    }
}
```

### Integration Tests

```csharp
public class TrainingIntegrationTests
{
    [Fact]
    [Trait("Category", "GPU")]
    public async Task TrainLoRA_SmallModel_Converges()
    {
        var service = new LoraTrainingEngine(...);
        var job = new TrainingJob
        {
            BaseModelPath = "TestModels/tiny-llama.gguf",
            DatasetPath = "TestData/small-dataset",
            Configuration = new TrainingConfiguration
            {
                Epochs = 1,
                BatchSize = 2,
                LoraRank = 4
            }
        };

        var result = await service.TrainAsync(job);

        Assert.True(result.Success);
        Assert.True(result.FinalMetrics.Loss < 10);
        Assert.True(File.Exists(result.AdapterPath));
    }
}
```

---

## Files to Create Summary (32 total)

**Core Project (12):**
- `Core/Training/TrainingHardwareInfo.cs`
- `Core/Training/TrainingConfiguration.cs`
- `Core/Training/TrainingExample.cs`
- `Core/Training/DatasetModels.cs`
- `Core/Training/DatasetCreationOptions.cs`
- `Core/Training/LoraAdapter.cs`
- `Core/Training/TrainingJob.cs`
- `Core/Training/TrainingProgress.cs`
- `Core/Models/TrainingSettings.cs`
- `Core/Interfaces/IHardwareDetectionService.cs`
- `Core/Interfaces/IDatasetService.cs`
- `Core/Interfaces/ITrainingService.cs`
- `Core/Interfaces/IHardwareMonitorService.cs`

**Services Project (12):**
- `Services/Training/HardwareDetectionService.cs`
- `Services/Training/TorchSharpInitializer.cs`
- `Services/Training/DatasetService.cs`
- `Services/Training/InstructionTemplateGenerator.cs`
- `Services/Training/TokenizationService.cs`
- `Services/Training/CodeSymbolExtractor.cs`
- `Services/Training/LoraLayer.cs`
- `Services/Training/LoraModel.cs`
- `Services/Training/LoraTrainingEngine.cs`
- `Services/Training/CheckpointManager.cs`
- `Services/Training/HardwareMonitorService.cs`
- `Services/Training/GpuMetricsProvider.cs`

**Desktop Project (8):**
- `Desktop/Views/TrainingLabView.axaml` + `.cs`
- `Desktop/ViewModels/TrainingLabViewModel.cs`
- `Desktop/Views/TrainingWizardDialog.axaml` + `.cs`
- `Desktop/Views/TrainingProgressView.axaml` + `.cs`
- `Desktop/Views/AdapterManagerView.axaml` + `.cs`
- `Desktop/Controls/MetricsChart.axaml` + `.cs`
- `Desktop/Controls/HardwareGauge.axaml` + `.cs`

---

## Acceptance Criteria

### v0.8.1
- [ ] TorchSharp loads and initializes correctly
- [ ] CUDA devices are detected on Windows/Linux
- [ ] Metal is detected on macOS
- [ ] Recommended configuration is generated based on hardware

### v0.8.2
- [ ] Code folder is scanned and analyzed
- [ ] Training examples are generated from code
- [ ] Tokenization produces correct input/label pairs
- [ ] Dataset statistics are accurate

### v0.8.3
- [ ] LoRA layers wrap linear modules correctly
- [ ] Training loop runs without errors
- [ ] Loss decreases over training
- [ ] Checkpoints save and load correctly
- [ ] Adapter can be merged with base model

### v0.8.4
- [ ] GPU utilization is monitored in real-time
- [ ] Memory usage is tracked accurately
- [ ] Temperature is reported (NVIDIA GPUs)
- [ ] Metrics history is maintained

### v0.8.5
- [ ] Training wizard guides user through setup
- [ ] Progress charts update in real-time
- [ ] Training can be paused and resumed
- [ ] Adapters can be loaded and managed
- [ ] Settings are persisted

---

## Risk Assessment

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| TorchSharp CUDA compatibility | Medium | High | Test on multiple GPU generations, provide CPU fallback |
| Out of memory during training | High | Medium | Gradient checkpointing, smaller batch sizes, clear memory estimates |
| Training instability (NaN loss) | Medium | Medium | Gradient clipping, learning rate warmup, mixed precision safeguards |
| Slow training on consumer hardware | High | Low | Set expectations in UI, support background training, checkpoint frequently |
| Model/adapter compatibility | Medium | High | Store base model hash, validate before loading adapter |
| macOS Metal training support | Medium | Medium | TorchSharp Metal support is newer, extensive testing needed |
