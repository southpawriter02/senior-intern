# v0.8.1: Training Infrastructure - Detailed Design Specification

## Overview

This document provides a comprehensive design specification for v0.8.1 of The Senior Intern project. This version establishes the TorchSharp-based training infrastructure with proper CUDA/Metal detection, memory management, device abstraction, and integration with the existing LLamaSharp model loading system. This forms the foundation for the LoRA fine-tuning capabilities introduced in the v0.8.x "Training Lab" series.

### Objectives
- Integrate TorchSharp as the training backend for LoRA fine-tuning
- Implement cross-platform hardware detection (CUDA, Metal, CPU)
- Create GPU device abstraction for unified training across platforms
- Build memory management utilities for safe GPU memory allocation
- Establish training configuration models and validation
- Initialize TorchSharp runtime with proper native library loading
- Provide recommended training configuration based on detected hardware

### Prerequisites
- v0.7.5 (UI & Integration) completed
- LLamaSharp infrastructure from v0.4.x available
- .NET 8.0 runtime with native interop support
- Understanding of GPU memory management and compute capabilities

---

## Architecture Overview

### Training Infrastructure Stack

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        Training Orchestrator (v0.8.3)                       │
│   ┌─────────────────────────────────────────────────────────────────────┐   │
│   │  TrainAsync() → Dataset → LoRA Adapters → Training Loop → Save      │   │
│   └─────────────────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────────────────┘
                                      │
                                      ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                    v0.8.1: Training Infrastructure                          │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────────────────┐  │
│  │ TorchSharp      │  │ Hardware        │  │ Training Configuration      │  │
│  │ Initializer     │  │ Detection       │  │ & Validation                │  │
│  └────────┬────────┘  └────────┬────────┘  └──────────────┬──────────────┘  │
│           │                    │                          │                  │
│           ▼                    ▼                          ▼                  │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                    Device Abstraction Layer                          │    │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐                  │    │
│  │  │ CUDA Device │  │Metal Device │  │ CPU Device  │                  │    │
│  │  └─────────────┘  └─────────────┘  └─────────────┘                  │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                    │                                         │
│                                    ▼                                         │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                    Memory Management                                 │    │
│  │  ┌──────────────────┐  ┌──────────────────┐  ┌──────────────────┐  │    │
│  │  │ Memory Estimator │  │ Memory Guard     │  │ OOM Recovery     │  │    │
│  │  └──────────────────┘  └──────────────────┘  └──────────────────┘  │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
└─────────────────────────────────────────────────────────────────────────────┘
                                      │
                                      ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                        Native Libraries                                      │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐              │
│  │ libtorch (CPU)  │  │ libtorch (CUDA) │  │ libtorch (MPS)  │              │
│  └─────────────────┘  └─────────────────┘  └─────────────────┘              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Hardware Detection Flow

```
Application Start
       │
       ▼
┌──────────────────┐
│ TorchSharp       │
│ Initializer      │
└────────┬─────────┘
         │
         ▼
┌──────────────────┐     ┌───────────────────────────────────────────────┐
│ Load Native Libs │────▶│ Platform Detection                            │
└────────┬─────────┘     │ • Windows: libtorch-cuda or libtorch-cpu     │
         │               │ • Linux: libtorch-cuda or libtorch-cpu       │
         │               │ • macOS: libtorch-mps (Apple Silicon) or cpu │
         │               └───────────────────────────────────────────────┘
         ▼
┌──────────────────┐
│ Detect Hardware  │
│   Capabilities   │
└────────┬─────────┘
         │
         ├─────────────────┬─────────────────┐
         ▼                 ▼                 ▼
┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐
│   CUDA GPUs     │ │   Metal (MPS)   │ │   CPU Only      │
│ • Count devices │ │ • Apple Silicon │ │ • Core count    │
│ • VRAM per GPU  │ │ • Unified memory│ │ • RAM available │
│ • Compute cap.  │ │ • Performance   │ │ • Fallback mode │
│ • Temperature   │ │   tier          │ │                 │
└─────────────────┘ └─────────────────┘ └─────────────────┘
         │                 │                 │
         └─────────────────┴─────────────────┘
                           │
                           ▼
              ┌────────────────────────┐
              │ TrainingHardwareInfo   │
              │ • Best device selected │
              │ • Memory budgets set   │
              │ • Recommendations made │
              └────────────────────────┘
```

---

## Sub-version Breakdown

| Version | Focus | Files to Create | Files to Modify |
|---------|-------|-----------------|-----------------|
| v0.8.1a | TorchSharp Initialization & Setup | 4 | 1 |
| v0.8.1b | Hardware Detection Models | 4 | 0 |
| v0.8.1c | Hardware Detection Service Interface | 3 | 0 |
| v0.8.1d | CUDA Hardware Detection | 3 | 0 |
| v0.8.1e | Metal (Apple Silicon) Hardware Detection | 3 | 0 |
| v0.8.1f | CPU Fallback Detection | 2 | 0 |
| v0.8.1g | Training Configuration Models | 4 | 0 |
| v0.8.1h | Configuration Validation & Recommendations | 3 | 0 |
| v0.8.1i | Memory Management Utilities | 4 | 0 |
| v0.8.1j | Unit Testing & Integration | 8 | 0 |

**Totals: 38 files to create, 1 file to modify**

---

## v0.8.1a: TorchSharp Initialization & Setup

### Objective
Initialize TorchSharp with proper native library loading, configure the runtime for the detected platform, and establish the foundation for tensor operations.

### File: `src/SeniorIntern.Core/Training/TorchSharpBackend.cs`

```csharp
namespace SeniorIntern.Core.Training;

/// <summary>
/// Supported TorchSharp backend types for training.
/// </summary>
public enum TorchBackend
{
    /// <summary>
    /// CPU-only backend, works on all platforms.
    /// </summary>
    Cpu,

    /// <summary>
    /// NVIDIA CUDA backend for GPU acceleration.
    /// Requires CUDA-capable GPU and drivers.
    /// </summary>
    Cuda,

    /// <summary>
    /// Apple Metal Performance Shaders backend.
    /// Available on Apple Silicon Macs.
    /// </summary>
    Mps
}

/// <summary>
/// Represents the initialization state of TorchSharp.
/// </summary>
public sealed class TorchSharpState
{
    /// <summary>
    /// Whether TorchSharp has been successfully initialized.
    /// </summary>
    public bool IsInitialized { get; init; }

    /// <summary>
    /// The active backend being used.
    /// </summary>
    public TorchBackend ActiveBackend { get; init; }

    /// <summary>
    /// TorchSharp version string.
    /// </summary>
    public string Version { get; init; } = string.Empty;

    /// <summary>
    /// LibTorch version string.
    /// </summary>
    public string LibTorchVersion { get; init; } = string.Empty;

    /// <summary>
    /// Path to the loaded native libraries.
    /// </summary>
    public string NativeLibraryPath { get; init; } = string.Empty;

    /// <summary>
    /// Any warnings or notes from initialization.
    /// </summary>
    public IReadOnlyList<string> Warnings { get; init; } = Array.Empty<string>();

    /// <summary>
    /// Error message if initialization failed.
    /// </summary>
    public string? ErrorMessage { get; init; }
}
```

### File: `src/SeniorIntern.Core/Interfaces/ITorchSharpInitializer.cs`

```csharp
using SeniorIntern.Core.Training;

namespace SeniorIntern.Core.Interfaces;

/// <summary>
/// Service responsible for initializing TorchSharp and managing the native runtime.
/// Must be initialized before any training operations.
/// </summary>
public interface ITorchSharpInitializer
{
    /// <summary>
    /// Current initialization state.
    /// </summary>
    TorchSharpState State { get; }

    /// <summary>
    /// Whether initialization has completed successfully.
    /// </summary>
    bool IsInitialized { get; }

    /// <summary>
    /// The preferred backend based on available hardware.
    /// </summary>
    TorchBackend PreferredBackend { get; }

    /// <summary>
    /// Initialize TorchSharp with the specified options.
    /// This should be called once at application startup.
    /// </summary>
    /// <param name="options">Initialization options.</param>
    /// <param name="ct">Cancellation token.</param>
    /// <returns>Initialization result with state information.</returns>
    Task<TorchSharpState> InitializeAsync(
        TorchSharpInitOptions options,
        CancellationToken ct = default);

    /// <summary>
    /// Force re-initialization with different options.
    /// Useful for switching between CPU and GPU backends.
    /// </summary>
    Task<TorchSharpState> ReinitializeAsync(
        TorchSharpInitOptions options,
        CancellationToken ct = default);

    /// <summary>
    /// Shutdown TorchSharp and release all native resources.
    /// Should be called during application shutdown.
    /// </summary>
    Task ShutdownAsync();

    /// <summary>
    /// Verify that TorchSharp is working correctly.
    /// Performs a simple tensor operation to validate the backend.
    /// </summary>
    Task<bool> VerifyAsync(CancellationToken ct = default);
}

/// <summary>
/// Options for TorchSharp initialization.
/// </summary>
public sealed class TorchSharpInitOptions
{
    /// <summary>
    /// Preferred backend. If unavailable, will fall back to CPU.
    /// </summary>
    public TorchBackend PreferredBackend { get; init; } = TorchBackend.Cuda;

    /// <summary>
    /// Whether to allow automatic fallback to CPU if GPU is unavailable.
    /// </summary>
    public bool AllowCpuFallback { get; init; } = true;

    /// <summary>
    /// Custom path to native libraries. If null, uses default locations.
    /// </summary>
    public string? NativeLibraryPath { get; init; }

    /// <summary>
    /// CUDA device index to use (0-based). Only applies to CUDA backend.
    /// -1 means auto-select the best GPU.
    /// </summary>
    public int CudaDeviceIndex { get; init; } = -1;

    /// <summary>
    /// Whether to enable TF32 for faster FP32 operations on Ampere+ GPUs.
    /// </summary>
    public bool EnableTf32 { get; init; } = true;

    /// <summary>
    /// Whether to enable cuDNN benchmark mode for faster convolutions.
    /// </summary>
    public bool EnableCudnnBenchmark { get; init; } = true;

    /// <summary>
    /// Set random seed for reproducibility. Null for random seed.
    /// </summary>
    public int? RandomSeed { get; init; }

    /// <summary>
    /// Maximum number of threads for CPU operations.
    /// 0 means use all available cores.
    /// </summary>
    public int MaxCpuThreads { get; init; } = 0;
}
```

### File: `src/SeniorIntern.Services/Training/TorchSharpInitializer.cs`

```csharp
using System.Runtime.InteropServices;
using Microsoft.Extensions.Logging;
using SeniorIntern.Core.Interfaces;
using SeniorIntern.Core.Training;
using TorchSharp;

namespace SeniorIntern.Services.Training;

/// <summary>
/// Initializes and manages the TorchSharp runtime.
/// Handles native library loading and backend configuration.
/// </summary>
public sealed class TorchSharpInitializer : ITorchSharpInitializer
{
    private readonly ILogger<TorchSharpInitializer> _logger;
    private readonly object _initLock = new();
    private TorchSharpState _state = new();
    private bool _initialized;

    public TorchSharpInitializer(ILogger<TorchSharpInitializer> logger)
    {
        _logger = logger;
    }

    public TorchSharpState State => _state;
    public bool IsInitialized => _initialized;

    public TorchBackend PreferredBackend
    {
        get
        {
            // Determine preferred backend based on platform
            if (RuntimeInformation.IsOSPlatform(OSPlatform.OSX) &&
                RuntimeInformation.ProcessArchitecture == Architecture.Arm64)
            {
                return TorchBackend.Mps;
            }

            if (RuntimeInformation.IsOSPlatform(OSPlatform.Windows) ||
                RuntimeInformation.IsOSPlatform(OSPlatform.Linux))
            {
                return TorchBackend.Cuda;
            }

            return TorchBackend.Cpu;
        }
    }

    public async Task<TorchSharpState> InitializeAsync(
        TorchSharpInitOptions options,
        CancellationToken ct = default)
    {
        return await Task.Run(() =>
        {
            lock (_initLock)
            {
                if (_initialized)
                {
                    _logger.LogWarning("TorchSharp already initialized, returning existing state");
                    return _state;
                }

                return InitializeInternal(options);
            }
        }, ct);
    }

    public async Task<TorchSharpState> ReinitializeAsync(
        TorchSharpInitOptions options,
        CancellationToken ct = default)
    {
        await ShutdownAsync();
        return await InitializeAsync(options, ct);
    }

    private TorchSharpState InitializeInternal(TorchSharpInitOptions options)
    {
        var warnings = new List<string>();
        TorchBackend activeBackend;
        string? errorMessage = null;

        try
        {
            _logger.LogInformation(
                "Initializing TorchSharp with preferred backend: {Backend}",
                options.PreferredBackend);

            // Try to initialize preferred backend
            activeBackend = TryInitializeBackend(options, warnings);

            // Configure runtime options
            ConfigureRuntime(options, activeBackend);

            // Set random seed if specified
            if (options.RandomSeed.HasValue)
            {
                torch.manual_seed(options.RandomSeed.Value);
                if (activeBackend == TorchBackend.Cuda)
                {
                    torch.cuda.manual_seed(options.RandomSeed.Value);
                    torch.cuda.manual_seed_all(options.RandomSeed.Value);
                }
                _logger.LogInformation("Random seed set to: {Seed}", options.RandomSeed.Value);
            }

            _initialized = true;
            _logger.LogInformation(
                "TorchSharp initialized successfully with backend: {Backend}",
                activeBackend);
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Failed to initialize TorchSharp");
            errorMessage = ex.Message;
            activeBackend = TorchBackend.Cpu;
            _initialized = false;
        }

        _state = new TorchSharpState
        {
            IsInitialized = _initialized,
            ActiveBackend = activeBackend,
            Version = GetTorchSharpVersion(),
            LibTorchVersion = GetLibTorchVersion(),
            NativeLibraryPath = GetNativeLibraryPath(),
            Warnings = warnings,
            ErrorMessage = errorMessage
        };

        return _state;
    }

    private TorchBackend TryInitializeBackend(
        TorchSharpInitOptions options,
        List<string> warnings)
    {
        var preferredBackend = options.PreferredBackend;

        // Try CUDA
        if (preferredBackend == TorchBackend.Cuda)
        {
            try
            {
                // This will load CUDA native libraries
                if (torch.cuda.is_available())
                {
                    var deviceCount = torch.cuda.device_count();
                    _logger.LogInformation("CUDA available with {Count} device(s)", deviceCount);

                    // Select device
                    var deviceIndex = options.CudaDeviceIndex >= 0
                        ? options.CudaDeviceIndex
                        : SelectBestCudaDevice();

                    if (deviceIndex >= deviceCount)
                    {
                        warnings.Add($"Requested CUDA device {deviceIndex} not available, using device 0");
                        deviceIndex = 0;
                    }

                    torch.cuda.set_device(deviceIndex);
                    torch.InitializeDevice(DeviceType.CUDA, deviceIndex);

                    _logger.LogInformation("Using CUDA device: {Index}", deviceIndex);
                    return TorchBackend.Cuda;
                }
                else
                {
                    warnings.Add("CUDA requested but not available");
                }
            }
            catch (Exception ex)
            {
                _logger.LogWarning(ex, "Failed to initialize CUDA backend");
                warnings.Add($"CUDA initialization failed: {ex.Message}");
            }
        }

        // Try MPS (Apple Silicon)
        if (preferredBackend == TorchBackend.Mps ||
            (preferredBackend == TorchBackend.Cuda && options.AllowCpuFallback))
        {
            if (RuntimeInformation.IsOSPlatform(OSPlatform.OSX) &&
                RuntimeInformation.ProcessArchitecture == Architecture.Arm64)
            {
                try
                {
                    // Check for MPS availability
                    // Note: TorchSharp MPS support may be limited
                    if (IsMpsAvailable())
                    {
                        torch.InitializeDevice(DeviceType.MPS);
                        _logger.LogInformation("Using Metal Performance Shaders (MPS) backend");
                        return TorchBackend.Mps;
                    }
                    else
                    {
                        warnings.Add("MPS not available on this Apple Silicon device");
                    }
                }
                catch (Exception ex)
                {
                    _logger.LogWarning(ex, "Failed to initialize MPS backend");
                    warnings.Add($"MPS initialization failed: {ex.Message}");
                }
            }
        }

        // Fallback to CPU
        if (options.AllowCpuFallback || preferredBackend == TorchBackend.Cpu)
        {
            torch.InitializeDevice(DeviceType.CPU);

            if (preferredBackend != TorchBackend.Cpu)
            {
                warnings.Add($"Falling back to CPU (preferred: {preferredBackend})");
            }

            _logger.LogInformation("Using CPU backend");
            return TorchBackend.Cpu;
        }

        throw new InvalidOperationException(
            $"Failed to initialize {preferredBackend} backend and fallback is disabled");
    }

    private void ConfigureRuntime(TorchSharpInitOptions options, TorchBackend backend)
    {
        // Configure CPU threads
        var threads = options.MaxCpuThreads > 0
            ? options.MaxCpuThreads
            : Environment.ProcessorCount;

        torch.set_num_threads(threads);
        _logger.LogDebug("CPU threads set to: {Threads}", threads);

        // Configure CUDA-specific options
        if (backend == TorchBackend.Cuda)
        {
            // Enable TF32 for faster FP32 operations on Ampere+ GPUs
            if (options.EnableTf32)
            {
                torch.backends.cuda.matmul.allow_tf32 = true;
                torch.backends.cudnn.allow_tf32 = true;
                _logger.LogDebug("TF32 enabled for faster FP32 operations");
            }

            // Enable cuDNN benchmark for optimized convolutions
            if (options.EnableCudnnBenchmark)
            {
                torch.backends.cudnn.benchmark = true;
                _logger.LogDebug("cuDNN benchmark mode enabled");
            }
        }
    }

    private int SelectBestCudaDevice()
    {
        var deviceCount = torch.cuda.device_count();
        if (deviceCount == 1) return 0;

        var bestDevice = 0;
        long maxFreeMemory = 0;

        for (int i = 0; i < deviceCount; i++)
        {
            try
            {
                torch.cuda.set_device(i);
                var (free, _) = torch.cuda.mem_get_info();

                if ((long)free > maxFreeMemory)
                {
                    maxFreeMemory = (long)free;
                    bestDevice = i;
                }
            }
            catch
            {
                // Skip unavailable devices
            }
        }

        _logger.LogDebug(
            "Selected CUDA device {Device} with {Memory}GB free memory",
            bestDevice,
            maxFreeMemory / 1e9);

        return bestDevice;
    }

    private static bool IsMpsAvailable()
    {
        // Check if MPS is available on this system
        // This is a runtime check for Apple Silicon Macs
        try
        {
            // TorchSharp may provide torch.mps.is_available() in newer versions
            return RuntimeInformation.IsOSPlatform(OSPlatform.OSX) &&
                   RuntimeInformation.ProcessArchitecture == Architecture.Arm64;
        }
        catch
        {
            return false;
        }
    }

    private static string GetTorchSharpVersion()
    {
        try
        {
            return typeof(torch).Assembly.GetName().Version?.ToString() ?? "unknown";
        }
        catch
        {
            return "unknown";
        }
    }

    private static string GetLibTorchVersion()
    {
        try
        {
            // TorchSharp may expose this information
            return "2.x"; // Placeholder
        }
        catch
        {
            return "unknown";
        }
    }

    private static string GetNativeLibraryPath()
    {
        try
        {
            // Return the path where native libraries were loaded from
            var assemblyLocation = typeof(torch).Assembly.Location;
            return Path.GetDirectoryName(assemblyLocation) ?? string.Empty;
        }
        catch
        {
            return string.Empty;
        }
    }

    public async Task ShutdownAsync()
    {
        await Task.Run(() =>
        {
            lock (_initLock)
            {
                if (!_initialized) return;

                try
                {
                    _logger.LogInformation("Shutting down TorchSharp");

                    // Clear CUDA cache if applicable
                    if (_state.ActiveBackend == TorchBackend.Cuda)
                    {
                        torch.cuda.empty_cache();
                    }

                    // Force garbage collection of tensor references
                    GC.Collect();
                    GC.WaitForPendingFinalizers();

                    _initialized = false;
                    _state = new TorchSharpState();

                    _logger.LogInformation("TorchSharp shutdown complete");
                }
                catch (Exception ex)
                {
                    _logger.LogError(ex, "Error during TorchSharp shutdown");
                }
            }
        });
    }

    public async Task<bool> VerifyAsync(CancellationToken ct = default)
    {
        if (!_initialized) return false;

        return await Task.Run(() =>
        {
            try
            {
                // Create a simple tensor operation to verify the backend
                using var a = torch.ones(10, 10, device: GetCurrentDevice());
                using var b = torch.ones(10, 10, device: GetCurrentDevice());
                using var c = torch.mm(a, b);

                var result = c.sum().item<float>();
                var expected = 100.0f;

                if (Math.Abs(result - expected) > 0.001f)
                {
                    _logger.LogWarning(
                        "TorchSharp verification failed: expected {Expected}, got {Result}",
                        expected, result);
                    return false;
                }

                _logger.LogDebug("TorchSharp verification passed");
                return true;
            }
            catch (Exception ex)
            {
                _logger.LogError(ex, "TorchSharp verification failed");
                return false;
            }
        }, ct);
    }

    private torch.Device GetCurrentDevice()
    {
        return _state.ActiveBackend switch
        {
            TorchBackend.Cuda => torch.device("cuda"),
            TorchBackend.Mps => torch.device("mps"),
            _ => torch.device("cpu")
        };
    }
}
```

### File: `src/SeniorIntern.Services/Training/TorchSharpStartupExtensions.cs`

```csharp
using Microsoft.Extensions.DependencyInjection;
using Microsoft.Extensions.Hosting;
using Microsoft.Extensions.Logging;
using SeniorIntern.Core.Interfaces;
using SeniorIntern.Core.Training;

namespace SeniorIntern.Services.Training;

/// <summary>
/// Extension methods for registering TorchSharp services.
/// </summary>
public static class TorchSharpStartupExtensions
{
    /// <summary>
    /// Add TorchSharp training infrastructure services.
    /// </summary>
    public static IServiceCollection AddTorchSharpTraining(
        this IServiceCollection services,
        Action<TorchSharpInitOptions>? configure = null)
    {
        var options = new TorchSharpInitOptions();
        configure?.Invoke(options);

        services.AddSingleton(options);
        services.AddSingleton<ITorchSharpInitializer, TorchSharpInitializer>();
        services.AddSingleton<IHardwareDetectionService, HardwareDetectionService>();

        return services;
    }

    /// <summary>
    /// Add TorchSharp as a hosted service that initializes on startup.
    /// </summary>
    public static IServiceCollection AddTorchSharpHostedService(
        this IServiceCollection services)
    {
        services.AddHostedService<TorchSharpHostedService>();
        return services;
    }
}

/// <summary>
/// Background service that initializes TorchSharp on application startup.
/// </summary>
public sealed class TorchSharpHostedService : IHostedService
{
    private readonly ITorchSharpInitializer _initializer;
    private readonly TorchSharpInitOptions _options;
    private readonly ILogger<TorchSharpHostedService> _logger;

    public TorchSharpHostedService(
        ITorchSharpInitializer initializer,
        TorchSharpInitOptions options,
        ILogger<TorchSharpHostedService> logger)
    {
        _initializer = initializer;
        _options = options;
        _logger = logger;
    }

    public async Task StartAsync(CancellationToken cancellationToken)
    {
        _logger.LogInformation("Starting TorchSharp initialization...");

        var state = await _initializer.InitializeAsync(_options, cancellationToken);

        if (state.IsInitialized)
        {
            _logger.LogInformation(
                "TorchSharp ready: Backend={Backend}, Version={Version}",
                state.ActiveBackend,
                state.Version);

            foreach (var warning in state.Warnings)
            {
                _logger.LogWarning("TorchSharp: {Warning}", warning);
            }

            // Verify the backend is working
            var verified = await _initializer.VerifyAsync(cancellationToken);
            if (!verified)
            {
                _logger.LogError("TorchSharp verification failed - training may not work correctly");
            }
        }
        else
        {
            _logger.LogError(
                "TorchSharp initialization failed: {Error}",
                state.ErrorMessage);
        }
    }

    public async Task StopAsync(CancellationToken cancellationToken)
    {
        _logger.LogInformation("Shutting down TorchSharp...");
        await _initializer.ShutdownAsync();
    }
}
```

### File to Modify: `src/SeniorIntern.Desktop/Program.cs`

Add TorchSharp service registration:

```csharp
// In the service configuration section, add:
services.AddTorchSharpTraining(options =>
{
    options.PreferredBackend = TorchBackend.Cuda;
    options.AllowCpuFallback = true;
    options.EnableTf32 = true;
    options.EnableCudnnBenchmark = true;
});

// If using hosted services:
services.AddTorchSharpHostedService();
```

---

## v0.8.1b: Hardware Detection Models

### Objective
Define comprehensive models for representing detected hardware capabilities, GPU device information, and training requirements.

### File: `src/SeniorIntern.Core/Training/TrainingHardwareInfo.cs`

```csharp
namespace SeniorIntern.Core.Training;

/// <summary>
/// Comprehensive information about available training hardware.
/// </summary>
public sealed class TrainingHardwareInfo
{
    /// <summary>
    /// Whether NVIDIA CUDA is available on this system.
    /// </summary>
    public bool CudaAvailable { get; init; }

    /// <summary>
    /// CUDA driver version string (e.g., "12.2").
    /// </summary>
    public string? CudaDriverVersion { get; init; }

    /// <summary>
    /// CUDA runtime version string.
    /// </summary>
    public string? CudaRuntimeVersion { get; init; }

    /// <summary>
    /// Number of CUDA-capable devices.
    /// </summary>
    public int CudaDeviceCount { get; init; }

    /// <summary>
    /// Whether Apple Metal Performance Shaders (MPS) is available.
    /// </summary>
    public bool MetalAvailable { get; init; }

    /// <summary>
    /// Whether this is an Apple Silicon Mac (M1/M2/M3).
    /// </summary>
    public bool IsAppleSilicon { get; init; }

    /// <summary>
    /// List of all detected GPU devices.
    /// </summary>
    public IReadOnlyList<GpuDeviceInfo> GpuDevices { get; init; } = Array.Empty<GpuDeviceInfo>();

    /// <summary>
    /// Total system RAM in bytes.
    /// </summary>
    public long TotalSystemMemory { get; init; }

    /// <summary>
    /// Currently available system RAM in bytes.
    /// </summary>
    public long AvailableSystemMemory { get; init; }

    /// <summary>
    /// Number of logical CPU cores.
    /// </summary>
    public int CpuCoreCount { get; init; }

    /// <summary>
    /// CPU model name.
    /// </summary>
    public string CpuName { get; init; } = string.Empty;

    /// <summary>
    /// Recommended device string for training (e.g., "cuda:0", "mps", "cpu").
    /// </summary>
    public string RecommendedDevice { get; init; } = "cpu";

    /// <summary>
    /// Recommended batch size based on available memory.
    /// </summary>
    public int RecommendedBatchSize { get; init; } = 1;

    /// <summary>
    /// Whether mixed precision (FP16/BF16) training is supported.
    /// </summary>
    public bool SupportsMixedPrecision { get; init; }

    /// <summary>
    /// Whether gradient checkpointing is recommended for this hardware.
    /// </summary>
    public bool RecommendGradientCheckpointing { get; init; }

    /// <summary>
    /// Timestamp when this info was collected.
    /// </summary>
    public DateTime DetectedAt { get; init; } = DateTime.UtcNow;

    /// <summary>
    /// Get the primary GPU device (best available).
    /// </summary>
    public GpuDeviceInfo? PrimaryGpu =>
        GpuDevices.OrderByDescending(g => g.FreeMemory).FirstOrDefault();

    /// <summary>
    /// Get total GPU memory across all devices.
    /// </summary>
    public long TotalGpuMemory => GpuDevices.Sum(g => g.TotalMemory);

    /// <summary>
    /// Get total free GPU memory across all devices.
    /// </summary>
    public long TotalFreeGpuMemory => GpuDevices.Sum(g => g.FreeMemory);
}
```

### File: `src/SeniorIntern.Core/Training/GpuDeviceInfo.cs`

```csharp
namespace SeniorIntern.Core.Training;

/// <summary>
/// Information about a single GPU device.
/// </summary>
public sealed class GpuDeviceInfo
{
    /// <summary>
    /// Device index (0-based).
    /// </summary>
    public int DeviceIndex { get; init; }

    /// <summary>
    /// Device name (e.g., "NVIDIA RTX 4090").
    /// </summary>
    public string Name { get; init; } = string.Empty;

    /// <summary>
    /// Total device memory in bytes.
    /// </summary>
    public long TotalMemory { get; init; }

    /// <summary>
    /// Free device memory in bytes.
    /// </summary>
    public long FreeMemory { get; init; }

    /// <summary>
    /// Currently used memory in bytes.
    /// </summary>
    public long UsedMemory => TotalMemory - FreeMemory;

    /// <summary>
    /// Memory usage percentage (0-100).
    /// </summary>
    public double MemoryUsagePercent =>
        TotalMemory > 0 ? (double)UsedMemory / TotalMemory * 100 : 0;

    /// <summary>
    /// GPU backend type.
    /// </summary>
    public GpuBackend Backend { get; init; }

    /// <summary>
    /// CUDA compute capability major version (CUDA only).
    /// </summary>
    public int ComputeCapabilityMajor { get; init; }

    /// <summary>
    /// CUDA compute capability minor version (CUDA only).
    /// </summary>
    public int ComputeCapabilityMinor { get; init; }

    /// <summary>
    /// Full compute capability string (e.g., "8.9").
    /// </summary>
    public string ComputeCapability =>
        Backend == GpuBackend.Cuda
            ? $"{ComputeCapabilityMajor}.{ComputeCapabilityMinor}"
            : "N/A";

    /// <summary>
    /// Number of CUDA multiprocessors (CUDA only).
    /// </summary>
    public int MultiprocessorCount { get; init; }

    /// <summary>
    /// GPU clock rate in MHz.
    /// </summary>
    public int ClockRateMHz { get; init; }

    /// <summary>
    /// Memory clock rate in MHz.
    /// </summary>
    public int MemoryClockRateMHz { get; init; }

    /// <summary>
    /// Memory bus width in bits.
    /// </summary>
    public int MemoryBusWidth { get; init; }

    /// <summary>
    /// Current GPU temperature in Celsius.
    /// </summary>
    public float TemperatureCelsius { get; init; }

    /// <summary>
    /// Current power usage in watts.
    /// </summary>
    public float PowerUsageWatts { get; init; }

    /// <summary>
    /// Maximum power limit in watts.
    /// </summary>
    public float PowerLimitWatts { get; init; }

    /// <summary>
    /// Current GPU utilization percentage (0-100).
    /// </summary>
    public float UtilizationPercent { get; init; }

    /// <summary>
    /// PCIe bus ID (e.g., "00000000:01:00.0").
    /// </summary>
    public string PciBusId { get; init; } = string.Empty;

    /// <summary>
    /// Whether this GPU supports FP16 operations.
    /// </summary>
    public bool SupportsFp16 { get; init; }

    /// <summary>
    /// Whether this GPU supports BF16 operations (Ampere+).
    /// </summary>
    public bool SupportsBf16 { get; init; }

    /// <summary>
    /// Whether this GPU supports TF32 operations (Ampere+).
    /// </summary>
    public bool SupportsTf32 { get; init; }

    /// <summary>
    /// Whether this GPU has tensor cores.
    /// </summary>
    public bool HasTensorCores { get; init; }

    /// <summary>
    /// Total memory in human-readable format.
    /// </summary>
    public string TotalMemoryFormatted => FormatBytes(TotalMemory);

    /// <summary>
    /// Free memory in human-readable format.
    /// </summary>
    public string FreeMemoryFormatted => FormatBytes(FreeMemory);

    private static string FormatBytes(long bytes)
    {
        string[] suffixes = { "B", "KB", "MB", "GB", "TB" };
        var order = 0;
        double size = bytes;

        while (size >= 1024 && order < suffixes.Length - 1)
        {
            order++;
            size /= 1024;
        }

        return $"{size:0.##} {suffixes[order]}";
    }
}

/// <summary>
/// GPU compute backend types.
/// </summary>
public enum GpuBackend
{
    /// <summary>
    /// No GPU / CPU only.
    /// </summary>
    None,

    /// <summary>
    /// NVIDIA CUDA backend.
    /// </summary>
    Cuda,

    /// <summary>
    /// Apple Metal Performance Shaders.
    /// </summary>
    Metal,

    /// <summary>
    /// Vulkan compute (future support).
    /// </summary>
    Vulkan,

    /// <summary>
    /// AMD ROCm (future support).
    /// </summary>
    Rocm
}
```

### File: `src/SeniorIntern.Core/Training/TrainingRequirements.cs`

```csharp
namespace SeniorIntern.Core.Training;

/// <summary>
/// Minimum hardware requirements for a training job.
/// </summary>
public sealed class TrainingRequirements
{
    /// <summary>
    /// Minimum VRAM required in bytes.
    /// </summary>
    public long MinimumVramBytes { get; init; }

    /// <summary>
    /// Minimum system RAM required in bytes.
    /// </summary>
    public long MinimumRamBytes { get; init; }

    /// <summary>
    /// Whether GPU acceleration is required (cannot run on CPU).
    /// </summary>
    public bool RequiresGpu { get; init; }

    /// <summary>
    /// Minimum CUDA compute capability (e.g., 70 for 7.0).
    /// </summary>
    public int MinimumComputeCapability { get; init; }

    /// <summary>
    /// Whether FP16/BF16 support is required.
    /// </summary>
    public bool RequiresMixedPrecision { get; init; }

    /// <summary>
    /// Minimum number of CPU cores.
    /// </summary>
    public int MinimumCpuCores { get; init; } = 1;

    /// <summary>
    /// Estimated training time multiplier on CPU vs GPU.
    /// Used to warn users about slow CPU training.
    /// </summary>
    public float CpuSlowdownFactor { get; init; } = 50.0f;

    /// <summary>
    /// Create requirements for a specific model size.
    /// </summary>
    public static TrainingRequirements ForModelSize(long modelSizeBytes, int loraRank = 16)
    {
        // LoRA training typically needs 2-4x model size in VRAM
        // More for larger ranks
        var vramMultiplier = loraRank <= 16 ? 2.5 : loraRank <= 32 ? 3.0 : 4.0;

        return new TrainingRequirements
        {
            MinimumVramBytes = (long)(modelSizeBytes * vramMultiplier),
            MinimumRamBytes = modelSizeBytes * 2,
            RequiresGpu = false, // Can train on CPU, just slower
            MinimumComputeCapability = 70, // Volta or newer for reasonable speed
            RequiresMixedPrecision = false,
            MinimumCpuCores = 4
        };
    }
}

/// <summary>
/// Result of checking if training is possible on this hardware.
/// </summary>
public sealed class TrainingCapabilityResult
{
    /// <summary>
    /// Whether training can proceed on this hardware.
    /// </summary>
    public bool CanTrain { get; init; }

    /// <summary>
    /// Primary reason if training is not possible.
    /// </summary>
    public string? Reason { get; init; }

    /// <summary>
    /// Warnings about potential issues (training may be slow, etc.).
    /// </summary>
    public IReadOnlyList<string> Warnings { get; init; } = Array.Empty<string>();

    /// <summary>
    /// Suggestions for improving training capability.
    /// </summary>
    public IReadOnlyList<string> Suggestions { get; init; } = Array.Empty<string>();

    /// <summary>
    /// The detected hardware info.
    /// </summary>
    public TrainingHardwareInfo Hardware { get; init; } = null!;

    /// <summary>
    /// Estimated training speed category.
    /// </summary>
    public TrainingSpeedCategory SpeedCategory { get; init; }

    /// <summary>
    /// Estimated time multiplier compared to reference hardware.
    /// </summary>
    public float EstimatedTimeMultiplier { get; init; } = 1.0f;
}

/// <summary>
/// Categories for training speed estimation.
/// </summary>
public enum TrainingSpeedCategory
{
    /// <summary>
    /// Training will be very slow (CPU or old GPU).
    /// </summary>
    VerySlow,

    /// <summary>
    /// Training will be slow but usable.
    /// </summary>
    Slow,

    /// <summary>
    /// Normal training speed.
    /// </summary>
    Normal,

    /// <summary>
    /// Fast training (modern GPU with good VRAM).
    /// </summary>
    Fast,

    /// <summary>
    /// Very fast training (high-end GPU).
    /// </summary>
    VeryFast
}
```

### File: `src/SeniorIntern.Core/Training/HardwareMetrics.cs`

```csharp
namespace SeniorIntern.Core.Training;

/// <summary>
/// Real-time hardware metrics snapshot.
/// </summary>
public sealed record HardwareMetrics
{
    /// <summary>
    /// GPU utilization percentage (0-100).
    /// </summary>
    public float GpuUtilization { get; init; }

    /// <summary>
    /// GPU memory currently used in bytes.
    /// </summary>
    public long GpuMemoryUsed { get; init; }

    /// <summary>
    /// Total GPU memory in bytes.
    /// </summary>
    public long GpuMemoryTotal { get; init; }

    /// <summary>
    /// GPU memory reserved by PyTorch allocator in bytes.
    /// </summary>
    public long GpuMemoryReserved { get; init; }

    /// <summary>
    /// GPU memory usage percentage.
    /// </summary>
    public float GpuMemoryUtilization =>
        GpuMemoryTotal > 0 ? (float)GpuMemoryUsed / GpuMemoryTotal * 100 : 0;

    /// <summary>
    /// CPU utilization percentage (0-100).
    /// </summary>
    public float CpuUtilization { get; init; }

    /// <summary>
    /// System RAM currently used by this process in bytes.
    /// </summary>
    public long RamUsed { get; init; }

    /// <summary>
    /// Total available system RAM in bytes.
    /// </summary>
    public long RamTotal { get; init; }

    /// <summary>
    /// RAM usage percentage.
    /// </summary>
    public float RamUtilization =>
        RamTotal > 0 ? (float)RamUsed / RamTotal * 100 : 0;

    /// <summary>
    /// GPU temperature in Celsius.
    /// </summary>
    public float GpuTemperature { get; init; }

    /// <summary>
    /// GPU power usage in watts.
    /// </summary>
    public float GpuPowerUsage { get; init; }

    /// <summary>
    /// GPU fan speed percentage (0-100).
    /// </summary>
    public float GpuFanSpeed { get; init; }

    /// <summary>
    /// Timestamp of this metrics snapshot.
    /// </summary>
    public DateTime Timestamp { get; init; } = DateTime.UtcNow;

    /// <summary>
    /// Format GPU memory as human-readable string.
    /// </summary>
    public string GpuMemoryFormatted =>
        $"{FormatBytes(GpuMemoryUsed)} / {FormatBytes(GpuMemoryTotal)}";

    /// <summary>
    /// Format RAM as human-readable string.
    /// </summary>
    public string RamFormatted =>
        $"{FormatBytes(RamUsed)} / {FormatBytes(RamTotal)}";

    private static string FormatBytes(long bytes)
    {
        if (bytes < 1024) return $"{bytes} B";
        if (bytes < 1024 * 1024) return $"{bytes / 1024.0:F1} KB";
        if (bytes < 1024 * 1024 * 1024) return $"{bytes / (1024.0 * 1024):F1} MB";
        return $"{bytes / (1024.0 * 1024 * 1024):F2} GB";
    }
}

/// <summary>
/// Hardware metrics with timestamp for historical tracking.
/// </summary>
public sealed class TimestampedMetrics
{
    /// <summary>
    /// When these metrics were captured.
    /// </summary>
    public DateTime Timestamp { get; init; }

    /// <summary>
    /// The metrics snapshot.
    /// </summary>
    public HardwareMetrics Metrics { get; init; } = null!;
}
```

---

## v0.8.1c: Hardware Detection Service Interface

### Objective
Define the interface for hardware detection and capability checking services.

### File: `src/SeniorIntern.Core/Interfaces/IHardwareDetectionService.cs`

```csharp
using SeniorIntern.Core.Training;

namespace SeniorIntern.Core.Interfaces;

/// <summary>
/// Service for detecting available training hardware and determining capabilities.
/// </summary>
public interface IHardwareDetectionService
{
    /// <summary>
    /// Detect all available training hardware.
    /// Results are cached and can be refreshed with the refresh parameter.
    /// </summary>
    /// <param name="refresh">Force refresh of cached hardware info.</param>
    /// <param name="ct">Cancellation token.</param>
    /// <returns>Comprehensive hardware information.</returns>
    Task<TrainingHardwareInfo> DetectHardwareAsync(
        bool refresh = false,
        CancellationToken ct = default);

    /// <summary>
    /// Check if the system meets requirements for a training job.
    /// </summary>
    /// <param name="requirements">Required hardware specifications.</param>
    /// <param name="ct">Cancellation token.</param>
    /// <returns>Capability check result with warnings and suggestions.</returns>
    Task<TrainingCapabilityResult> CheckTrainingCapabilityAsync(
        TrainingRequirements requirements,
        CancellationToken ct = default);

    /// <summary>
    /// Get recommended training configuration based on detected hardware
    /// and the specified model/dataset size.
    /// </summary>
    /// <param name="hardware">Detected hardware info.</param>
    /// <param name="modelSizeBytes">Size of the base model in bytes.</param>
    /// <param name="datasetSize">Number of training examples.</param>
    /// <returns>Optimized training configuration.</returns>
    TrainingConfiguration GetRecommendedConfiguration(
        TrainingHardwareInfo hardware,
        long modelSizeBytes,
        int datasetSize);

    /// <summary>
    /// Estimate memory requirements for a training configuration.
    /// </summary>
    /// <param name="config">Proposed training configuration.</param>
    /// <param name="modelSizeBytes">Size of the base model in bytes.</param>
    /// <returns>Estimated memory requirements.</returns>
    MemoryEstimate EstimateMemoryRequirements(
        TrainingConfiguration config,
        long modelSizeBytes);

    /// <summary>
    /// Get the current GPU device index being used.
    /// Returns -1 if no GPU is available.
    /// </summary>
    int CurrentGpuDeviceIndex { get; }

    /// <summary>
    /// Event fired when hardware state changes significantly.
    /// (e.g., GPU memory pressure, temperature throttling)
    /// </summary>
    event EventHandler<HardwareStateChangedEventArgs>? HardwareStateChanged;
}

/// <summary>
/// Event args for hardware state changes.
/// </summary>
public sealed class HardwareStateChangedEventArgs : EventArgs
{
    /// <summary>
    /// Type of state change.
    /// </summary>
    public HardwareStateChangeType ChangeType { get; init; }

    /// <summary>
    /// Descriptive message about the change.
    /// </summary>
    public string Message { get; init; } = string.Empty;

    /// <summary>
    /// Previous hardware metrics (if applicable).
    /// </summary>
    public HardwareMetrics? PreviousMetrics { get; init; }

    /// <summary>
    /// Current hardware metrics.
    /// </summary>
    public HardwareMetrics CurrentMetrics { get; init; } = null!;
}

/// <summary>
/// Types of hardware state changes.
/// </summary>
public enum HardwareStateChangeType
{
    /// <summary>
    /// GPU memory is running low.
    /// </summary>
    MemoryPressure,

    /// <summary>
    /// GPU temperature is high, may throttle.
    /// </summary>
    TemperatureWarning,

    /// <summary>
    /// GPU is being throttled due to heat/power.
    /// </summary>
    Throttling,

    /// <summary>
    /// GPU device became unavailable.
    /// </summary>
    DeviceLost,

    /// <summary>
    /// New GPU device detected.
    /// </summary>
    DeviceAdded,

    /// <summary>
    /// Memory pressure resolved.
    /// </summary>
    MemoryRecovered
}
```

### File: `src/SeniorIntern.Core/Training/MemoryEstimate.cs`

```csharp
namespace SeniorIntern.Core.Training;

/// <summary>
/// Estimated memory requirements for training.
/// </summary>
public sealed class MemoryEstimate
{
    /// <summary>
    /// Estimated GPU memory required in bytes.
    /// </summary>
    public long GpuMemoryRequired { get; init; }

    /// <summary>
    /// Estimated system RAM required in bytes.
    /// </summary>
    public long RamRequired { get; init; }

    /// <summary>
    /// Peak GPU memory expected during training.
    /// </summary>
    public long PeakGpuMemory { get; init; }

    /// <summary>
    /// Memory for model weights (frozen).
    /// </summary>
    public long ModelWeightsMemory { get; init; }

    /// <summary>
    /// Memory for LoRA adapter weights.
    /// </summary>
    public long LoraWeightsMemory { get; init; }

    /// <summary>
    /// Memory for optimizer states.
    /// </summary>
    public long OptimizerMemory { get; init; }

    /// <summary>
    /// Memory for gradients.
    /// </summary>
    public long GradientsMemory { get; init; }

    /// <summary>
    /// Memory for activations (forward pass).
    /// </summary>
    public long ActivationsMemory { get; init; }

    /// <summary>
    /// Memory for input batches.
    /// </summary>
    public long BatchMemory { get; init; }

    /// <summary>
    /// Additional overhead buffer.
    /// </summary>
    public long OverheadBuffer { get; init; }

    /// <summary>
    /// Whether this estimate fits in available GPU memory.
    /// </summary>
    public bool FitsInGpuMemory { get; init; }

    /// <summary>
    /// Available GPU memory at estimation time.
    /// </summary>
    public long AvailableGpuMemory { get; init; }

    /// <summary>
    /// Recommendations if memory is insufficient.
    /// </summary>
    public IReadOnlyList<string> Recommendations { get; init; } = Array.Empty<string>();

    /// <summary>
    /// Get a detailed breakdown as formatted string.
    /// </summary>
    public string GetBreakdown()
    {
        return $"""
            Memory Estimate Breakdown:
            ─────────────────────────────────
            Model Weights:    {FormatBytes(ModelWeightsMemory)}
            LoRA Weights:     {FormatBytes(LoraWeightsMemory)}
            Optimizer States: {FormatBytes(OptimizerMemory)}
            Gradients:        {FormatBytes(GradientsMemory)}
            Activations:      {FormatBytes(ActivationsMemory)}
            Batch Data:       {FormatBytes(BatchMemory)}
            Overhead Buffer:  {FormatBytes(OverheadBuffer)}
            ─────────────────────────────────
            Total Required:   {FormatBytes(GpuMemoryRequired)}
            Peak Expected:    {FormatBytes(PeakGpuMemory)}
            Available:        {FormatBytes(AvailableGpuMemory)}
            Status:           {(FitsInGpuMemory ? "✓ Fits" : "✗ Insufficient")}
            """;
    }

    private static string FormatBytes(long bytes)
    {
        if (bytes < 1024) return $"{bytes} B";
        if (bytes < 1024 * 1024) return $"{bytes / 1024.0:F1} KB";
        if (bytes < 1024 * 1024 * 1024) return $"{bytes / (1024.0 * 1024):F1} MB";
        return $"{bytes / (1024.0 * 1024 * 1024):F2} GB";
    }
}
```

### File: `src/SeniorIntern.Core/Training/MemoryEstimator.cs`

```csharp
namespace SeniorIntern.Core.Training;

/// <summary>
/// Utility class for estimating training memory requirements.
/// </summary>
public static class MemoryEstimator
{
    // Approximate bytes per parameter for different data types
    private const int BytesPerFloat32 = 4;
    private const int BytesPerFloat16 = 2;
    private const int BytesPerBFloat16 = 2;

    /// <summary>
    /// Estimate memory requirements for LoRA training.
    /// </summary>
    /// <param name="modelSizeBytes">Size of base model file.</param>
    /// <param name="config">Training configuration.</param>
    /// <param name="sequenceLength">Maximum sequence length.</param>
    /// <param name="availableGpuMemory">Available GPU memory in bytes.</param>
    public static MemoryEstimate EstimateLoraTrainingMemory(
        long modelSizeBytes,
        TrainingConfiguration config,
        int sequenceLength,
        long availableGpuMemory)
    {
        // Estimate number of parameters from model size
        // GGUF Q4 is roughly 0.5 bytes per param, FP16 is 2 bytes
        var estimatedParams = modelSizeBytes * 2; // Rough estimate

        var bytesPerParam = config.UseMixedPrecision
            ? BytesPerFloat16
            : BytesPerFloat32;

        // Model weights in memory (during training, may be FP16 or FP32)
        var modelWeightsMemory = estimatedParams * bytesPerParam;

        // LoRA weights: r * (in + out) for each target layer
        // Assume ~7 layers per transformer block, ~32 blocks for 7B model
        var loraParamsPerLayer = config.LoraRank * 2 * 4096; // Rough estimate
        var numLoraLayers = config.TargetModules.Count * 32; // Per transformer block
        var totalLoraParams = loraParamsPerLayer * numLoraLayers;
        var loraWeightsMemory = totalLoraParams * BytesPerFloat32; // LoRA always FP32

        // Optimizer states (AdamW: 2 states per parameter)
        var optimizerMemory = totalLoraParams * BytesPerFloat32 * 2;

        // Gradients for LoRA parameters
        var gradientsMemory = totalLoraParams * bytesPerParam;

        // Activations: roughly sequence_length * hidden_dim * batch_size * num_layers
        // With gradient checkpointing, this is significantly reduced
        var hiddenDim = 4096; // Typical for 7B model
        var numLayers = 32;
        var activationsPerSample = sequenceLength * hiddenDim * numLayers * bytesPerParam;

        if (config.UseGradientCheckpointing)
        {
            // Checkpointing reduces activation memory by sqrt(layers) factor
            activationsPerSample = (long)(activationsPerSample / Math.Sqrt(numLayers));
        }

        var activationsMemory = activationsPerSample * config.BatchSize;

        // Batch data: input_ids + attention_mask + labels
        var batchMemory = (long)config.BatchSize * sequenceLength * 3 * sizeof(int);

        // Overhead buffer (10-20% for fragmentation, temp allocations)
        var subtotal = modelWeightsMemory + loraWeightsMemory + optimizerMemory +
                       gradientsMemory + activationsMemory + batchMemory;
        var overheadBuffer = (long)(subtotal * 0.15);

        var totalRequired = subtotal + overheadBuffer;
        var peakMemory = (long)(totalRequired * 1.2); // Account for spikes

        var fitsInMemory = totalRequired <= availableGpuMemory;

        var recommendations = new List<string>();
        if (!fitsInMemory)
        {
            var deficit = totalRequired - availableGpuMemory;
            recommendations.Add($"Need {FormatBytes(deficit)} more GPU memory");

            if (!config.UseGradientCheckpointing)
            {
                recommendations.Add("Enable gradient checkpointing to reduce activation memory");
            }
            if (config.BatchSize > 1)
            {
                recommendations.Add($"Reduce batch size from {config.BatchSize} to 1");
            }
            if (!config.UseMixedPrecision)
            {
                recommendations.Add("Enable mixed precision (FP16) training");
            }
            if (config.LoraRank > 8)
            {
                recommendations.Add($"Reduce LoRA rank from {config.LoraRank} to 8");
            }
        }

        return new MemoryEstimate
        {
            GpuMemoryRequired = totalRequired,
            RamRequired = modelSizeBytes * 2, // For loading model
            PeakGpuMemory = peakMemory,
            ModelWeightsMemory = modelWeightsMemory,
            LoraWeightsMemory = loraWeightsMemory,
            OptimizerMemory = optimizerMemory,
            GradientsMemory = gradientsMemory,
            ActivationsMemory = activationsMemory,
            BatchMemory = batchMemory,
            OverheadBuffer = overheadBuffer,
            FitsInGpuMemory = fitsInMemory,
            AvailableGpuMemory = availableGpuMemory,
            Recommendations = recommendations
        };
    }

    private static string FormatBytes(long bytes)
    {
        if (bytes < 1024) return $"{bytes} B";
        if (bytes < 1024 * 1024) return $"{bytes / 1024.0:F1} KB";
        if (bytes < 1024 * 1024 * 1024) return $"{bytes / (1024.0 * 1024):F1} MB";
        return $"{bytes / (1024.0 * 1024 * 1024):F2} GB";
    }
}
```

---

## v0.8.1d: CUDA Hardware Detection

### Objective
Implement NVIDIA CUDA-specific hardware detection including device enumeration, memory queries, compute capability detection, and NVML integration for detailed GPU metrics.

### File: `src/SeniorIntern.Services/Training/CudaHardwareDetector.cs`

```csharp
using System.Diagnostics;
using System.Runtime.InteropServices;
using Microsoft.Extensions.Logging;
using SeniorIntern.Core.Training;
using TorchSharp;

namespace SeniorIntern.Services.Training;

/// <summary>
/// Detects NVIDIA CUDA hardware capabilities.
/// </summary>
public sealed class CudaHardwareDetector
{
    private readonly ILogger<CudaHardwareDetector> _logger;

    public CudaHardwareDetector(ILogger<CudaHardwareDetector> logger)
    {
        _logger = logger;
    }

    /// <summary>
    /// Check if CUDA is available on this system.
    /// </summary>
    public bool IsCudaAvailable()
    {
        try
        {
            return torch.cuda.is_available();
        }
        catch (Exception ex)
        {
            _logger.LogDebug(ex, "CUDA availability check failed");
            return false;
        }
    }

    /// <summary>
    /// Get the number of CUDA devices.
    /// </summary>
    public int GetDeviceCount()
    {
        try
        {
            return torch.cuda.is_available() ? torch.cuda.device_count() : 0;
        }
        catch
        {
            return 0;
        }
    }

    /// <summary>
    /// Detect all CUDA GPU devices with detailed information.
    /// </summary>
    public async Task<IReadOnlyList<GpuDeviceInfo>> DetectDevicesAsync(
        CancellationToken ct = default)
    {
        if (!IsCudaAvailable())
        {
            return Array.Empty<GpuDeviceInfo>();
        }

        return await Task.Run(() =>
        {
            var devices = new List<GpuDeviceInfo>();
            var deviceCount = GetDeviceCount();

            for (int i = 0; i < deviceCount; i++)
            {
                ct.ThrowIfCancellationRequested();

                try
                {
                    var device = GetDeviceInfo(i);
                    devices.Add(device);
                }
                catch (Exception ex)
                {
                    _logger.LogWarning(ex, "Failed to get info for CUDA device {Index}", i);
                }
            }

            return devices;
        }, ct);
    }

    /// <summary>
    /// Get detailed information about a specific CUDA device.
    /// </summary>
    public GpuDeviceInfo GetDeviceInfo(int deviceIndex)
    {
        // Get basic properties from TorchSharp
        torch.cuda.set_device(deviceIndex);
        var props = torch.cuda.get_device_properties(deviceIndex);

        // Get memory info
        var (freeMemory, totalMemory) = torch.cuda.mem_get_info(deviceIndex);

        // Get additional metrics from nvidia-smi if available
        var nvmlMetrics = TryGetNvmlMetrics(deviceIndex);

        // Determine feature support based on compute capability
        var major = props.major;
        var minor = props.minor;
        var computeCapability = major * 10 + minor;

        return new GpuDeviceInfo
        {
            DeviceIndex = deviceIndex,
            Name = props.name,
            TotalMemory = (long)totalMemory,
            FreeMemory = (long)freeMemory,
            Backend = GpuBackend.Cuda,
            ComputeCapabilityMajor = major,
            ComputeCapabilityMinor = minor,
            MultiprocessorCount = props.multi_processor_count,
            ClockRateMHz = nvmlMetrics?.ClockRateMHz ?? 0,
            MemoryClockRateMHz = nvmlMetrics?.MemoryClockRateMHz ?? 0,
            MemoryBusWidth = nvmlMetrics?.MemoryBusWidth ?? 0,
            TemperatureCelsius = nvmlMetrics?.Temperature ?? 0,
            PowerUsageWatts = nvmlMetrics?.PowerUsage ?? 0,
            PowerLimitWatts = nvmlMetrics?.PowerLimit ?? 0,
            UtilizationPercent = nvmlMetrics?.Utilization ?? 0,
            PciBusId = nvmlMetrics?.PciBusId ?? string.Empty,
            SupportsFp16 = computeCapability >= 53, // Maxwell+
            SupportsBf16 = computeCapability >= 80, // Ampere+
            SupportsTf32 = computeCapability >= 80, // Ampere+
            HasTensorCores = computeCapability >= 70 // Volta+
        };
    }

    /// <summary>
    /// Get current memory usage for a device.
    /// </summary>
    public (long Used, long Free, long Total) GetMemoryInfo(int deviceIndex)
    {
        try
        {
            var (free, total) = torch.cuda.mem_get_info(deviceIndex);
            var used = total - free;
            return ((long)used, (long)free, (long)total);
        }
        catch (Exception ex)
        {
            _logger.LogWarning(ex, "Failed to get memory info for device {Index}", deviceIndex);
            return (0, 0, 0);
        }
    }

    /// <summary>
    /// Get the CUDA driver version.
    /// </summary>
    public string GetDriverVersion()
    {
        try
        {
            var output = RunNvidiaSmi("--query-gpu=driver_version", "--format=csv,noheader");
            return output?.Trim() ?? "unknown";
        }
        catch
        {
            return "unknown";
        }
    }

    /// <summary>
    /// Get the CUDA runtime version.
    /// </summary>
    public string GetRuntimeVersion()
    {
        try
        {
            // TorchSharp doesn't directly expose this, query from nvidia-smi
            var output = RunNvidiaSmi("--query-gpu=cuda_version", "--format=csv,noheader");
            return output?.Trim() ?? "unknown";
        }
        catch
        {
            return "unknown";
        }
    }

    private NvmlMetrics? TryGetNvmlMetrics(int deviceIndex)
    {
        try
        {
            // Query nvidia-smi for detailed metrics
            var query = "temperature.gpu,power.draw,power.limit,utilization.gpu," +
                        "clocks.current.graphics,clocks.current.memory," +
                        "memory.total,memory.free,pcie.link.width.max,gpu_bus_id";

            var output = RunNvidiaSmi(
                $"--query-gpu={query}",
                "--format=csv,noheader,nounits",
                $"-i {deviceIndex}");

            if (string.IsNullOrEmpty(output))
                return null;

            var parts = output.Split(',').Select(p => p.Trim()).ToArray();
            if (parts.Length < 10)
                return null;

            return new NvmlMetrics
            {
                Temperature = float.TryParse(parts[0], out var temp) ? temp : 0,
                PowerUsage = float.TryParse(parts[1], out var power) ? power : 0,
                PowerLimit = float.TryParse(parts[2], out var limit) ? limit : 0,
                Utilization = float.TryParse(parts[3], out var util) ? util : 0,
                ClockRateMHz = int.TryParse(parts[4], out var clock) ? clock : 0,
                MemoryClockRateMHz = int.TryParse(parts[5], out var memClock) ? memClock : 0,
                MemoryBusWidth = int.TryParse(parts[8], out var busWidth) ? busWidth * 8 : 0,
                PciBusId = parts[9]
            };
        }
        catch (Exception ex)
        {
            _logger.LogDebug(ex, "Failed to get NVML metrics for device {Index}", deviceIndex);
            return null;
        }
    }

    private string? RunNvidiaSmi(params string[] args)
    {
        try
        {
            var startInfo = new ProcessStartInfo
            {
                FileName = "nvidia-smi",
                Arguments = string.Join(" ", args),
                RedirectStandardOutput = true,
                RedirectStandardError = true,
                UseShellExecute = false,
                CreateNoWindow = true
            };

            using var process = Process.Start(startInfo);
            if (process == null) return null;

            var output = process.StandardOutput.ReadToEnd();
            process.WaitForExit(5000);

            return process.ExitCode == 0 ? output : null;
        }
        catch
        {
            return null;
        }
    }

    private sealed class NvmlMetrics
    {
        public float Temperature { get; init; }
        public float PowerUsage { get; init; }
        public float PowerLimit { get; init; }
        public float Utilization { get; init; }
        public int ClockRateMHz { get; init; }
        public int MemoryClockRateMHz { get; init; }
        public int MemoryBusWidth { get; init; }
        public string PciBusId { get; init; } = string.Empty;
    }
}
```

### File: `src/SeniorIntern.Services/Training/CudaMemoryManager.cs`

```csharp
using Microsoft.Extensions.Logging;
using TorchSharp;

namespace SeniorIntern.Services.Training;

/// <summary>
/// Manages CUDA memory allocation and cleanup.
/// </summary>
public sealed class CudaMemoryManager : IDisposable
{
    private readonly ILogger<CudaMemoryManager> _logger;
    private readonly int _deviceIndex;
    private long _peakMemoryUsed;
    private bool _disposed;

    public CudaMemoryManager(
        ILogger<CudaMemoryManager> logger,
        int deviceIndex = 0)
    {
        _logger = logger;
        _deviceIndex = deviceIndex;
    }

    /// <summary>
    /// Peak memory usage observed during this session.
    /// </summary>
    public long PeakMemoryUsed => _peakMemoryUsed;

    /// <summary>
    /// Get current memory usage.
    /// </summary>
    public (long Allocated, long Reserved, long Free) GetMemoryUsage()
    {
        if (!torch.cuda.is_available()) return (0, 0, 0);

        try
        {
            torch.cuda.set_device(_deviceIndex);
            var allocated = torch.cuda.memory_allocated(_deviceIndex);
            var reserved = torch.cuda.memory_reserved(_deviceIndex);
            var (free, _) = torch.cuda.mem_get_info(_deviceIndex);

            var allocatedLong = (long)allocated;
            if (allocatedLong > _peakMemoryUsed)
            {
                _peakMemoryUsed = allocatedLong;
            }

            return (allocatedLong, (long)reserved, (long)free);
        }
        catch (Exception ex)
        {
            _logger.LogWarning(ex, "Failed to get CUDA memory usage");
            return (0, 0, 0);
        }
    }

    /// <summary>
    /// Clear the CUDA memory cache.
    /// This releases unused cached memory back to the system.
    /// </summary>
    public void ClearCache()
    {
        if (!torch.cuda.is_available()) return;

        try
        {
            torch.cuda.empty_cache();
            _logger.LogDebug("CUDA cache cleared");
        }
        catch (Exception ex)
        {
            _logger.LogWarning(ex, "Failed to clear CUDA cache");
        }
    }

    /// <summary>
    /// Synchronize the CUDA device.
    /// Waits for all CUDA operations to complete.
    /// </summary>
    public void Synchronize()
    {
        if (!torch.cuda.is_available()) return;

        try
        {
            torch.cuda.synchronize(_deviceIndex);
        }
        catch (Exception ex)
        {
            _logger.LogWarning(ex, "Failed to synchronize CUDA device");
        }
    }

    /// <summary>
    /// Reset peak memory tracking.
    /// </summary>
    public void ResetPeakMemory()
    {
        _peakMemoryUsed = 0;

        if (torch.cuda.is_available())
        {
            try
            {
                torch.cuda.reset_peak_memory_stats(_deviceIndex);
            }
            catch
            {
                // Not critical if this fails
            }
        }
    }

    /// <summary>
    /// Attempt to recover from an out-of-memory condition.
    /// </summary>
    /// <returns>True if recovery was successful.</returns>
    public bool TryRecoverFromOom()
    {
        _logger.LogWarning("Attempting to recover from CUDA OOM");

        try
        {
            // Force garbage collection of .NET objects holding tensors
            GC.Collect();
            GC.WaitForPendingFinalizers();
            GC.Collect();

            // Clear CUDA cache
            ClearCache();

            // Synchronize to ensure all operations complete
            Synchronize();

            // Check if we recovered some memory
            var (_, _, free) = GetMemoryUsage();
            _logger.LogInformation("OOM recovery freed {Memory:F2} GB", free / 1e9);

            return free > 100 * 1024 * 1024; // At least 100MB free
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Failed to recover from CUDA OOM");
            return false;
        }
    }

    /// <summary>
    /// Set memory fraction limit for this device.
    /// </summary>
    /// <param name="fraction">Fraction of total memory to use (0-1).</param>
    public void SetMemoryFraction(double fraction)
    {
        if (fraction <= 0 || fraction > 1)
            throw new ArgumentOutOfRangeException(nameof(fraction), "Must be between 0 and 1");

        try
        {
            torch.cuda.set_per_process_memory_fraction(fraction, _deviceIndex);
            _logger.LogInformation(
                "Set CUDA memory fraction to {Fraction:P0} for device {Device}",
                fraction, _deviceIndex);
        }
        catch (Exception ex)
        {
            _logger.LogWarning(ex, "Failed to set CUDA memory fraction");
        }
    }

    public void Dispose()
    {
        if (_disposed) return;
        _disposed = true;

        try
        {
            ClearCache();
            Synchronize();
        }
        catch
        {
            // Ignore errors during disposal
        }
    }
}
```

### File: `src/SeniorIntern.Services/Training/CudaDeviceGuard.cs`

```csharp
using TorchSharp;

namespace SeniorIntern.Services.Training;

/// <summary>
/// RAII guard for temporarily switching CUDA device context.
/// Restores the previous device when disposed.
/// </summary>
public readonly struct CudaDeviceGuard : IDisposable
{
    private readonly int _previousDevice;
    private readonly bool _hadDevice;

    /// <summary>
    /// Switch to the specified CUDA device for the scope of this guard.
    /// </summary>
    /// <param name="deviceIndex">Device index to switch to.</param>
    public CudaDeviceGuard(int deviceIndex)
    {
        if (torch.cuda.is_available())
        {
            _hadDevice = true;
            _previousDevice = (int)torch.cuda.current_device();
            torch.cuda.set_device(deviceIndex);
        }
        else
        {
            _hadDevice = false;
            _previousDevice = 0;
        }
    }

    /// <summary>
    /// Restore the previous device.
    /// </summary>
    public void Dispose()
    {
        if (_hadDevice && torch.cuda.is_available())
        {
            torch.cuda.set_device(_previousDevice);
        }
    }
}
```

---

## v0.8.1e: Metal (Apple Silicon) Hardware Detection

### Objective
Implement Apple Metal Performance Shaders (MPS) detection for Apple Silicon Macs, including unified memory queries and performance tier detection.

### File: `src/SeniorIntern.Services/Training/MetalHardwareDetector.cs`

```csharp
using System.Diagnostics;
using System.Runtime.InteropServices;
using Microsoft.Extensions.Logging;
using SeniorIntern.Core.Training;

namespace SeniorIntern.Services.Training;

/// <summary>
/// Detects Apple Metal hardware capabilities on macOS.
/// </summary>
public sealed class MetalHardwareDetector
{
    private readonly ILogger<MetalHardwareDetector> _logger;

    public MetalHardwareDetector(ILogger<MetalHardwareDetector> logger)
    {
        _logger = logger;
    }

    /// <summary>
    /// Check if this is an Apple Silicon Mac.
    /// </summary>
    public bool IsAppleSilicon()
    {
        return RuntimeInformation.IsOSPlatform(OSPlatform.OSX) &&
               RuntimeInformation.ProcessArchitecture == Architecture.Arm64;
    }

    /// <summary>
    /// Check if Metal Performance Shaders are available for ML.
    /// </summary>
    public bool IsMpsAvailable()
    {
        if (!IsAppleSilicon())
            return false;

        try
        {
            // Check if MPS is available by querying the system
            // TorchSharp may have torch.mps.is_available() in newer versions
            return CheckMpsAvailability();
        }
        catch (Exception ex)
        {
            _logger.LogDebug(ex, "MPS availability check failed");
            return false;
        }
    }

    /// <summary>
    /// Detect Metal GPU device information.
    /// </summary>
    public async Task<IReadOnlyList<GpuDeviceInfo>> DetectDevicesAsync(
        CancellationToken ct = default)
    {
        if (!IsAppleSilicon())
        {
            return Array.Empty<GpuDeviceInfo>();
        }

        return await Task.Run(() =>
        {
            var devices = new List<GpuDeviceInfo>();

            try
            {
                var deviceInfo = GetAppleSiliconInfo();
                if (deviceInfo != null)
                {
                    devices.Add(deviceInfo);
                }
            }
            catch (Exception ex)
            {
                _logger.LogWarning(ex, "Failed to detect Metal device");
            }

            return devices;
        }, ct);
    }

    private GpuDeviceInfo? GetAppleSiliconInfo()
    {
        var chipInfo = GetChipInfo();
        var memoryInfo = GetUnifiedMemoryInfo();

        if (chipInfo == null)
            return null;

        return new GpuDeviceInfo
        {
            DeviceIndex = 0,
            Name = chipInfo.ChipName,
            TotalMemory = memoryInfo.TotalMemory,
            FreeMemory = memoryInfo.FreeMemory,
            Backend = GpuBackend.Metal,
            ComputeCapabilityMajor = 0, // N/A for Metal
            ComputeCapabilityMinor = 0,
            MultiprocessorCount = chipInfo.GpuCoreCount,
            ClockRateMHz = 0, // Not exposed by Metal
            MemoryClockRateMHz = 0,
            MemoryBusWidth = 0,
            TemperatureCelsius = 0, // Would need IOKit for this
            PowerUsageWatts = 0,
            PowerLimitWatts = chipInfo.MaxPowerWatts,
            UtilizationPercent = 0,
            PciBusId = string.Empty,
            SupportsFp16 = true, // All Apple Silicon supports FP16
            SupportsBf16 = chipInfo.SupportsBf16,
            SupportsTf32 = false, // NVIDIA-specific
            HasTensorCores = chipInfo.HasNeuralEngine
        };
    }

    private AppleSiliconChipInfo? GetChipInfo()
    {
        try
        {
            // Get chip name using sysctl
            var chipName = RunSysctl("machdep.cpu.brand_string");
            var coreCount = int.TryParse(RunSysctl("hw.ncpu"), out var cores) ? cores : 8;

            // Determine chip type and capabilities from name
            var chipType = ParseChipType(chipName);

            return new AppleSiliconChipInfo
            {
                ChipName = chipName ?? "Apple Silicon",
                ChipType = chipType,
                CpuCoreCount = coreCount,
                GpuCoreCount = EstimateGpuCores(chipType),
                HasNeuralEngine = true, // All Apple Silicon has Neural Engine
                SupportsBf16 = chipType >= AppleSiliconChipType.M2, // M2+ supports BF16
                MaxPowerWatts = EstimateMaxPower(chipType)
            };
        }
        catch (Exception ex)
        {
            _logger.LogWarning(ex, "Failed to get Apple Silicon chip info");
            return null;
        }
    }

    private UnifiedMemoryInfo GetUnifiedMemoryInfo()
    {
        try
        {
            // Get total physical memory
            var totalMemStr = RunSysctl("hw.memsize");
            var totalMemory = long.TryParse(totalMemStr, out var total) ? total : 8L * 1024 * 1024 * 1024;

            // Get available memory (this is approximate)
            var pageSize = int.TryParse(RunSysctl("hw.pagesize"), out var ps) ? ps : 4096;
            var freePages = long.TryParse(RunVmStat("Pages free"), out var fp) ? fp : 0;
            var freeMemory = freePages * pageSize;

            // On Apple Silicon, GPU can use most of unified memory
            // But we should leave some for the system
            var gpuAvailable = (long)(totalMemory * 0.75); // 75% available for GPU

            return new UnifiedMemoryInfo
            {
                TotalMemory = totalMemory,
                FreeMemory = Math.Min(freeMemory, gpuAvailable),
                GpuAvailableMemory = gpuAvailable
            };
        }
        catch (Exception ex)
        {
            _logger.LogWarning(ex, "Failed to get unified memory info");
            return new UnifiedMemoryInfo
            {
                TotalMemory = 8L * 1024 * 1024 * 1024,
                FreeMemory = 4L * 1024 * 1024 * 1024,
                GpuAvailableMemory = 6L * 1024 * 1024 * 1024
            };
        }
    }

    private static bool CheckMpsAvailability()
    {
        // On Apple Silicon macOS, MPS should always be available
        // This is a simplified check - in practice, TorchSharp would verify
        return RuntimeInformation.IsOSPlatform(OSPlatform.OSX) &&
               RuntimeInformation.ProcessArchitecture == Architecture.Arm64 &&
               Environment.OSVersion.Version.Major >= 12; // macOS 12+ for ML
    }

    private static AppleSiliconChipType ParseChipType(string? chipName)
    {
        if (string.IsNullOrEmpty(chipName))
            return AppleSiliconChipType.M1;

        chipName = chipName.ToUpperInvariant();

        if (chipName.Contains("M3 MAX")) return AppleSiliconChipType.M3Max;
        if (chipName.Contains("M3 PRO")) return AppleSiliconChipType.M3Pro;
        if (chipName.Contains("M3 ULTRA")) return AppleSiliconChipType.M3Ultra;
        if (chipName.Contains("M3")) return AppleSiliconChipType.M3;
        if (chipName.Contains("M2 MAX")) return AppleSiliconChipType.M2Max;
        if (chipName.Contains("M2 PRO")) return AppleSiliconChipType.M2Pro;
        if (chipName.Contains("M2 ULTRA")) return AppleSiliconChipType.M2Ultra;
        if (chipName.Contains("M2")) return AppleSiliconChipType.M2;
        if (chipName.Contains("M1 MAX")) return AppleSiliconChipType.M1Max;
        if (chipName.Contains("M1 PRO")) return AppleSiliconChipType.M1Pro;
        if (chipName.Contains("M1 ULTRA")) return AppleSiliconChipType.M1Ultra;
        if (chipName.Contains("M1")) return AppleSiliconChipType.M1;

        return AppleSiliconChipType.M1;
    }

    private static int EstimateGpuCores(AppleSiliconChipType chipType)
    {
        return chipType switch
        {
            AppleSiliconChipType.M1 => 8,
            AppleSiliconChipType.M1Pro => 16,
            AppleSiliconChipType.M1Max => 32,
            AppleSiliconChipType.M1Ultra => 64,
            AppleSiliconChipType.M2 => 10,
            AppleSiliconChipType.M2Pro => 19,
            AppleSiliconChipType.M2Max => 38,
            AppleSiliconChipType.M2Ultra => 76,
            AppleSiliconChipType.M3 => 10,
            AppleSiliconChipType.M3Pro => 18,
            AppleSiliconChipType.M3Max => 40,
            AppleSiliconChipType.M3Ultra => 80,
            _ => 8
        };
    }

    private static float EstimateMaxPower(AppleSiliconChipType chipType)
    {
        return chipType switch
        {
            AppleSiliconChipType.M1 => 20f,
            AppleSiliconChipType.M1Pro => 30f,
            AppleSiliconChipType.M1Max => 60f,
            AppleSiliconChipType.M1Ultra => 120f,
            AppleSiliconChipType.M2 => 22f,
            AppleSiliconChipType.M2Pro => 35f,
            AppleSiliconChipType.M2Max => 70f,
            AppleSiliconChipType.M2Ultra => 140f,
            AppleSiliconChipType.M3 => 25f,
            AppleSiliconChipType.M3Pro => 40f,
            AppleSiliconChipType.M3Max => 80f,
            AppleSiliconChipType.M3Ultra => 160f,
            _ => 20f
        };
    }

    private static string? RunSysctl(string key)
    {
        try
        {
            var startInfo = new ProcessStartInfo
            {
                FileName = "sysctl",
                Arguments = $"-n {key}",
                RedirectStandardOutput = true,
                UseShellExecute = false,
                CreateNoWindow = true
            };

            using var process = Process.Start(startInfo);
            if (process == null) return null;

            var output = process.StandardOutput.ReadToEnd().Trim();
            process.WaitForExit(1000);

            return process.ExitCode == 0 ? output : null;
        }
        catch
        {
            return null;
        }
    }

    private static string? RunVmStat(string field)
    {
        try
        {
            var startInfo = new ProcessStartInfo
            {
                FileName = "vm_stat",
                RedirectStandardOutput = true,
                UseShellExecute = false,
                CreateNoWindow = true
            };

            using var process = Process.Start(startInfo);
            if (process == null) return null;

            var output = process.StandardOutput.ReadToEnd();
            process.WaitForExit(1000);

            if (process.ExitCode != 0) return null;

            // Parse vm_stat output
            foreach (var line in output.Split('\n'))
            {
                if (line.Contains(field, StringComparison.OrdinalIgnoreCase))
                {
                    var parts = line.Split(':');
                    if (parts.Length >= 2)
                    {
                        return parts[1].Trim().TrimEnd('.');
                    }
                }
            }

            return null;
        }
        catch
        {
            return null;
        }
    }

    private enum AppleSiliconChipType
    {
        M1, M1Pro, M1Max, M1Ultra,
        M2, M2Pro, M2Max, M2Ultra,
        M3, M3Pro, M3Max, M3Ultra
    }

    private sealed class AppleSiliconChipInfo
    {
        public string ChipName { get; init; } = string.Empty;
        public AppleSiliconChipType ChipType { get; init; }
        public int CpuCoreCount { get; init; }
        public int GpuCoreCount { get; init; }
        public bool HasNeuralEngine { get; init; }
        public bool SupportsBf16 { get; init; }
        public float MaxPowerWatts { get; init; }
    }

    private sealed class UnifiedMemoryInfo
    {
        public long TotalMemory { get; init; }
        public long FreeMemory { get; init; }
        public long GpuAvailableMemory { get; init; }
    }
}
```

### File: `src/SeniorIntern.Services/Training/MetalMemoryManager.cs`

```csharp
using Microsoft.Extensions.Logging;

namespace SeniorIntern.Services.Training;

/// <summary>
/// Manages Metal/MPS memory for Apple Silicon.
/// Note: MPS uses unified memory with system RAM.
/// </summary>
public sealed class MetalMemoryManager : IDisposable
{
    private readonly ILogger<MetalMemoryManager> _logger;
    private readonly MetalHardwareDetector _detector;
    private long _peakMemoryUsed;
    private bool _disposed;

    public MetalMemoryManager(
        ILogger<MetalMemoryManager> logger,
        MetalHardwareDetector detector)
    {
        _logger = logger;
        _detector = detector;
    }

    /// <summary>
    /// Peak memory usage observed during this session.
    /// </summary>
    public long PeakMemoryUsed => _peakMemoryUsed;

    /// <summary>
    /// Get current memory usage estimate.
    /// Note: MPS uses unified memory, so this reports process memory.
    /// </summary>
    public (long Used, long Free, long Total) GetMemoryUsage()
    {
        try
        {
            var gcInfo = GC.GetGCMemoryInfo();
            var processMemory = System.Diagnostics.Process.GetCurrentProcess().WorkingSet64;
            var totalMemory = gcInfo.TotalAvailableMemoryBytes;
            var freeMemory = totalMemory - processMemory;

            if (processMemory > _peakMemoryUsed)
            {
                _peakMemoryUsed = processMemory;
            }

            return (processMemory, freeMemory, totalMemory);
        }
        catch (Exception ex)
        {
            _logger.LogWarning(ex, "Failed to get memory usage");
            return (0, 0, 0);
        }
    }

    /// <summary>
    /// Clear memory caches.
    /// For MPS, this triggers a GC and autoreleasepool drain.
    /// </summary>
    public void ClearCache()
    {
        try
        {
            // Force garbage collection
            GC.Collect();
            GC.WaitForPendingFinalizers();
            GC.Collect();

            // TorchSharp MPS may have its own cache clearing
            // torch.mps.empty_cache() if available

            _logger.LogDebug("Metal/MPS cache cleared");
        }
        catch (Exception ex)
        {
            _logger.LogWarning(ex, "Failed to clear Metal cache");
        }
    }

    /// <summary>
    /// Synchronize MPS operations.
    /// </summary>
    public void Synchronize()
    {
        try
        {
            // TorchSharp MPS synchronization
            // torch.mps.synchronize() if available

            _logger.LogDebug("Metal/MPS synchronized");
        }
        catch (Exception ex)
        {
            _logger.LogWarning(ex, "Failed to synchronize Metal");
        }
    }

    /// <summary>
    /// Reset peak memory tracking.
    /// </summary>
    public void ResetPeakMemory()
    {
        _peakMemoryUsed = 0;
    }

    /// <summary>
    /// Attempt to recover from an out-of-memory condition.
    /// </summary>
    public bool TryRecoverFromOom()
    {
        _logger.LogWarning("Attempting to recover from Metal OOM");

        try
        {
            ClearCache();
            Synchronize();

            var (_, free, _) = GetMemoryUsage();
            _logger.LogInformation("OOM recovery freed {Memory:F2} GB", free / 1e9);

            return free > 100 * 1024 * 1024;
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Failed to recover from Metal OOM");
            return false;
        }
    }

    public void Dispose()
    {
        if (_disposed) return;
        _disposed = true;

        try
        {
            ClearCache();
            Synchronize();
        }
        catch
        {
            // Ignore errors during disposal
        }
    }
}
```

### File: `src/SeniorIntern.Services/Training/MpsDeviceGuard.cs`

```csharp
namespace SeniorIntern.Services.Training;

/// <summary>
/// RAII guard for MPS device context.
/// Note: MPS typically has only one device, so this is simpler than CUDA.
/// </summary>
public readonly struct MpsDeviceGuard : IDisposable
{
    private readonly bool _active;

    /// <summary>
    /// Enter MPS device context.
    /// </summary>
    public MpsDeviceGuard(bool activate = true)
    {
        _active = activate;

        if (_active)
        {
            // Any MPS-specific setup would go here
            // TorchSharp handles device selection automatically for MPS
        }
    }

    /// <summary>
    /// Exit MPS device context.
    /// </summary>
    public void Dispose()
    {
        if (_active)
        {
            // Any MPS-specific cleanup would go here
        }
    }
}
```

---

## v0.8.1f: CPU Fallback Detection

### Objective
Implement CPU-only fallback detection for systems without GPU support, including core count, memory detection, and AVX/AVX2 instruction set detection for optimized CPU inference.

### File: `src/SeniorIntern.Services/Training/CpuHardwareDetector.cs`

```csharp
using System.Diagnostics;
using System.Runtime.InteropServices;
using System.Runtime.Intrinsics.X86;
using Microsoft.Extensions.Logging;
using SeniorIntern.Core.Training;

namespace SeniorIntern.Services.Training;

/// <summary>
/// Detects CPU hardware capabilities for CPU-only training.
/// </summary>
public sealed class CpuHardwareDetector
{
    private readonly ILogger<CpuHardwareDetector> _logger;

    public CpuHardwareDetector(ILogger<CpuHardwareDetector> logger)
    {
        _logger = logger;
    }

    /// <summary>
    /// Get comprehensive CPU information.
    /// </summary>
    public CpuInfo GetCpuInfo()
    {
        return new CpuInfo
        {
            Name = GetCpuName(),
            CoreCount = Environment.ProcessorCount,
            PhysicalCoreCount = GetPhysicalCoreCount(),
            Architecture = RuntimeInformation.ProcessArchitecture.ToString(),
            HasAvx = Avx.IsSupported,
            HasAvx2 = Avx2.IsSupported,
            HasAvx512 = IsAvx512Supported(),
            HasFma = Fma.IsSupported,
            HasSse42 = Sse42.IsSupported,
            CacheL1 = GetCacheSize(1),
            CacheL2 = GetCacheSize(2),
            CacheL3 = GetCacheSize(3),
            MaxFrequencyMHz = GetMaxFrequency()
        };
    }

    /// <summary>
    /// Get system memory information.
    /// </summary>
    public SystemMemoryInfo GetMemoryInfo()
    {
        var gcInfo = GC.GetGCMemoryInfo();
        var process = Process.GetCurrentProcess();

        return new SystemMemoryInfo
        {
            TotalPhysicalMemory = gcInfo.TotalAvailableMemoryBytes,
            AvailableMemory = gcInfo.TotalAvailableMemoryBytes - process.WorkingSet64,
            ProcessMemoryUsed = process.WorkingSet64,
            ProcessPrivateMemory = process.PrivateMemorySize64,
            ProcessVirtualMemory = process.VirtualMemorySize64
        };
    }

    /// <summary>
    /// Estimate CPU training capability.
    /// </summary>
    public TrainingSpeedCategory EstimateTrainingSpeed(long modelSizeBytes)
    {
        var cpuInfo = GetCpuInfo();
        var memoryInfo = GetMemoryInfo();

        // CPU training is generally slow, but varies by hardware
        var score = 0;

        // Core count
        if (cpuInfo.CoreCount >= 16) score += 3;
        else if (cpuInfo.CoreCount >= 8) score += 2;
        else if (cpuInfo.CoreCount >= 4) score += 1;

        // AVX support (crucial for matrix operations)
        if (cpuInfo.HasAvx512) score += 3;
        else if (cpuInfo.HasAvx2) score += 2;
        else if (cpuInfo.HasAvx) score += 1;

        // FMA support
        if (cpuInfo.HasFma) score += 1;

        // Memory (can we fit the model?)
        if (memoryInfo.AvailableMemory < modelSizeBytes * 2)
        {
            return TrainingSpeedCategory.VerySlow;
        }

        return score switch
        {
            >= 7 => TrainingSpeedCategory.Slow, // Best case for CPU is still "slow"
            >= 4 => TrainingSpeedCategory.VerySlow,
            _ => TrainingSpeedCategory.VerySlow
        };
    }

    private string GetCpuName()
    {
        try
        {
            if (RuntimeInformation.IsOSPlatform(OSPlatform.Windows))
            {
                return GetWindowsCpuName();
            }
            else if (RuntimeInformation.IsOSPlatform(OSPlatform.Linux))
            {
                return GetLinuxCpuName();
            }
            else if (RuntimeInformation.IsOSPlatform(OSPlatform.OSX))
            {
                return GetMacOsCpuName();
            }
        }
        catch (Exception ex)
        {
            _logger.LogDebug(ex, "Failed to get CPU name");
        }

        return "Unknown CPU";
    }

    private static string GetWindowsCpuName()
    {
        try
        {
            var startInfo = new ProcessStartInfo
            {
                FileName = "wmic",
                Arguments = "cpu get name",
                RedirectStandardOutput = true,
                UseShellExecute = false,
                CreateNoWindow = true
            };

            using var process = Process.Start(startInfo);
            if (process == null) return "Unknown CPU";

            var output = process.StandardOutput.ReadToEnd();
            process.WaitForExit(1000);

            var lines = output.Split('\n', StringSplitOptions.RemoveEmptyEntries);
            return lines.Length > 1 ? lines[1].Trim() : "Unknown CPU";
        }
        catch
        {
            return "Unknown CPU";
        }
    }

    private static string GetLinuxCpuName()
    {
        try
        {
            var cpuInfo = File.ReadAllText("/proc/cpuinfo");
            foreach (var line in cpuInfo.Split('\n'))
            {
                if (line.StartsWith("model name", StringComparison.OrdinalIgnoreCase))
                {
                    var parts = line.Split(':');
                    if (parts.Length > 1)
                    {
                        return parts[1].Trim();
                    }
                }
            }
        }
        catch
        {
            // Ignore
        }

        return "Unknown CPU";
    }

    private static string GetMacOsCpuName()
    {
        try
        {
            var startInfo = new ProcessStartInfo
            {
                FileName = "sysctl",
                Arguments = "-n machdep.cpu.brand_string",
                RedirectStandardOutput = true,
                UseShellExecute = false,
                CreateNoWindow = true
            };

            using var process = Process.Start(startInfo);
            if (process == null) return "Unknown CPU";

            var output = process.StandardOutput.ReadToEnd().Trim();
            process.WaitForExit(1000);

            return !string.IsNullOrEmpty(output) ? output : "Unknown CPU";
        }
        catch
        {
            return "Unknown CPU";
        }
    }

    private int GetPhysicalCoreCount()
    {
        try
        {
            if (RuntimeInformation.IsOSPlatform(OSPlatform.Windows))
            {
                // Simplified - Windows reports logical cores
                return Environment.ProcessorCount / 2;
            }
            else if (RuntimeInformation.IsOSPlatform(OSPlatform.Linux))
            {
                var cpuInfo = File.ReadAllText("/proc/cpuinfo");
                var coreIds = new HashSet<string>();

                foreach (var line in cpuInfo.Split('\n'))
                {
                    if (line.StartsWith("core id", StringComparison.OrdinalIgnoreCase))
                    {
                        var parts = line.Split(':');
                        if (parts.Length > 1)
                        {
                            coreIds.Add(parts[1].Trim());
                        }
                    }
                }

                return coreIds.Count > 0 ? coreIds.Count : Environment.ProcessorCount;
            }
            else if (RuntimeInformation.IsOSPlatform(OSPlatform.OSX))
            {
                var startInfo = new ProcessStartInfo
                {
                    FileName = "sysctl",
                    Arguments = "-n hw.physicalcpu",
                    RedirectStandardOutput = true,
                    UseShellExecute = false,
                    CreateNoWindow = true
                };

                using var process = Process.Start(startInfo);
                if (process != null)
                {
                    var output = process.StandardOutput.ReadToEnd().Trim();
                    process.WaitForExit(1000);

                    if (int.TryParse(output, out var cores))
                    {
                        return cores;
                    }
                }
            }
        }
        catch
        {
            // Ignore
        }

        return Environment.ProcessorCount;
    }

    private static bool IsAvx512Supported()
    {
        try
        {
            // Check for AVX-512 foundation
            return System.Runtime.Intrinsics.X86.Avx512F.IsSupported;
        }
        catch
        {
            return false;
        }
    }

    private static long GetCacheSize(int level)
    {
        // Platform-specific cache detection would go here
        // For now, return common defaults
        return level switch
        {
            1 => 32 * 1024, // 32 KB typical L1
            2 => 256 * 1024, // 256 KB typical L2
            3 => 8 * 1024 * 1024, // 8 MB typical L3
            _ => 0
        };
    }

    private static int GetMaxFrequency()
    {
        // Platform-specific frequency detection
        // Return 0 if unknown
        return 0;
    }
}

/// <summary>
/// CPU hardware information.
/// </summary>
public sealed class CpuInfo
{
    public string Name { get; init; } = string.Empty;
    public int CoreCount { get; init; }
    public int PhysicalCoreCount { get; init; }
    public string Architecture { get; init; } = string.Empty;
    public bool HasAvx { get; init; }
    public bool HasAvx2 { get; init; }
    public bool HasAvx512 { get; init; }
    public bool HasFma { get; init; }
    public bool HasSse42 { get; init; }
    public long CacheL1 { get; init; }
    public long CacheL2 { get; init; }
    public long CacheL3 { get; init; }
    public int MaxFrequencyMHz { get; init; }
}

/// <summary>
/// System memory information.
/// </summary>
public sealed class SystemMemoryInfo
{
    public long TotalPhysicalMemory { get; init; }
    public long AvailableMemory { get; init; }
    public long ProcessMemoryUsed { get; init; }
    public long ProcessPrivateMemory { get; init; }
    public long ProcessVirtualMemory { get; init; }
}
```

### File: `src/SeniorIntern.Services/Training/CpuMemoryManager.cs`

```csharp
using Microsoft.Extensions.Logging;

namespace SeniorIntern.Services.Training;

/// <summary>
/// Manages CPU memory for CPU-only training.
/// </summary>
public sealed class CpuMemoryManager : IDisposable
{
    private readonly ILogger<CpuMemoryManager> _logger;
    private long _peakMemoryUsed;
    private bool _disposed;

    public CpuMemoryManager(ILogger<CpuMemoryManager> logger)
    {
        _logger = logger;
    }

    /// <summary>
    /// Peak memory usage observed during this session.
    /// </summary>
    public long PeakMemoryUsed => _peakMemoryUsed;

    /// <summary>
    /// Get current memory usage.
    /// </summary>
    public (long Used, long Free, long Total) GetMemoryUsage()
    {
        try
        {
            var gcInfo = GC.GetGCMemoryInfo();
            var process = Process.GetCurrentProcess();

            var used = process.WorkingSet64;
            var total = gcInfo.TotalAvailableMemoryBytes;
            var free = total - used;

            if (used > _peakMemoryUsed)
            {
                _peakMemoryUsed = used;
            }

            return (used, free, total);
        }
        catch (Exception ex)
        {
            _logger.LogWarning(ex, "Failed to get memory usage");
            return (0, 0, 0);
        }
    }

    /// <summary>
    /// Trigger garbage collection to free memory.
    /// </summary>
    public void ClearCache()
    {
        try
        {
            GC.Collect(GC.MaxGeneration, GCCollectionMode.Aggressive, blocking: true);
            GC.WaitForPendingFinalizers();
            GC.Collect();

            _logger.LogDebug("CPU memory cache cleared via GC");
        }
        catch (Exception ex)
        {
            _logger.LogWarning(ex, "Failed to clear memory cache");
        }
    }

    /// <summary>
    /// Reset peak memory tracking.
    /// </summary>
    public void ResetPeakMemory()
    {
        _peakMemoryUsed = 0;
    }

    /// <summary>
    /// Attempt to recover from an out-of-memory condition.
    /// </summary>
    public bool TryRecoverFromOom()
    {
        _logger.LogWarning("Attempting to recover from CPU OOM");

        try
        {
            ClearCache();

            var (_, free, _) = GetMemoryUsage();
            _logger.LogInformation("OOM recovery freed {Memory:F2} GB", free / 1e9);

            return free > 100 * 1024 * 1024;
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Failed to recover from CPU OOM");
            return false;
        }
    }

    public void Dispose()
    {
        if (_disposed) return;
        _disposed = true;

        ClearCache();
    }
}
```

---

## v0.8.1g: Training Configuration Models

### Objective
Define comprehensive training configuration models with LoRA-specific settings, hyperparameters, and scheduling options.

### File: `src/SeniorIntern.Core/Training/TrainingConfiguration.cs`

```csharp
namespace SeniorIntern.Core.Training;

/// <summary>
/// Complete configuration for a LoRA training run.
/// </summary>
public sealed record TrainingConfiguration
{
    // ═══════════════════════════════════════════════════════════════════════
    // Hardware Configuration
    // ═══════════════════════════════════════════════════════════════════════

    /// <summary>
    /// Device to train on (e.g., "cuda:0", "mps", "cpu").
    /// </summary>
    public string Device { get; init; } = "cpu";

    /// <summary>
    /// Whether to use mixed precision (FP16/BF16) training.
    /// Significantly reduces memory usage and speeds up training on supported GPUs.
    /// </summary>
    public bool UseMixedPrecision { get; init; } = true;

    /// <summary>
    /// Mixed precision data type to use.
    /// </summary>
    public MixedPrecisionType MixedPrecisionType { get; init; } = MixedPrecisionType.Fp16;

    /// <summary>
    /// Whether to use gradient checkpointing to reduce memory usage.
    /// Trades compute time for memory - enables training larger models.
    /// </summary>
    public bool UseGradientCheckpointing { get; init; } = false;

    // ═══════════════════════════════════════════════════════════════════════
    // Training Hyperparameters
    // ═══════════════════════════════════════════════════════════════════════

    /// <summary>
    /// Number of training epochs.
    /// </summary>
    public int Epochs { get; init; } = 3;

    /// <summary>
    /// Batch size per device.
    /// </summary>
    public int BatchSize { get; init; } = 4;

    /// <summary>
    /// Number of gradient accumulation steps.
    /// Effective batch size = BatchSize * GradientAccumulationSteps.
    /// </summary>
    public int GradientAccumulationSteps { get; init; } = 1;

    /// <summary>
    /// Effective batch size (BatchSize * GradientAccumulationSteps).
    /// </summary>
    public int EffectiveBatchSize => BatchSize * GradientAccumulationSteps;

    /// <summary>
    /// Initial learning rate.
    /// </summary>
    public float LearningRate { get; init; } = 2e-4f;

    /// <summary>
    /// Weight decay for regularization.
    /// </summary>
    public float WeightDecay { get; init; } = 0.01f;

    /// <summary>
    /// Number of warmup steps for learning rate scheduler.
    /// </summary>
    public int WarmupSteps { get; init; } = 100;

    /// <summary>
    /// Warmup ratio (alternative to WarmupSteps).
    /// If > 0, overrides WarmupSteps.
    /// </summary>
    public float WarmupRatio { get; init; } = 0.0f;

    /// <summary>
    /// Learning rate scheduler type.
    /// </summary>
    public LrSchedulerType LrScheduler { get; init; } = LrSchedulerType.Cosine;

    /// <summary>
    /// Optimizer type.
    /// </summary>
    public OptimizerType Optimizer { get; init; } = OptimizerType.AdamW;

    /// <summary>
    /// Maximum gradient norm for clipping.
    /// </summary>
    public float MaxGradNorm { get; init; } = 1.0f;

    // ═══════════════════════════════════════════════════════════════════════
    // LoRA Configuration
    // ═══════════════════════════════════════════════════════════════════════

    /// <summary>
    /// LoRA rank (r). Higher ranks = more parameters = more capacity.
    /// Common values: 8, 16, 32, 64.
    /// </summary>
    public int LoraRank { get; init; } = 16;

    /// <summary>
    /// LoRA alpha scaling factor.
    /// Effective scaling = alpha / rank.
    /// </summary>
    public float LoraAlpha { get; init; } = 32f;

    /// <summary>
    /// LoRA dropout probability.
    /// </summary>
    public float LoraDropout { get; init; } = 0.05f;

    /// <summary>
    /// Target modules to apply LoRA to.
    /// </summary>
    public IReadOnlyList<string> TargetModules { get; init; } = new[]
    {
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    };

    /// <summary>
    /// Whether to apply LoRA to the embedding layer.
    /// </summary>
    public bool LoraOnEmbedding { get; init; } = false;

    /// <summary>
    /// Whether to apply LoRA to the output (lm_head) layer.
    /// </summary>
    public bool LoraOnOutput { get; init; } = false;

    /// <summary>
    /// Whether to use RSLoRA (Rank-Stabilized LoRA) scaling.
    /// Uses sqrt(rank) instead of rank for alpha scaling.
    /// </summary>
    public bool UseRsLora { get; init; } = false;

    // ═══════════════════════════════════════════════════════════════════════
    // Data Configuration
    // ═══════════════════════════════════════════════════════════════════════

    /// <summary>
    /// Maximum sequence length for training.
    /// </summary>
    public int MaxSequenceLength { get; init; } = 2048;

    /// <summary>
    /// Whether to pack multiple short sequences into one.
    /// Improves efficiency for datasets with variable-length examples.
    /// </summary>
    public bool PackSequences { get; init; } = true;

    /// <summary>
    /// Fraction of data to use for validation.
    /// </summary>
    public float ValidationSplit { get; init; } = 0.1f;

    /// <summary>
    /// Random seed for reproducibility.
    /// Null for random initialization.
    /// </summary>
    public int? Seed { get; init; }

    // ═══════════════════════════════════════════════════════════════════════
    // Checkpointing & Saving
    // ═══════════════════════════════════════════════════════════════════════

    /// <summary>
    /// Save checkpoint every N steps.
    /// </summary>
    public int SaveEveryNSteps { get; init; } = 500;

    /// <summary>
    /// Maximum number of checkpoints to keep.
    /// Older checkpoints are deleted when limit is reached.
    /// </summary>
    public int MaxCheckpoints { get; init; } = 3;

    /// <summary>
    /// Directory for checkpoint storage.
    /// </summary>
    public string CheckpointDir { get; init; } = string.Empty;

    /// <summary>
    /// Whether to save the best model based on validation loss.
    /// </summary>
    public bool SaveBestModel { get; init; } = true;

    // ═══════════════════════════════════════════════════════════════════════
    // Evaluation
    // ═══════════════════════════════════════════════════════════════════════

    /// <summary>
    /// Evaluate on validation set every N steps.
    /// </summary>
    public int EvalEveryNSteps { get; init; } = 100;

    /// <summary>
    /// Log training metrics every N steps.
    /// </summary>
    public int LogEveryNSteps { get; init; } = 10;

    // ═══════════════════════════════════════════════════════════════════════
    // Advanced Options
    // ═══════════════════════════════════════════════════════════════════════

    /// <summary>
    /// Whether to use 8-bit Adam optimizer (experimental).
    /// Reduces optimizer memory footprint.
    /// </summary>
    public bool Use8BitAdam { get; init; } = false;

    /// <summary>
    /// Label smoothing factor for cross-entropy loss.
    /// </summary>
    public float LabelSmoothing { get; init; } = 0.0f;

    /// <summary>
    /// Minimum learning rate (for schedulers that don't go to zero).
    /// </summary>
    public float MinLearningRate { get; init; } = 0.0f;

    /// <summary>
    /// Number of dataloader workers.
    /// </summary>
    public int DataloaderNumWorkers { get; init; } = 0;

    /// <summary>
    /// Whether to pin memory for faster GPU transfer.
    /// </summary>
    public bool PinMemory { get; init; } = true;
}

/// <summary>
/// Mixed precision data types.
/// </summary>
public enum MixedPrecisionType
{
    /// <summary>
    /// 16-bit floating point (IEEE half precision).
    /// </summary>
    Fp16,

    /// <summary>
    /// Brain floating point 16 (better range than FP16).
    /// Requires Ampere+ GPU.
    /// </summary>
    Bf16
}

/// <summary>
/// Learning rate scheduler types.
/// </summary>
public enum LrSchedulerType
{
    /// <summary>
    /// Constant learning rate.
    /// </summary>
    Constant,

    /// <summary>
    /// Linear decay to zero.
    /// </summary>
    Linear,

    /// <summary>
    /// Cosine annealing.
    /// </summary>
    Cosine,

    /// <summary>
    /// Cosine with restarts.
    /// </summary>
    CosineWithRestarts,

    /// <summary>
    /// Polynomial decay.
    /// </summary>
    Polynomial,

    /// <summary>
    /// Inverse square root decay.
    /// </summary>
    InverseSqrt
}

/// <summary>
/// Optimizer types.
/// </summary>
public enum OptimizerType
{
    /// <summary>
    /// Adam with decoupled weight decay.
    /// </summary>
    AdamW,

    /// <summary>
    /// Stochastic gradient descent with momentum.
    /// </summary>
    Sgd,

    /// <summary>
    /// Adaptive gradient algorithm.
    /// </summary>
    Adagrad,

    /// <summary>
    /// 8-bit Adam (memory efficient).
    /// </summary>
    Adam8bit
}
```

### File: `src/SeniorIntern.Core/Training/TrainingConfigurationBuilder.cs`

```csharp
namespace SeniorIntern.Core.Training;

/// <summary>
/// Builder for creating training configurations with sensible defaults.
/// </summary>
public sealed class TrainingConfigurationBuilder
{
    private TrainingConfiguration _config = new();

    /// <summary>
    /// Start with default configuration.
    /// </summary>
    public static TrainingConfigurationBuilder Default() => new();

    /// <summary>
    /// Start with configuration for small/edge devices.
    /// </summary>
    public static TrainingConfigurationBuilder ForSmallGpu()
    {
        return new TrainingConfigurationBuilder()
            .WithBatchSize(1)
            .WithGradientAccumulationSteps(8)
            .WithGradientCheckpointing(true)
            .WithMixedPrecision(true)
            .WithLoraRank(8);
    }

    /// <summary>
    /// Start with configuration for mid-range GPUs (8-12GB).
    /// </summary>
    public static TrainingConfigurationBuilder ForMidRangeGpu()
    {
        return new TrainingConfigurationBuilder()
            .WithBatchSize(2)
            .WithGradientAccumulationSteps(4)
            .WithGradientCheckpointing(false)
            .WithMixedPrecision(true)
            .WithLoraRank(16);
    }

    /// <summary>
    /// Start with configuration for high-end GPUs (16GB+).
    /// </summary>
    public static TrainingConfigurationBuilder ForHighEndGpu()
    {
        return new TrainingConfigurationBuilder()
            .WithBatchSize(4)
            .WithGradientAccumulationSteps(2)
            .WithGradientCheckpointing(false)
            .WithMixedPrecision(true)
            .WithLoraRank(32);
    }

    /// <summary>
    /// Start with configuration for CPU-only training.
    /// </summary>
    public static TrainingConfigurationBuilder ForCpu()
    {
        return new TrainingConfigurationBuilder()
            .WithDevice("cpu")
            .WithBatchSize(1)
            .WithGradientAccumulationSteps(16)
            .WithGradientCheckpointing(true)
            .WithMixedPrecision(false)
            .WithLoraRank(8);
    }

    public TrainingConfigurationBuilder WithDevice(string device)
    {
        _config = _config with { Device = device };
        return this;
    }

    public TrainingConfigurationBuilder WithMixedPrecision(bool enabled, MixedPrecisionType type = MixedPrecisionType.Fp16)
    {
        _config = _config with
        {
            UseMixedPrecision = enabled,
            MixedPrecisionType = type
        };
        return this;
    }

    public TrainingConfigurationBuilder WithGradientCheckpointing(bool enabled)
    {
        _config = _config with { UseGradientCheckpointing = enabled };
        return this;
    }

    public TrainingConfigurationBuilder WithEpochs(int epochs)
    {
        _config = _config with { Epochs = epochs };
        return this;
    }

    public TrainingConfigurationBuilder WithBatchSize(int batchSize)
    {
        _config = _config with { BatchSize = batchSize };
        return this;
    }

    public TrainingConfigurationBuilder WithGradientAccumulationSteps(int steps)
    {
        _config = _config with { GradientAccumulationSteps = steps };
        return this;
    }

    public TrainingConfigurationBuilder WithLearningRate(float lr)
    {
        _config = _config with { LearningRate = lr };
        return this;
    }

    public TrainingConfigurationBuilder WithWarmupSteps(int steps)
    {
        _config = _config with { WarmupSteps = steps };
        return this;
    }

    public TrainingConfigurationBuilder WithScheduler(LrSchedulerType scheduler)
    {
        _config = _config with { LrScheduler = scheduler };
        return this;
    }

    public TrainingConfigurationBuilder WithLoraRank(int rank)
    {
        _config = _config with { LoraRank = rank };
        return this;
    }

    public TrainingConfigurationBuilder WithLoraAlpha(float alpha)
    {
        _config = _config with { LoraAlpha = alpha };
        return this;
    }

    public TrainingConfigurationBuilder WithLoraDropout(float dropout)
    {
        _config = _config with { LoraDropout = dropout };
        return this;
    }

    public TrainingConfigurationBuilder WithTargetModules(params string[] modules)
    {
        _config = _config with { TargetModules = modules };
        return this;
    }

    public TrainingConfigurationBuilder WithMaxSequenceLength(int length)
    {
        _config = _config with { MaxSequenceLength = length };
        return this;
    }

    public TrainingConfigurationBuilder WithSeed(int seed)
    {
        _config = _config with { Seed = seed };
        return this;
    }

    public TrainingConfigurationBuilder WithCheckpointDir(string dir)
    {
        _config = _config with { CheckpointDir = dir };
        return this;
    }

    public TrainingConfigurationBuilder WithSaveEveryNSteps(int steps)
    {
        _config = _config with { SaveEveryNSteps = steps };
        return this;
    }

    public TrainingConfigurationBuilder WithEvalEveryNSteps(int steps)
    {
        _config = _config with { EvalEveryNSteps = steps };
        return this;
    }

    /// <summary>
    /// Build the final configuration.
    /// </summary>
    public TrainingConfiguration Build() => _config;
}
```

### File: `src/SeniorIntern.Core/Training/LoraConfiguration.cs`

```csharp
namespace SeniorIntern.Core.Training;

/// <summary>
/// Standalone LoRA configuration for adapter creation.
/// </summary>
public sealed record LoraConfiguration
{
    /// <summary>
    /// LoRA rank.
    /// </summary>
    public int Rank { get; init; } = 16;

    /// <summary>
    /// LoRA alpha scaling factor.
    /// </summary>
    public float Alpha { get; init; } = 32f;

    /// <summary>
    /// Dropout probability.
    /// </summary>
    public float Dropout { get; init; } = 0.05f;

    /// <summary>
    /// Target modules to apply LoRA.
    /// </summary>
    public IReadOnlyList<string> TargetModules { get; init; } = new[]
    {
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    };

    /// <summary>
    /// Whether to use bias terms in LoRA layers.
    /// </summary>
    public LoraBiasMode BiasMode { get; init; } = LoraBiasMode.None;

    /// <summary>
    /// Whether to use RSLoRA scaling.
    /// </summary>
    public bool UseRsLora { get; init; } = false;

    /// <summary>
    /// Initialization standard deviation.
    /// </summary>
    public float InitStd { get; init; } = 0.02f;

    /// <summary>
    /// Calculate the effective scaling factor.
    /// </summary>
    public float Scaling => UseRsLora
        ? Alpha / MathF.Sqrt(Rank)
        : Alpha / Rank;

    /// <summary>
    /// Estimate number of trainable parameters.
    /// </summary>
    /// <param name="hiddenSize">Model hidden size.</param>
    /// <param name="numLayers">Number of transformer layers.</param>
    public long EstimateTrainableParameters(int hiddenSize, int numLayers)
    {
        // Each LoRA layer: 2 * rank * dim parameters (A and B matrices)
        var paramsPerModule = 2L * Rank * hiddenSize;
        var modulesPerLayer = TargetModules.Count;

        return paramsPerModule * modulesPerLayer * numLayers;
    }

    /// <summary>
    /// Get default LoRA configuration.
    /// </summary>
    public static LoraConfiguration Default => new();

    /// <summary>
    /// Get aggressive memory-saving configuration.
    /// </summary>
    public static LoraConfiguration LowMemory => new()
    {
        Rank = 4,
        Alpha = 8f,
        Dropout = 0.1f,
        TargetModules = new[] { "q_proj", "v_proj" }
    };

    /// <summary>
    /// Get high-quality training configuration.
    /// </summary>
    public static LoraConfiguration HighQuality => new()
    {
        Rank = 64,
        Alpha = 128f,
        Dropout = 0.05f,
        UseRsLora = true
    };
}

/// <summary>
/// Bias mode for LoRA layers.
/// </summary>
public enum LoraBiasMode
{
    /// <summary>
    /// No bias terms.
    /// </summary>
    None,

    /// <summary>
    /// Only LoRA layers have bias.
    /// </summary>
    LoraOnly,

    /// <summary>
    /// All layers (including original) have trainable bias.
    /// </summary>
    All
}
```

### File: `src/SeniorIntern.Core/Training/AdamWConfiguration.cs`

```csharp
namespace SeniorIntern.Core.Training;

/// <summary>
/// Configuration for AdamW optimizer.
/// </summary>
public sealed record AdamWConfiguration
{
    /// <summary>
    /// Learning rate.
    /// </summary>
    public float LearningRate { get; init; } = 2e-4f;

    /// <summary>
    /// Beta1 (first moment decay).
    /// </summary>
    public float Beta1 { get; init; } = 0.9f;

    /// <summary>
    /// Beta2 (second moment decay).
    /// </summary>
    public float Beta2 { get; init; } = 0.999f;

    /// <summary>
    /// Epsilon for numerical stability.
    /// </summary>
    public float Epsilon { get; init; } = 1e-8f;

    /// <summary>
    /// Weight decay coefficient.
    /// </summary>
    public float WeightDecay { get; init; } = 0.01f;

    /// <summary>
    /// Whether to use AMSGrad variant.
    /// </summary>
    public bool Amsgrad { get; init; } = false;

    /// <summary>
    /// Whether to fuse operations for better performance.
    /// </summary>
    public bool Fused { get; init; } = true;
}
```

---

## v0.8.1h: Configuration Validation & Recommendations

### Objective
Implement configuration validation logic and provide automated recommendations based on detected hardware.

### File: `src/SeniorIntern.Services/Training/TrainingConfigurationValidator.cs`

```csharp
using SeniorIntern.Core.Training;

namespace SeniorIntern.Services.Training;

/// <summary>
/// Validates training configurations and provides recommendations.
/// </summary>
public sealed class TrainingConfigurationValidator
{
    /// <summary>
    /// Validate a training configuration.
    /// </summary>
    public ValidationResult Validate(TrainingConfiguration config)
    {
        var errors = new List<string>();
        var warnings = new List<string>();

        // Validate epochs
        if (config.Epochs <= 0)
            errors.Add("Epochs must be greater than 0");
        else if (config.Epochs > 100)
            warnings.Add("Training for more than 100 epochs may lead to overfitting");

        // Validate batch size
        if (config.BatchSize <= 0)
            errors.Add("Batch size must be greater than 0");
        if (config.BatchSize > 64)
            warnings.Add("Very large batch sizes may not fit in memory");

        // Validate gradient accumulation
        if (config.GradientAccumulationSteps <= 0)
            errors.Add("Gradient accumulation steps must be greater than 0");

        // Validate learning rate
        if (config.LearningRate <= 0)
            errors.Add("Learning rate must be greater than 0");
        else if (config.LearningRate > 1e-2f)
            warnings.Add("Learning rate > 1e-2 is very high for fine-tuning");
        else if (config.LearningRate < 1e-6f)
            warnings.Add("Learning rate < 1e-6 is very low, training may be slow");

        // Validate LoRA configuration
        if (config.LoraRank <= 0)
            errors.Add("LoRA rank must be greater than 0");
        else if (config.LoraRank > 256)
            warnings.Add("LoRA rank > 256 provides diminishing returns");

        if (config.LoraAlpha <= 0)
            errors.Add("LoRA alpha must be greater than 0");

        if (config.LoraDropout < 0 || config.LoraDropout >= 1)
            errors.Add("LoRA dropout must be between 0 and 1");

        if (config.TargetModules.Count == 0)
            errors.Add("At least one target module must be specified");

        // Validate sequence length
        if (config.MaxSequenceLength <= 0)
            errors.Add("Max sequence length must be greater than 0");
        else if (config.MaxSequenceLength > 8192)
            warnings.Add("Very long sequences require significant memory");

        // Validate mixed precision settings
        if (config.UseMixedPrecision && config.Device == "cpu")
            warnings.Add("Mixed precision on CPU may not provide benefits");

        if (config.MixedPrecisionType == MixedPrecisionType.Bf16 &&
            !config.Device.StartsWith("cuda"))
            warnings.Add("BF16 is only well-supported on CUDA devices with Ampere+ GPUs");

        // Validate checkpointing
        if (config.SaveEveryNSteps <= 0)
            warnings.Add("Checkpoint saving is disabled (SaveEveryNSteps = 0)");

        if (config.MaxCheckpoints <= 0)
            warnings.Add("No checkpoint limit set, disk space may fill up");

        // Validate eval frequency
        if (config.EvalEveryNSteps <= 0)
            warnings.Add("Validation is disabled (EvalEveryNSteps = 0)");

        // Validate weight decay
        if (config.WeightDecay < 0)
            errors.Add("Weight decay cannot be negative");
        else if (config.WeightDecay > 0.1f)
            warnings.Add("Weight decay > 0.1 is unusually high");

        // Check for common misconfigurations
        if (config.BatchSize == 1 && config.GradientAccumulationSteps == 1)
            warnings.Add("Effective batch size of 1 may lead to unstable training");

        if (config.ValidationSplit >= 1.0f || config.ValidationSplit < 0)
            errors.Add("Validation split must be between 0 and 1");

        return new ValidationResult
        {
            IsValid = errors.Count == 0,
            Errors = errors,
            Warnings = warnings
        };
    }

    /// <summary>
    /// Fix common issues in a configuration.
    /// </summary>
    public TrainingConfiguration AutoFix(TrainingConfiguration config)
    {
        var fixed = config;

        // Clamp values to valid ranges
        if (fixed.Epochs <= 0)
            fixed = fixed with { Epochs = 3 };

        if (fixed.BatchSize <= 0)
            fixed = fixed with { BatchSize = 1 };

        if (fixed.GradientAccumulationSteps <= 0)
            fixed = fixed with { GradientAccumulationSteps = 1 };

        if (fixed.LearningRate <= 0)
            fixed = fixed with { LearningRate = 2e-4f };

        if (fixed.LoraRank <= 0)
            fixed = fixed with { LoraRank = 16 };

        if (fixed.LoraAlpha <= 0)
            fixed = fixed with { LoraAlpha = fixed.LoraRank * 2 };

        if (fixed.LoraDropout < 0 || fixed.LoraDropout >= 1)
            fixed = fixed with { LoraDropout = 0.05f };

        if (fixed.MaxSequenceLength <= 0)
            fixed = fixed with { MaxSequenceLength = 2048 };

        if (fixed.WeightDecay < 0)
            fixed = fixed with { WeightDecay = 0.01f };

        if (fixed.ValidationSplit < 0 || fixed.ValidationSplit >= 1)
            fixed = fixed with { ValidationSplit = 0.1f };

        return fixed;
    }
}

/// <summary>
/// Result of configuration validation.
/// </summary>
public sealed class ValidationResult
{
    public bool IsValid { get; init; }
    public IReadOnlyList<string> Errors { get; init; } = Array.Empty<string>();
    public IReadOnlyList<string> Warnings { get; init; } = Array.Empty<string>();
}
```

### File: `src/SeniorIntern.Services/Training/HardwareDetectionService.cs`

```csharp
using Microsoft.Extensions.Logging;
using SeniorIntern.Core.Interfaces;
using SeniorIntern.Core.Training;

namespace SeniorIntern.Services.Training;

/// <summary>
/// Main hardware detection service that coordinates platform-specific detectors.
/// </summary>
public sealed class HardwareDetectionService : IHardwareDetectionService
{
    private readonly ILogger<HardwareDetectionService> _logger;
    private readonly CudaHardwareDetector _cudaDetector;
    private readonly MetalHardwareDetector _metalDetector;
    private readonly CpuHardwareDetector _cpuDetector;

    private TrainingHardwareInfo? _cachedInfo;
    private DateTime _cacheTime;
    private readonly TimeSpan _cacheDuration = TimeSpan.FromMinutes(5);

    public event EventHandler<HardwareStateChangedEventArgs>? HardwareStateChanged;

    public HardwareDetectionService(
        ILogger<HardwareDetectionService> logger,
        CudaHardwareDetector cudaDetector,
        MetalHardwareDetector metalDetector,
        CpuHardwareDetector cpuDetector)
    {
        _logger = logger;
        _cudaDetector = cudaDetector;
        _metalDetector = metalDetector;
        _cpuDetector = cpuDetector;
    }

    public int CurrentGpuDeviceIndex { get; private set; } = -1;

    public async Task<TrainingHardwareInfo> DetectHardwareAsync(
        bool refresh = false,
        CancellationToken ct = default)
    {
        if (!refresh && _cachedInfo != null &&
            DateTime.UtcNow - _cacheTime < _cacheDuration)
        {
            return _cachedInfo;
        }

        _logger.LogInformation("Detecting training hardware...");

        var gpuDevices = new List<GpuDeviceInfo>();
        var cpuInfo = _cpuDetector.GetCpuInfo();
        var memoryInfo = _cpuDetector.GetMemoryInfo();

        // Check for CUDA
        var cudaAvailable = _cudaDetector.IsCudaAvailable();
        string? cudaDriver = null;
        string? cudaRuntime = null;

        if (cudaAvailable)
        {
            _logger.LogDebug("CUDA is available");
            cudaDriver = _cudaDetector.GetDriverVersion();
            cudaRuntime = _cudaDetector.GetRuntimeVersion();

            var cudaDevices = await _cudaDetector.DetectDevicesAsync(ct);
            gpuDevices.AddRange(cudaDevices);
        }

        // Check for Metal
        var metalAvailable = _metalDetector.IsMpsAvailable();
        var isAppleSilicon = _metalDetector.IsAppleSilicon();

        if (metalAvailable)
        {
            _logger.LogDebug("Metal is available");
            var metalDevices = await _metalDetector.DetectDevicesAsync(ct);
            gpuDevices.AddRange(metalDevices);
        }

        // Determine recommended configuration
        var recommendedDevice = DetermineRecommendedDevice(gpuDevices, cudaAvailable, metalAvailable);
        var recommendedBatchSize = CalculateRecommendedBatchSize(gpuDevices);
        var supportsMixedPrecision = DeterminesMixedPrecisionSupport(gpuDevices, cudaAvailable);
        var recommendGradientCheckpointing = ShouldRecommendGradientCheckpointing(gpuDevices);

        if (recommendedDevice.StartsWith("cuda"))
        {
            var parts = recommendedDevice.Split(':');
            CurrentGpuDeviceIndex = parts.Length > 1 ? int.Parse(parts[1]) : 0;
        }
        else
        {
            CurrentGpuDeviceIndex = -1;
        }

        _cachedInfo = new TrainingHardwareInfo
        {
            CudaAvailable = cudaAvailable,
            CudaDriverVersion = cudaDriver,
            CudaRuntimeVersion = cudaRuntime,
            CudaDeviceCount = gpuDevices.Count(g => g.Backend == GpuBackend.Cuda),
            MetalAvailable = metalAvailable,
            IsAppleSilicon = isAppleSilicon,
            GpuDevices = gpuDevices,
            TotalSystemMemory = memoryInfo.TotalPhysicalMemory,
            AvailableSystemMemory = memoryInfo.AvailableMemory,
            CpuCoreCount = cpuInfo.CoreCount,
            CpuName = cpuInfo.Name,
            RecommendedDevice = recommendedDevice,
            RecommendedBatchSize = recommendedBatchSize,
            SupportsMixedPrecision = supportsMixedPrecision,
            RecommendGradientCheckpointing = recommendGradientCheckpointing,
            DetectedAt = DateTime.UtcNow
        };

        _cacheTime = DateTime.UtcNow;

        _logger.LogInformation(
            "Hardware detection complete: {GpuCount} GPU(s), Recommended: {Device}, Batch: {Batch}",
            gpuDevices.Count,
            recommendedDevice,
            recommendedBatchSize);

        return _cachedInfo;
    }

    public async Task<TrainingCapabilityResult> CheckTrainingCapabilityAsync(
        TrainingRequirements requirements,
        CancellationToken ct = default)
    {
        var hardware = await DetectHardwareAsync(false, ct);
        var warnings = new List<string>();
        var suggestions = new List<string>();

        // Check GPU requirements
        if (requirements.RequiresGpu && hardware.GpuDevices.Count == 0)
        {
            return new TrainingCapabilityResult
            {
                CanTrain = false,
                Reason = "GPU required but none available",
                Hardware = hardware
            };
        }

        // Check VRAM
        var availableVram = hardware.TotalFreeGpuMemory;
        if (availableVram < requirements.MinimumVramBytes && requirements.RequiresGpu)
        {
            return new TrainingCapabilityResult
            {
                CanTrain = false,
                Reason = $"Insufficient VRAM: {FormatBytes(availableVram)} available, " +
                         $"{FormatBytes(requirements.MinimumVramBytes)} required",
                Hardware = hardware
            };
        }

        if (availableVram < requirements.MinimumVramBytes)
        {
            warnings.Add($"GPU has only {FormatBytes(availableVram)} free VRAM, " +
                        $"training may fall back to CPU or require optimization");
            suggestions.Add("Enable gradient checkpointing");
            suggestions.Add("Reduce batch size");
            suggestions.Add("Use lower LoRA rank");
        }

        // Check RAM
        if (hardware.AvailableSystemMemory < requirements.MinimumRamBytes)
        {
            warnings.Add($"Low system RAM: {FormatBytes(hardware.AvailableSystemMemory)} available");
            suggestions.Add("Close other applications to free memory");
        }

        // Check compute capability
        if (requirements.MinimumComputeCapability > 0)
        {
            var bestCapability = hardware.GpuDevices
                .Where(g => g.Backend == GpuBackend.Cuda)
                .Select(g => g.ComputeCapabilityMajor * 10 + g.ComputeCapabilityMinor)
                .DefaultIfEmpty(0)
                .Max();

            if (bestCapability < requirements.MinimumComputeCapability)
            {
                warnings.Add($"GPU compute capability {bestCapability / 10}.{bestCapability % 10} " +
                            $"is below recommended {requirements.MinimumComputeCapability / 10}." +
                            $"{requirements.MinimumComputeCapability % 10}");
            }
        }

        // Determine speed category
        var speedCategory = DetermineSpeedCategory(hardware, requirements);
        var timeMultiplier = EstimateTimeMultiplier(hardware, speedCategory);

        if (speedCategory == TrainingSpeedCategory.VerySlow)
        {
            warnings.Add("Training will be very slow on this hardware");
        }
        else if (speedCategory == TrainingSpeedCategory.Slow)
        {
            warnings.Add("Training will be slower than optimal");
        }

        return new TrainingCapabilityResult
        {
            CanTrain = true,
            Warnings = warnings,
            Suggestions = suggestions,
            Hardware = hardware,
            SpeedCategory = speedCategory,
            EstimatedTimeMultiplier = timeMultiplier
        };
    }

    public TrainingConfiguration GetRecommendedConfiguration(
        TrainingHardwareInfo hardware,
        long modelSizeBytes,
        int datasetSize)
    {
        var builder = hardware.RecommendedDevice switch
        {
            "cpu" => TrainingConfigurationBuilder.ForCpu(),
            "mps" => TrainingConfigurationBuilder.ForMidRangeGpu(),
            _ when hardware.TotalFreeGpuMemory >= 16L * 1024 * 1024 * 1024 =>
                TrainingConfigurationBuilder.ForHighEndGpu(),
            _ when hardware.TotalFreeGpuMemory >= 8L * 1024 * 1024 * 1024 =>
                TrainingConfigurationBuilder.ForMidRangeGpu(),
            _ => TrainingConfigurationBuilder.ForSmallGpu()
        };

        builder.WithDevice(hardware.RecommendedDevice);

        // Adjust for model size
        if (modelSizeBytes > 10L * 1024 * 1024 * 1024) // > 10GB
        {
            builder.WithGradientCheckpointing(true)
                   .WithBatchSize(1)
                   .WithGradientAccumulationSteps(8);
        }

        // Adjust for dataset size
        var epochs = datasetSize < 100 ? 10
                   : datasetSize < 1000 ? 5
                   : datasetSize < 10000 ? 3
                   : 1;
        builder.WithEpochs(epochs);

        // Adjust warmup
        var warmupSteps = Math.Max(10, Math.Min(datasetSize / 10, 500));
        builder.WithWarmupSteps(warmupSteps);

        // Mixed precision
        if (hardware.SupportsMixedPrecision)
        {
            var mpType = hardware.GpuDevices.Any(g => g.SupportsBf16)
                ? MixedPrecisionType.Bf16
                : MixedPrecisionType.Fp16;
            builder.WithMixedPrecision(true, mpType);
        }

        return builder.Build();
    }

    public MemoryEstimate EstimateMemoryRequirements(
        TrainingConfiguration config,
        long modelSizeBytes)
    {
        var availableGpuMemory = _cachedInfo?.TotalFreeGpuMemory ?? 0;

        return MemoryEstimator.EstimateLoraTrainingMemory(
            modelSizeBytes,
            config,
            config.MaxSequenceLength,
            availableGpuMemory);
    }

    private static string DetermineRecommendedDevice(
        IReadOnlyList<GpuDeviceInfo> gpuDevices,
        bool cudaAvailable,
        bool metalAvailable)
    {
        if (cudaAvailable && gpuDevices.Any(g => g.Backend == GpuBackend.Cuda))
        {
            var best = gpuDevices
                .Where(g => g.Backend == GpuBackend.Cuda)
                .OrderByDescending(g => g.FreeMemory)
                .First();
            return $"cuda:{best.DeviceIndex}";
        }

        if (metalAvailable && gpuDevices.Any(g => g.Backend == GpuBackend.Metal))
        {
            return "mps";
        }

        return "cpu";
    }

    private static int CalculateRecommendedBatchSize(IReadOnlyList<GpuDeviceInfo> gpuDevices)
    {
        if (gpuDevices.Count == 0) return 1;

        var maxFreeMemory = gpuDevices.Max(g => g.FreeMemory);

        // Rough heuristic: 8GB = batch 4, 16GB = batch 8, etc.
        var batchSize = (int)(maxFreeMemory / (2L * 1024 * 1024 * 1024));
        return Math.Max(1, Math.Min(batchSize, 16));
    }

    private static bool DeterminesMixedPrecisionSupport(
        IReadOnlyList<GpuDeviceInfo> gpuDevices,
        bool cudaAvailable)
    {
        if (!cudaAvailable) return false;

        // FP16 requires compute capability 5.3+
        return gpuDevices.Any(g =>
            g.Backend == GpuBackend.Cuda &&
            (g.ComputeCapabilityMajor * 10 + g.ComputeCapabilityMinor) >= 53);
    }

    private static bool ShouldRecommendGradientCheckpointing(IReadOnlyList<GpuDeviceInfo> gpuDevices)
    {
        if (gpuDevices.Count == 0) return true;

        // Recommend checkpointing if < 12GB free
        return gpuDevices.Max(g => g.FreeMemory) < 12L * 1024 * 1024 * 1024;
    }

    private static TrainingSpeedCategory DetermineSpeedCategory(
        TrainingHardwareInfo hardware,
        TrainingRequirements requirements)
    {
        if (hardware.GpuDevices.Count == 0)
        {
            return TrainingSpeedCategory.VerySlow;
        }

        var bestGpu = hardware.GpuDevices
            .OrderByDescending(g => g.FreeMemory)
            .First();

        // Score based on GPU capabilities
        var score = 0;

        // Memory score
        if (bestGpu.FreeMemory >= 24L * 1024 * 1024 * 1024) score += 3;
        else if (bestGpu.FreeMemory >= 12L * 1024 * 1024 * 1024) score += 2;
        else if (bestGpu.FreeMemory >= 8L * 1024 * 1024 * 1024) score += 1;

        // Compute capability score
        var cc = bestGpu.ComputeCapabilityMajor;
        if (cc >= 9) score += 3; // Ada Lovelace
        else if (cc >= 8) score += 2; // Ampere
        else if (cc >= 7) score += 1; // Volta/Turing

        // Tensor cores
        if (bestGpu.HasTensorCores) score += 1;

        return score switch
        {
            >= 6 => TrainingSpeedCategory.VeryFast,
            >= 4 => TrainingSpeedCategory.Fast,
            >= 2 => TrainingSpeedCategory.Normal,
            >= 1 => TrainingSpeedCategory.Slow,
            _ => TrainingSpeedCategory.VerySlow
        };
    }

    private static float EstimateTimeMultiplier(
        TrainingHardwareInfo hardware,
        TrainingSpeedCategory speedCategory)
    {
        return speedCategory switch
        {
            TrainingSpeedCategory.VeryFast => 0.5f,
            TrainingSpeedCategory.Fast => 1.0f,
            TrainingSpeedCategory.Normal => 1.5f,
            TrainingSpeedCategory.Slow => 3.0f,
            TrainingSpeedCategory.VerySlow => 10.0f,
            _ => 1.0f
        };
    }

    private static string FormatBytes(long bytes)
    {
        if (bytes < 1024) return $"{bytes} B";
        if (bytes < 1024 * 1024) return $"{bytes / 1024.0:F1} KB";
        if (bytes < 1024 * 1024 * 1024) return $"{bytes / (1024.0 * 1024):F1} MB";
        return $"{bytes / (1024.0 * 1024 * 1024):F2} GB";
    }
}
```

### File: `src/SeniorIntern.Services/Training/ConfigurationRecommender.cs`

```csharp
using SeniorIntern.Core.Training;

namespace SeniorIntern.Services.Training;

/// <summary>
/// Provides training configuration recommendations based on hardware and use case.
/// </summary>
public sealed class ConfigurationRecommender
{
    /// <summary>
    /// Get recommended configuration for a specific training scenario.
    /// </summary>
    public TrainingConfiguration RecommendForScenario(
        TrainingScenario scenario,
        TrainingHardwareInfo hardware,
        long modelSizeBytes)
    {
        var baseConfig = scenario switch
        {
            TrainingScenario.QuickExperiment => CreateQuickExperimentConfig(),
            TrainingScenario.FullTraining => CreateFullTrainingConfig(),
            TrainingScenario.MemoryConstrained => CreateMemoryConstrainedConfig(),
            TrainingScenario.HighQuality => CreateHighQualityConfig(),
            _ => CreateDefaultConfig()
        };

        return OptimizeForHardware(baseConfig, hardware, modelSizeBytes);
    }

    private static TrainingConfiguration CreateQuickExperimentConfig()
    {
        return TrainingConfigurationBuilder.Default()
            .WithEpochs(1)
            .WithLoraRank(8)
            .WithEvalEveryNSteps(50)
            .WithSaveEveryNSteps(0) // Don't save for quick experiments
            .Build();
    }

    private static TrainingConfiguration CreateFullTrainingConfig()
    {
        return TrainingConfigurationBuilder.Default()
            .WithEpochs(3)
            .WithLoraRank(16)
            .WithLearningRate(2e-4f)
            .WithWarmupSteps(100)
            .Build();
    }

    private static TrainingConfiguration CreateMemoryConstrainedConfig()
    {
        return TrainingConfigurationBuilder.ForSmallGpu()
            .WithGradientCheckpointing(true)
            .WithBatchSize(1)
            .WithGradientAccumulationSteps(16)
            .WithLoraRank(4)
            .Build();
    }

    private static TrainingConfiguration CreateHighQualityConfig()
    {
        return TrainingConfigurationBuilder.ForHighEndGpu()
            .WithEpochs(5)
            .WithLoraRank(64)
            .WithLoraAlpha(128)
            .WithLearningRate(1e-4f)
            .WithEvalEveryNSteps(50)
            .Build();
    }

    private static TrainingConfiguration CreateDefaultConfig()
    {
        return TrainingConfigurationBuilder.Default().Build();
    }

    private static TrainingConfiguration OptimizeForHardware(
        TrainingConfiguration config,
        TrainingHardwareInfo hardware,
        long modelSizeBytes)
    {
        var optimized = config;

        // Set device
        optimized = optimized with { Device = hardware.RecommendedDevice };

        // Enable mixed precision if supported
        if (hardware.SupportsMixedPrecision)
        {
            var mpType = hardware.GpuDevices.Any(g => g.SupportsBf16)
                ? MixedPrecisionType.Bf16
                : MixedPrecisionType.Fp16;
            optimized = optimized with
            {
                UseMixedPrecision = true,
                MixedPrecisionType = mpType
            };
        }

        // Adjust batch size based on memory
        var availableVram = hardware.TotalFreeGpuMemory;
        var estimatedVramPerSample = modelSizeBytes / 4; // Rough estimate

        if (availableVram > 0)
        {
            var maxBatchSize = (int)(availableVram / estimatedVramPerSample / 2);
            maxBatchSize = Math.Max(1, Math.Min(maxBatchSize, config.BatchSize));

            if (maxBatchSize < config.BatchSize)
            {
                optimized = optimized with
                {
                    BatchSize = maxBatchSize,
                    GradientAccumulationSteps = config.EffectiveBatchSize / maxBatchSize
                };
            }
        }

        // Enable gradient checkpointing if recommended
        if (hardware.RecommendGradientCheckpointing)
        {
            optimized = optimized with { UseGradientCheckpointing = true };
        }

        return optimized;
    }
}

/// <summary>
/// Training scenarios for configuration recommendations.
/// </summary>
public enum TrainingScenario
{
    /// <summary>
    /// Quick experiment to test settings.
    /// </summary>
    QuickExperiment,

    /// <summary>
    /// Standard full training run.
    /// </summary>
    FullTraining,

    /// <summary>
    /// Optimized for limited GPU memory.
    /// </summary>
    MemoryConstrained,

    /// <summary>
    /// Maximum quality, longer training.
    /// </summary>
    HighQuality
}
```

---

## v0.8.1i: Memory Management Utilities

### Objective
Create memory management utilities for safe tensor allocation, automatic cleanup, and OOM recovery.

### File: `src/SeniorIntern.Services/Training/TensorMemoryPool.cs`

```csharp
using System.Collections.Concurrent;
using Microsoft.Extensions.Logging;
using TorchSharp;

namespace SeniorIntern.Services.Training;

/// <summary>
/// Memory pool for tensor reuse to reduce allocation overhead.
/// </summary>
public sealed class TensorMemoryPool : IDisposable
{
    private readonly ILogger<TensorMemoryPool> _logger;
    private readonly ConcurrentDictionary<string, ConcurrentBag<torch.Tensor>> _pools = new();
    private readonly long _maxPoolSizeBytes;
    private long _currentPoolSizeBytes;
    private bool _disposed;

    public TensorMemoryPool(
        ILogger<TensorMemoryPool> logger,
        long maxPoolSizeBytes = 1024 * 1024 * 1024) // 1GB default
    {
        _logger = logger;
        _maxPoolSizeBytes = maxPoolSizeBytes;
    }

    /// <summary>
    /// Get or create a tensor with the specified shape and dtype.
    /// </summary>
    public torch.Tensor GetTensor(
        long[] shape,
        torch.ScalarType dtype = torch.ScalarType.Float32,
        torch.Device? device = null)
    {
        var key = GetPoolKey(shape, dtype, device);

        if (_pools.TryGetValue(key, out var pool) && pool.TryTake(out var tensor))
        {
            // Reuse existing tensor, zero it out
            tensor.zero_();
            return tensor;
        }

        // Create new tensor
        device ??= torch.CPU;
        return torch.zeros(shape, dtype, device);
    }

    /// <summary>
    /// Return a tensor to the pool for reuse.
    /// </summary>
    public void ReturnTensor(torch.Tensor tensor)
    {
        if (_disposed)
        {
            tensor.Dispose();
            return;
        }

        var size = tensor.numel() * GetDtypeSize(tensor.dtype);

        // Don't pool if we're over capacity
        if (_currentPoolSizeBytes + size > _maxPoolSizeBytes)
        {
            tensor.Dispose();
            return;
        }

        var key = GetPoolKey(tensor.shape, tensor.dtype, tensor.device);
        var pool = _pools.GetOrAdd(key, _ => new ConcurrentBag<torch.Tensor>());
        pool.Add(tensor);

        Interlocked.Add(ref _currentPoolSizeBytes, size);
    }

    /// <summary>
    /// Clear all pooled tensors.
    /// </summary>
    public void Clear()
    {
        foreach (var pool in _pools.Values)
        {
            while (pool.TryTake(out var tensor))
            {
                tensor.Dispose();
            }
        }

        _pools.Clear();
        _currentPoolSizeBytes = 0;
        _logger.LogDebug("Tensor memory pool cleared");
    }

    private static string GetPoolKey(
        long[] shape,
        torch.ScalarType dtype,
        torch.Device? device)
    {
        var shapeStr = string.Join("x", shape);
        var deviceStr = device?.ToString() ?? "cpu";
        return $"{shapeStr}_{dtype}_{deviceStr}";
    }

    private static long GetDtypeSize(torch.ScalarType dtype)
    {
        return dtype switch
        {
            torch.ScalarType.Float32 => 4,
            torch.ScalarType.Float64 => 8,
            torch.ScalarType.Float16 => 2,
            torch.ScalarType.BFloat16 => 2,
            torch.ScalarType.Int32 => 4,
            torch.ScalarType.Int64 => 8,
            torch.ScalarType.Int16 => 2,
            torch.ScalarType.Int8 => 1,
            torch.ScalarType.Byte => 1,
            torch.ScalarType.Bool => 1,
            _ => 4
        };
    }

    public void Dispose()
    {
        if (_disposed) return;
        _disposed = true;
        Clear();
    }
}
```

### File: `src/SeniorIntern.Services/Training/MemoryGuard.cs`

```csharp
using Microsoft.Extensions.Logging;
using TorchSharp;

namespace SeniorIntern.Services.Training;

/// <summary>
/// Guards against out-of-memory conditions during training.
/// </summary>
public sealed class MemoryGuard
{
    private readonly ILogger<MemoryGuard> _logger;
    private readonly long _memoryThresholdBytes;
    private readonly float _memoryThresholdPercent;
    private int _oomRecoveryAttempts;
    private const int MaxOomRecoveryAttempts = 3;

    public MemoryGuard(
        ILogger<MemoryGuard> logger,
        long memoryThresholdBytes = 500 * 1024 * 1024, // 500MB
        float memoryThresholdPercent = 0.9f)
    {
        _logger = logger;
        _memoryThresholdBytes = memoryThresholdBytes;
        _memoryThresholdPercent = memoryThresholdPercent;
    }

    /// <summary>
    /// Check if memory is below threshold before an operation.
    /// </summary>
    /// <param name="device">Device to check (null for all).</param>
    /// <returns>True if safe to proceed, false if memory is critically low.</returns>
    public bool CheckMemoryAvailable(torch.Device? device = null)
    {
        try
        {
            if (device?.type == DeviceType.CUDA ||
                (device == null && torch.cuda.is_available()))
            {
                var deviceIndex = device?.index ?? 0;
                var (free, total) = torch.cuda.mem_get_info(deviceIndex);

                var freeBytes = (long)free;
                var usedPercent = 1.0f - (float)free / total;

                if (freeBytes < _memoryThresholdBytes)
                {
                    _logger.LogWarning(
                        "GPU memory critically low: {Free:F0}MB free",
                        freeBytes / 1e6);
                    return false;
                }

                if (usedPercent > _memoryThresholdPercent)
                {
                    _logger.LogWarning(
                        "GPU memory usage high: {Used:P0}",
                        usedPercent);
                    return false;
                }
            }

            // Check system RAM
            var gcInfo = GC.GetGCMemoryInfo();
            var availableRam = gcInfo.TotalAvailableMemoryBytes -
                               System.Diagnostics.Process.GetCurrentProcess().WorkingSet64;

            if (availableRam < _memoryThresholdBytes)
            {
                _logger.LogWarning(
                    "System memory critically low: {Free:F0}MB free",
                    availableRam / 1e6);
                return false;
            }

            return true;
        }
        catch (Exception ex)
        {
            _logger.LogWarning(ex, "Failed to check memory availability");
            return true; // Assume OK if we can't check
        }
    }

    /// <summary>
    /// Execute an action with OOM protection.
    /// Automatically attempts recovery if OOM occurs.
    /// </summary>
    public async Task<T> ExecuteWithOomProtectionAsync<T>(
        Func<Task<T>> action,
        Func<Task<T>>? fallbackAction = null,
        CancellationToken ct = default)
    {
        while (_oomRecoveryAttempts < MaxOomRecoveryAttempts)
        {
            try
            {
                return await action();
            }
            catch (Exception ex) when (IsOutOfMemoryException(ex))
            {
                _oomRecoveryAttempts++;
                _logger.LogWarning(
                    "OOM detected (attempt {Attempt}/{Max}), attempting recovery",
                    _oomRecoveryAttempts, MaxOomRecoveryAttempts);

                if (!await TryRecoverFromOomAsync())
                {
                    if (fallbackAction != null)
                    {
                        _logger.LogInformation("Executing fallback action after OOM");
                        return await fallbackAction();
                    }
                    throw;
                }
            }
        }

        throw new InvalidOperationException(
            $"Failed to recover from OOM after {MaxOomRecoveryAttempts} attempts");
    }

    /// <summary>
    /// Attempt to recover from an out-of-memory condition.
    /// </summary>
    public async Task<bool> TryRecoverFromOomAsync()
    {
        _logger.LogInformation("Attempting OOM recovery...");

        try
        {
            // 1. Force .NET garbage collection
            GC.Collect(GC.MaxGeneration, GCCollectionMode.Aggressive, blocking: true);
            GC.WaitForPendingFinalizers();
            GC.Collect();

            // 2. Clear CUDA cache if available
            if (torch.cuda.is_available())
            {
                torch.cuda.empty_cache();
                torch.cuda.synchronize();
            }

            // 3. Wait a bit for memory to settle
            await Task.Delay(100);

            // 4. Check if we recovered enough memory
            var memoryOk = CheckMemoryAvailable();

            if (memoryOk)
            {
                _logger.LogInformation("OOM recovery successful");
                _oomRecoveryAttempts = 0; // Reset counter on success
            }

            return memoryOk;
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "OOM recovery failed");
            return false;
        }
    }

    /// <summary>
    /// Reset the OOM recovery attempt counter.
    /// Call this at the start of each training step.
    /// </summary>
    public void ResetRecoveryCounter()
    {
        _oomRecoveryAttempts = 0;
    }

    private static bool IsOutOfMemoryException(Exception ex)
    {
        if (ex is OutOfMemoryException) return true;

        // TorchSharp/CUDA OOM messages
        var message = ex.Message.ToLowerInvariant();
        return message.Contains("out of memory") ||
               message.Contains("cuda error") && message.Contains("memory") ||
               message.Contains("oom") ||
               message.Contains("alloc");
    }
}
```

### File: `src/SeniorIntern.Services/Training/GradientCheckpointer.cs`

```csharp
using TorchSharp;
using static TorchSharp.torch;

namespace SeniorIntern.Services.Training;

/// <summary>
/// Implements gradient checkpointing for memory-efficient training.
/// Trades compute for memory by recomputing activations during backward pass.
/// </summary>
public static class GradientCheckpointer
{
    /// <summary>
    /// Run a function with gradient checkpointing.
    /// </summary>
    /// <param name="function">Function to checkpoint.</param>
    /// <param name="inputs">Input tensors.</param>
    /// <param name="preserveRngState">Whether to preserve RNG state.</param>
    public static Tensor Checkpoint(
        Func<Tensor[], Tensor> function,
        Tensor[] inputs,
        bool preserveRngState = true)
    {
        // TorchSharp's checkpointing implementation
        // This is a simplified version - actual implementation would use
        // torch.utils.checkpoint.checkpoint equivalent

        // For now, just run the function normally
        // A full implementation would:
        // 1. Disable gradients, run forward
        // 2. During backward, re-run forward with gradients enabled

        return function(inputs);
    }

    /// <summary>
    /// Apply gradient checkpointing to a sequential module.
    /// </summary>
    /// <param name="module">Module to checkpoint.</param>
    /// <param name="segments">Number of checkpoint segments.</param>
    public static nn.Module ApplyCheckpointing(nn.Module module, int segments = 2)
    {
        // This would wrap the module layers with checkpointing
        // Actual implementation depends on module structure
        return module;
    }
}
```

### File: `src/SeniorIntern.Services/Training/AutoMixedPrecision.cs`

```csharp
using Microsoft.Extensions.Logging;
using TorchSharp;
using static TorchSharp.torch;

namespace SeniorIntern.Services.Training;

/// <summary>
/// Handles automatic mixed precision (AMP) training.
/// </summary>
public sealed class AutoMixedPrecision : IDisposable
{
    private readonly ILogger<AutoMixedPrecision> _logger;
    private readonly bool _enabled;
    private readonly ScalarType _dtype;
    private GradScaler? _scaler;
    private bool _disposed;

    public AutoMixedPrecision(
        ILogger<AutoMixedPrecision> logger,
        bool enabled = true,
        ScalarType dtype = ScalarType.Float16)
    {
        _logger = logger;
        _enabled = enabled && cuda.is_available();
        _dtype = dtype;

        if (_enabled)
        {
            _scaler = new GradScaler();
            _logger.LogInformation("AMP enabled with dtype: {Dtype}", dtype);
        }
    }

    /// <summary>
    /// Whether AMP is enabled.
    /// </summary>
    public bool Enabled => _enabled;

    /// <summary>
    /// Get the current gradient scaler.
    /// </summary>
    public GradScaler? Scaler => _scaler;

    /// <summary>
    /// Run forward pass with autocast.
    /// </summary>
    public Tensor Forward(Func<Tensor> forwardFunc)
    {
        if (!_enabled)
        {
            return forwardFunc();
        }

        // Use autocast for forward pass
        using var _ = autocast(DeviceType.CUDA, _dtype);
        return forwardFunc();
    }

    /// <summary>
    /// Scale loss for mixed precision training.
    /// </summary>
    public Tensor ScaleLoss(Tensor loss)
    {
        if (!_enabled || _scaler == null)
        {
            return loss;
        }

        return _scaler.scale(loss);
    }

    /// <summary>
    /// Unscale gradients before clipping.
    /// </summary>
    public void UnscaleGradients(optim.Optimizer optimizer)
    {
        if (!_enabled || _scaler == null) return;

        _scaler.unscale_(optimizer);
    }

    /// <summary>
    /// Step optimizer with gradient scaler.
    /// </summary>
    public void Step(optim.Optimizer optimizer)
    {
        if (!_enabled || _scaler == null)
        {
            optimizer.step();
            return;
        }

        _scaler.step(optimizer);
        _scaler.update();
    }

    /// <summary>
    /// Check if gradients contain inf/nan after unscaling.
    /// </summary>
    public bool GradientsAreFinite(optim.Optimizer optimizer)
    {
        if (!_enabled || _scaler == null) return true;

        // Check if any gradients are inf or nan
        foreach (var group in optimizer.param_groups)
        {
            foreach (var p in group.parameters())
            {
                if (p.grad is not null)
                {
                    if (!torch.isfinite(p.grad).all().item<bool>())
                    {
                        return false;
                    }
                }
            }
        }

        return true;
    }

    public void Dispose()
    {
        if (_disposed) return;
        _disposed = true;

        _scaler?.Dispose();
        _scaler = null;
    }
}

/// <summary>
/// Gradient scaler for mixed precision training.
/// </summary>
public sealed class GradScaler : IDisposable
{
    private float _scale = 65536.0f;
    private float _growthFactor = 2.0f;
    private float _backoffFactor = 0.5f;
    private int _growthInterval = 2000;
    private int _stepsSinceGrowth = 0;
    private bool _foundInf = false;

    public Tensor scale(Tensor loss)
    {
        return loss * _scale;
    }

    public void unscale_(optim.Optimizer optimizer)
    {
        var invScale = 1.0f / _scale;
        _foundInf = false;

        foreach (var group in optimizer.param_groups)
        {
            foreach (var p in group.parameters())
            {
                if (p.grad is not null)
                {
                    p.grad.mul_(invScale);

                    if (!torch.isfinite(p.grad).all().item<bool>())
                    {
                        _foundInf = true;
                    }
                }
            }
        }
    }

    public void step(optim.Optimizer optimizer)
    {
        if (!_foundInf)
        {
            optimizer.step();
        }
    }

    public void update()
    {
        if (_foundInf)
        {
            _scale *= _backoffFactor;
            _stepsSinceGrowth = 0;
        }
        else
        {
            _stepsSinceGrowth++;
            if (_stepsSinceGrowth >= _growthInterval)
            {
                _scale *= _growthFactor;
                _stepsSinceGrowth = 0;
            }
        }

        _scale = Math.Clamp(_scale, 1.0f, 65536.0f);
    }

    public void Dispose()
    {
        // Nothing to dispose
    }
}
```

---

## v0.8.1j: Unit Testing & Integration

### Objective
Create comprehensive unit tests and integration tests for all training infrastructure components.

### File: `tests/SeniorIntern.Tests/Training/TorchSharpInitializerTests.cs`

```csharp
using Microsoft.Extensions.Logging;
using Moq;
using SeniorIntern.Core.Interfaces;
using SeniorIntern.Core.Training;
using SeniorIntern.Services.Training;
using Xunit;

namespace SeniorIntern.Tests.Training;

public class TorchSharpInitializerTests
{
    private readonly Mock<ILogger<TorchSharpInitializer>> _loggerMock;
    private readonly TorchSharpInitializer _initializer;

    public TorchSharpInitializerTests()
    {
        _loggerMock = new Mock<ILogger<TorchSharpInitializer>>();
        _initializer = new TorchSharpInitializer(_loggerMock.Object);
    }

    [Fact]
    public async Task InitializeAsync_WithDefaultOptions_Succeeds()
    {
        // Arrange
        var options = new TorchSharpInitOptions
        {
            PreferredBackend = TorchBackend.Cpu,
            AllowCpuFallback = true
        };

        // Act
        var state = await _initializer.InitializeAsync(options);

        // Assert
        Assert.True(state.IsInitialized);
        Assert.NotEmpty(state.Version);
    }

    [Fact]
    public async Task InitializeAsync_CalledTwice_ReturnsExistingState()
    {
        // Arrange
        var options = new TorchSharpInitOptions { PreferredBackend = TorchBackend.Cpu };

        // Act
        var state1 = await _initializer.InitializeAsync(options);
        var state2 = await _initializer.InitializeAsync(options);

        // Assert
        Assert.Equal(state1.IsInitialized, state2.IsInitialized);
    }

    [Fact]
    public async Task VerifyAsync_AfterInitialization_ReturnsTrue()
    {
        // Arrange
        var options = new TorchSharpInitOptions { PreferredBackend = TorchBackend.Cpu };
        await _initializer.InitializeAsync(options);

        // Act
        var result = await _initializer.VerifyAsync();

        // Assert
        Assert.True(result);
    }

    [Fact]
    public async Task VerifyAsync_BeforeInitialization_ReturnsFalse()
    {
        // Act
        var result = await _initializer.VerifyAsync();

        // Assert
        Assert.False(result);
    }

    [Fact]
    public async Task ShutdownAsync_AfterInitialization_ResetsState()
    {
        // Arrange
        var options = new TorchSharpInitOptions { PreferredBackend = TorchBackend.Cpu };
        await _initializer.InitializeAsync(options);

        // Act
        await _initializer.ShutdownAsync();

        // Assert
        Assert.False(_initializer.IsInitialized);
    }

    [Fact]
    public void PreferredBackend_OnDifferentPlatforms_ReturnsAppropriateBackend()
    {
        // Act
        var backend = _initializer.PreferredBackend;

        // Assert
        // Backend should be Cpu, Cuda, or Mps based on platform
        Assert.True(
            backend == TorchBackend.Cpu ||
            backend == TorchBackend.Cuda ||
            backend == TorchBackend.Mps);
    }
}
```

### File: `tests/SeniorIntern.Tests/Training/HardwareDetectionServiceTests.cs`

```csharp
using Microsoft.Extensions.Logging;
using Moq;
using SeniorIntern.Core.Training;
using SeniorIntern.Services.Training;
using Xunit;

namespace SeniorIntern.Tests.Training;

public class HardwareDetectionServiceTests
{
    private readonly HardwareDetectionService _service;

    public HardwareDetectionServiceTests()
    {
        var loggerMock = new Mock<ILogger<HardwareDetectionService>>();
        var cudaDetector = new CudaHardwareDetector(
            Mock.Of<ILogger<CudaHardwareDetector>>());
        var metalDetector = new MetalHardwareDetector(
            Mock.Of<ILogger<MetalHardwareDetector>>());
        var cpuDetector = new CpuHardwareDetector(
            Mock.Of<ILogger<CpuHardwareDetector>>());

        _service = new HardwareDetectionService(
            loggerMock.Object,
            cudaDetector,
            metalDetector,
            cpuDetector);
    }

    [Fact]
    public async Task DetectHardwareAsync_ReturnsValidInfo()
    {
        // Act
        var info = await _service.DetectHardwareAsync();

        // Assert
        Assert.NotNull(info);
        Assert.True(info.CpuCoreCount > 0);
        Assert.True(info.TotalSystemMemory > 0);
        Assert.NotEmpty(info.RecommendedDevice);
    }

    [Fact]
    public async Task DetectHardwareAsync_CachesResult()
    {
        // Act
        var info1 = await _service.DetectHardwareAsync();
        var info2 = await _service.DetectHardwareAsync();

        // Assert
        Assert.Equal(info1.DetectedAt, info2.DetectedAt);
    }

    [Fact]
    public async Task DetectHardwareAsync_WithRefresh_UpdatesCache()
    {
        // Act
        var info1 = await _service.DetectHardwareAsync();
        await Task.Delay(10);
        var info2 = await _service.DetectHardwareAsync(refresh: true);

        // Assert
        Assert.NotEqual(info1.DetectedAt, info2.DetectedAt);
    }

    [Fact]
    public async Task CheckTrainingCapabilityAsync_WithMinimalRequirements_ReturnsCanTrain()
    {
        // Arrange
        var requirements = new TrainingRequirements
        {
            MinimumVramBytes = 0,
            MinimumRamBytes = 0,
            RequiresGpu = false
        };

        // Act
        var result = await _service.CheckTrainingCapabilityAsync(requirements);

        // Assert
        Assert.True(result.CanTrain);
    }

    [Fact]
    public async Task GetRecommendedConfiguration_ReturnsValidConfig()
    {
        // Arrange
        var hardware = await _service.DetectHardwareAsync();
        var modelSize = 4L * 1024 * 1024 * 1024; // 4GB
        var datasetSize = 1000;

        // Act
        var config = _service.GetRecommendedConfiguration(hardware, modelSize, datasetSize);

        // Assert
        Assert.NotNull(config);
        Assert.True(config.Epochs > 0);
        Assert.True(config.BatchSize > 0);
        Assert.True(config.LoraRank > 0);
    }
}
```

### File: `tests/SeniorIntern.Tests/Training/TrainingConfigurationTests.cs`

```csharp
using SeniorIntern.Core.Training;
using SeniorIntern.Services.Training;
using Xunit;

namespace SeniorIntern.Tests.Training;

public class TrainingConfigurationTests
{
    [Fact]
    public void TrainingConfiguration_DefaultValues_AreValid()
    {
        // Arrange & Act
        var config = new TrainingConfiguration();

        // Assert
        Assert.Equal("cpu", config.Device);
        Assert.Equal(3, config.Epochs);
        Assert.Equal(4, config.BatchSize);
        Assert.Equal(16, config.LoraRank);
        Assert.True(config.UseMixedPrecision);
    }

    [Fact]
    public void TrainingConfiguration_EffectiveBatchSize_CalculatedCorrectly()
    {
        // Arrange
        var config = new TrainingConfiguration
        {
            BatchSize = 4,
            GradientAccumulationSteps = 8
        };

        // Act
        var effectiveBatchSize = config.EffectiveBatchSize;

        // Assert
        Assert.Equal(32, effectiveBatchSize);
    }

    [Fact]
    public void TrainingConfigurationBuilder_Default_CreatesValidConfig()
    {
        // Arrange & Act
        var config = TrainingConfigurationBuilder.Default().Build();

        // Assert
        Assert.NotNull(config);
    }

    [Fact]
    public void TrainingConfigurationBuilder_ForSmallGpu_CreatesOptimizedConfig()
    {
        // Arrange & Act
        var config = TrainingConfigurationBuilder.ForSmallGpu().Build();

        // Assert
        Assert.Equal(1, config.BatchSize);
        Assert.True(config.UseGradientCheckpointing);
        Assert.Equal(8, config.LoraRank);
    }

    [Fact]
    public void TrainingConfigurationBuilder_ForCpu_DisablesMixedPrecision()
    {
        // Arrange & Act
        var config = TrainingConfigurationBuilder.ForCpu().Build();

        // Assert
        Assert.Equal("cpu", config.Device);
        Assert.False(config.UseMixedPrecision);
    }

    [Fact]
    public void TrainingConfigurationBuilder_Chaining_WorksCorrectly()
    {
        // Arrange & Act
        var config = TrainingConfigurationBuilder.Default()
            .WithDevice("cuda:0")
            .WithEpochs(5)
            .WithBatchSize(8)
            .WithLoraRank(32)
            .WithLearningRate(1e-4f)
            .Build();

        // Assert
        Assert.Equal("cuda:0", config.Device);
        Assert.Equal(5, config.Epochs);
        Assert.Equal(8, config.BatchSize);
        Assert.Equal(32, config.LoraRank);
        Assert.Equal(1e-4f, config.LearningRate);
    }
}
```

### File: `tests/SeniorIntern.Tests/Training/TrainingConfigurationValidatorTests.cs`

```csharp
using SeniorIntern.Core.Training;
using SeniorIntern.Services.Training;
using Xunit;

namespace SeniorIntern.Tests.Training;

public class TrainingConfigurationValidatorTests
{
    private readonly TrainingConfigurationValidator _validator;

    public TrainingConfigurationValidatorTests()
    {
        _validator = new TrainingConfigurationValidator();
    }

    [Fact]
    public void Validate_DefaultConfiguration_IsValid()
    {
        // Arrange
        var config = new TrainingConfiguration();

        // Act
        var result = _validator.Validate(config);

        // Assert
        Assert.True(result.IsValid);
    }

    [Fact]
    public void Validate_ZeroEpochs_ReturnsError()
    {
        // Arrange
        var config = new TrainingConfiguration { Epochs = 0 };

        // Act
        var result = _validator.Validate(config);

        // Assert
        Assert.False(result.IsValid);
        Assert.Contains(result.Errors, e => e.Contains("Epochs"));
    }

    [Fact]
    public void Validate_NegativeLearningRate_ReturnsError()
    {
        // Arrange
        var config = new TrainingConfiguration { LearningRate = -0.001f };

        // Act
        var result = _validator.Validate(config);

        // Assert
        Assert.False(result.IsValid);
        Assert.Contains(result.Errors, e => e.Contains("Learning rate"));
    }

    [Fact]
    public void Validate_HighLearningRate_ReturnsWarning()
    {
        // Arrange
        var config = new TrainingConfiguration { LearningRate = 0.1f };

        // Act
        var result = _validator.Validate(config);

        // Assert
        Assert.True(result.IsValid); // Still valid, just a warning
        Assert.Contains(result.Warnings, w => w.Contains("Learning rate"));
    }

    [Fact]
    public void Validate_NoTargetModules_ReturnsError()
    {
        // Arrange
        var config = new TrainingConfiguration { TargetModules = Array.Empty<string>() };

        // Act
        var result = _validator.Validate(config);

        // Assert
        Assert.False(result.IsValid);
        Assert.Contains(result.Errors, e => e.Contains("target module"));
    }

    [Fact]
    public void AutoFix_InvalidConfiguration_FixesIssues()
    {
        // Arrange
        var config = new TrainingConfiguration
        {
            Epochs = 0,
            LearningRate = -1,
            LoraRank = 0
        };

        // Act
        var fixed = _validator.AutoFix(config);

        // Assert
        Assert.True(fixed.Epochs > 0);
        Assert.True(fixed.LearningRate > 0);
        Assert.True(fixed.LoraRank > 0);
    }
}
```

### File: `tests/SeniorIntern.Tests/Training/MemoryEstimatorTests.cs`

```csharp
using SeniorIntern.Core.Training;
using Xunit;

namespace SeniorIntern.Tests.Training;

public class MemoryEstimatorTests
{
    [Fact]
    public void EstimateLoraTrainingMemory_ReturnsValidEstimate()
    {
        // Arrange
        var modelSize = 4L * 1024 * 1024 * 1024; // 4GB
        var config = new TrainingConfiguration();
        var seqLength = 2048;
        var availableMemory = 16L * 1024 * 1024 * 1024; // 16GB

        // Act
        var estimate = MemoryEstimator.EstimateLoraTrainingMemory(
            modelSize, config, seqLength, availableMemory);

        // Assert
        Assert.True(estimate.GpuMemoryRequired > 0);
        Assert.True(estimate.RamRequired > 0);
        Assert.True(estimate.ModelWeightsMemory > 0);
        Assert.True(estimate.LoraWeightsMemory > 0);
    }

    [Fact]
    public void EstimateLoraTrainingMemory_WithGradientCheckpointing_ReducesActivationMemory()
    {
        // Arrange
        var modelSize = 4L * 1024 * 1024 * 1024;
        var configWithoutCheckpointing = new TrainingConfiguration
        {
            UseGradientCheckpointing = false
        };
        var configWithCheckpointing = new TrainingConfiguration
        {
            UseGradientCheckpointing = true
        };
        var seqLength = 2048;
        var availableMemory = 16L * 1024 * 1024 * 1024;

        // Act
        var estimateWithout = MemoryEstimator.EstimateLoraTrainingMemory(
            modelSize, configWithoutCheckpointing, seqLength, availableMemory);
        var estimateWith = MemoryEstimator.EstimateLoraTrainingMemory(
            modelSize, configWithCheckpointing, seqLength, availableMemory);

        // Assert
        Assert.True(estimateWith.ActivationsMemory < estimateWithout.ActivationsMemory);
    }

    [Fact]
    public void EstimateLoraTrainingMemory_InsufficientMemory_IncludesRecommendations()
    {
        // Arrange
        var modelSize = 20L * 1024 * 1024 * 1024; // 20GB model
        var config = new TrainingConfiguration { BatchSize = 8 };
        var seqLength = 4096;
        var availableMemory = 8L * 1024 * 1024 * 1024; // Only 8GB

        // Act
        var estimate = MemoryEstimator.EstimateLoraTrainingMemory(
            modelSize, config, seqLength, availableMemory);

        // Assert
        Assert.False(estimate.FitsInGpuMemory);
        Assert.NotEmpty(estimate.Recommendations);
    }
}
```

### File: `tests/SeniorIntern.Tests/Training/LoraConfigurationTests.cs`

```csharp
using SeniorIntern.Core.Training;
using Xunit;

namespace SeniorIntern.Tests.Training;

public class LoraConfigurationTests
{
    [Fact]
    public void LoraConfiguration_Scaling_CalculatedCorrectly()
    {
        // Arrange
        var config = new LoraConfiguration { Rank = 16, Alpha = 32f };

        // Act
        var scaling = config.Scaling;

        // Assert
        Assert.Equal(2.0f, scaling);
    }

    [Fact]
    public void LoraConfiguration_RsLoraScaling_UsesSquareRoot()
    {
        // Arrange
        var config = new LoraConfiguration
        {
            Rank = 16,
            Alpha = 32f,
            UseRsLora = true
        };

        // Act
        var scaling = config.Scaling;

        // Assert
        Assert.Equal(32f / MathF.Sqrt(16), scaling);
    }

    [Fact]
    public void LoraConfiguration_EstimateTrainableParameters_ReturnsReasonableEstimate()
    {
        // Arrange
        var config = new LoraConfiguration { Rank = 16 };
        var hiddenSize = 4096;
        var numLayers = 32;

        // Act
        var params_ = config.EstimateTrainableParameters(hiddenSize, numLayers);

        // Assert
        Assert.True(params_ > 0);
        // Each module: 2 * rank * hidden = 2 * 16 * 4096 = 131072
        // 7 default modules * 32 layers = 224 LoRA layers
        // 131072 * 224 = ~29M params
        Assert.True(params_ > 1_000_000); // At least 1M params
    }

    [Fact]
    public void LoraConfiguration_Presets_AreValid()
    {
        // Act
        var defaultConfig = LoraConfiguration.Default;
        var lowMemConfig = LoraConfiguration.LowMemory;
        var highQualityConfig = LoraConfiguration.HighQuality;

        // Assert
        Assert.True(defaultConfig.Rank > 0);
        Assert.True(lowMemConfig.Rank < defaultConfig.Rank);
        Assert.True(highQualityConfig.Rank > defaultConfig.Rank);
    }
}
```

### File: `tests/SeniorIntern.Tests/Training/CpuHardwareDetectorTests.cs`

```csharp
using Microsoft.Extensions.Logging;
using Moq;
using SeniorIntern.Core.Training;
using SeniorIntern.Services.Training;
using Xunit;

namespace SeniorIntern.Tests.Training;

public class CpuHardwareDetectorTests
{
    private readonly CpuHardwareDetector _detector;

    public CpuHardwareDetectorTests()
    {
        var loggerMock = new Mock<ILogger<CpuHardwareDetector>>();
        _detector = new CpuHardwareDetector(loggerMock.Object);
    }

    [Fact]
    public void GetCpuInfo_ReturnsValidInfo()
    {
        // Act
        var info = _detector.GetCpuInfo();

        // Assert
        Assert.NotNull(info);
        Assert.True(info.CoreCount > 0);
        Assert.NotEmpty(info.Architecture);
    }

    [Fact]
    public void GetMemoryInfo_ReturnsValidInfo()
    {
        // Act
        var info = _detector.GetMemoryInfo();

        // Assert
        Assert.True(info.TotalPhysicalMemory > 0);
        Assert.True(info.ProcessMemoryUsed > 0);
    }

    [Fact]
    public void EstimateTrainingSpeed_ReturnsCategory()
    {
        // Arrange
        var modelSize = 4L * 1024 * 1024 * 1024;

        // Act
        var speed = _detector.EstimateTrainingSpeed(modelSize);

        // Assert
        Assert.True(
            speed == TrainingSpeedCategory.VerySlow ||
            speed == TrainingSpeedCategory.Slow);
    }
}
```

### File: `tests/SeniorIntern.Tests/Training/MemoryGuardTests.cs`

```csharp
using Microsoft.Extensions.Logging;
using Moq;
using SeniorIntern.Services.Training;
using Xunit;

namespace SeniorIntern.Tests.Training;

public class MemoryGuardTests
{
    private readonly MemoryGuard _guard;

    public MemoryGuardTests()
    {
        var loggerMock = new Mock<ILogger<MemoryGuard>>();
        _guard = new MemoryGuard(loggerMock.Object);
    }

    [Fact]
    public void CheckMemoryAvailable_ReturnsTrue_WhenMemoryAvailable()
    {
        // Act
        var result = _guard.CheckMemoryAvailable();

        // Assert
        Assert.True(result);
    }

    [Fact]
    public async Task ExecuteWithOomProtectionAsync_Success_ExecutesAction()
    {
        // Arrange
        var executed = false;

        // Act
        await _guard.ExecuteWithOomProtectionAsync(async () =>
        {
            executed = true;
            return await Task.FromResult(true);
        });

        // Assert
        Assert.True(executed);
    }

    [Fact]
    public async Task ExecuteWithOomProtectionAsync_WithFallback_ExecutesFallbackOnOom()
    {
        // Arrange
        var oomCount = 0;
        var fallbackExecuted = false;

        // Act
        await _guard.ExecuteWithOomProtectionAsync(
            async () =>
            {
                oomCount++;
                if (oomCount <= 3)
                {
                    throw new OutOfMemoryException();
                }
                return await Task.FromResult(true);
            },
            async () =>
            {
                fallbackExecuted = true;
                return await Task.FromResult(true);
            });

        // Assert
        Assert.True(fallbackExecuted);
    }

    [Fact]
    public void ResetRecoveryCounter_ResetsCounter()
    {
        // Act - Should not throw
        _guard.ResetRecoveryCounter();

        // Assert - Counter should be reset (no way to verify directly)
        Assert.True(true);
    }
}
```

---

## Success Criteria

### v0.8.1 is complete when:

1. **TorchSharp Initialization**
   - [ ] TorchSharp loads native libraries correctly on Windows, Linux, and macOS
   - [ ] CUDA, Metal, and CPU backends are detected and initialized properly
   - [ ] Backend verification passes with a simple tensor operation
   - [ ] Shutdown properly releases all native resources

2. **Hardware Detection**
   - [ ] CUDA GPUs are detected with correct VRAM, compute capability, and name
   - [ ] Apple Silicon Macs are detected with correct unified memory info
   - [ ] CPU information (cores, AVX support) is detected correctly
   - [ ] Hardware detection is cached with configurable refresh

3. **Training Configuration**
   - [ ] Configuration models cover all LoRA training parameters
   - [ ] Builder pattern allows easy configuration creation
   - [ ] Validator catches invalid configurations with helpful messages
   - [ ] Auto-fix corrects common configuration issues

4. **Memory Management**
   - [ ] Memory estimation is reasonably accurate for different model sizes
   - [ ] OOM recovery successfully frees memory in most cases
   - [ ] Memory guard prevents operations when memory is critically low
   - [ ] Mixed precision scaler handles gradient scaling correctly

5. **Testing**
   - [ ] All unit tests pass
   - [ ] Integration tests verify end-to-end hardware detection
   - [ ] Tests cover edge cases and error conditions

---

## Dependencies

### NuGet Packages Required

```xml
<PackageReference Include="TorchSharp" Version="0.102.0" />
<PackageReference Include="TorchSharp-cuda-windows" Version="0.102.0" Condition="'$(OS)' == 'Windows_NT'" />
<PackageReference Include="TorchSharp-cuda-linux" Version="0.102.0" Condition="'$(OS)' == 'Unix' AND !$([MSBuild]::IsOSPlatform('OSX'))" />
<PackageReference Include="libtorch-cpu" Version="2.1.0.1" />
```

### Runtime Requirements

- .NET 8.0 or later
- CUDA 11.8+ with compatible drivers (for CUDA backend)
- macOS 12+ on Apple Silicon (for MPS backend)
- 8GB+ RAM recommended
- 4GB+ VRAM recommended for GPU training

---

## File Summary

| Sub-version | Files to Create | Files to Modify |
|-------------|-----------------|-----------------|
| v0.8.1a | 4 | 1 |
| v0.8.1b | 4 | 0 |
| v0.8.1c | 3 | 0 |
| v0.8.1d | 3 | 0 |
| v0.8.1e | 3 | 0 |
| v0.8.1f | 2 | 0 |
| v0.8.1g | 4 | 0 |
| v0.8.1h | 3 | 0 |
| v0.8.1i | 4 | 0 |
| v0.8.1j | 8 | 0 |
| **Total** | **38** | **1** |
