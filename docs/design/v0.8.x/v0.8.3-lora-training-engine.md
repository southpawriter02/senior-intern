# v0.8.3: LoRA Training Engine - Detailed Design Specification

## Overview

This document provides a comprehensive design specification for v0.8.3 of The Senior Intern project. This version implements the core LoRA (Low-Rank Adaptation) training algorithm using TorchSharp, including the training loop, loss computation, optimizer and scheduler configuration, gradient management, checkpoint saving/loading, and adapter management. This is the heart of the fine-tuning system that allows users to train custom adapters on their code.

### Objectives
- Implement LoRA layer architecture with proper weight initialization
- Create LoRA model wrapper that applies adapters to target modules
- Build the complete training loop with gradient accumulation
- Implement cross-entropy loss computation for causal language modeling
- Create optimizer factory with AdamW and learning rate schedulers
- Build checkpoint manager for saving and resuming training
- Implement adapter serialization in GGUF-compatible format
- Create training service interface for orchestrating the full training pipeline
- Add early stopping and best model tracking

### Prerequisites
- v0.8.1 (Training Infrastructure) completed - TorchSharp, hardware detection
- v0.8.2 (Dataset Pipeline) completed - data loaders, tokenization
- TorchSharp properly initialized with appropriate backend (CUDA/Metal/CPU)
- LLamaSharp for base model loading

---

## Architecture Overview

### LoRA Training Architecture

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           ITrainingService                                   │
│                      (Main Training Orchestrator)                            │
│  TrainAsync() │ ResumeTrainingAsync() │ PauseTrainingAsync() │ CancelTraining│
└─────────────────────────────────────────────────────────────────────────────┘
                                     │
         ┌───────────────────────────┼───────────────────────────┐
         ▼                           ▼                           ▼
┌─────────────────┐       ┌─────────────────┐       ┌─────────────────┐
│  TrainingJob    │       │ TrainingConfig  │       │ TrainingResult  │
│  • ModelPath    │       │ • Epochs        │       │ • Success       │
│  • DatasetPath  │       │ • BatchSize     │       │ • AdapterPath   │
│  • OutputDir    │       │ • LearningRate  │       │ • Metrics       │
└─────────────────┘       │ • LoRA params   │       └─────────────────┘
                          └─────────────────┘
                                     │
                                     ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                         LoraTrainingEngine                                   │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                        Training Pipeline                             │   │
│  │                                                                      │   │
│  │  ┌──────────┐   ┌──────────┐   ┌──────────┐   ┌──────────┐         │   │
│  │  │ Load     │   │ Apply    │   │ Load     │   │ Create   │         │   │
│  │  │ Base     │ → │ LoRA     │ → │ Dataset  │ → │ Optimizer│         │   │
│  │  │ Model    │   │ Adapters │   │ Loaders  │   │ Scheduler│         │   │
│  │  └──────────┘   └──────────┘   └──────────┘   └──────────┘         │   │
│  │       │                                              │               │   │
│  │       ▼                                              ▼               │   │
│  │  ┌───────────────────────────────────────────────────────────────┐  │   │
│  │  │                     Training Loop                              │  │   │
│  │  │  ┌─────────────────────────────────────────────────────────┐  │  │   │
│  │  │  │ For each epoch:                                          │  │  │   │
│  │  │  │   For each batch:                                        │  │  │   │
│  │  │  │     1. Forward pass through LoRA model                   │  │  │   │
│  │  │  │     2. Compute cross-entropy loss                        │  │  │   │
│  │  │  │     3. Backward pass (gradients)                         │  │  │   │
│  │  │  │     4. Gradient accumulation (if needed)                 │  │  │   │
│  │  │  │     5. Gradient clipping                                 │  │  │   │
│  │  │  │     6. Optimizer step                                    │  │  │   │
│  │  │  │     7. Scheduler step                                    │  │  │   │
│  │  │  │   Validation evaluation                                  │  │  │   │
│  │  │  │   Save checkpoint if best                                │  │  │   │
│  │  │  └─────────────────────────────────────────────────────────┘  │  │   │
│  │  └───────────────────────────────────────────────────────────────┘  │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────────────────────────┘
                                     │
                                     ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                          Output Artifacts                                    │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐             │
│  │ LoRA Adapter    │  │ Checkpoints     │  │ Training Logs   │             │
│  │ (adapter.safet) │  │ (checkpoint-*)  │  │ (metrics.json)  │             │
│  └─────────────────┘  └─────────────────┘  └─────────────────┘             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### LoRA Layer Architecture

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                            LoRA Layer                                        │
│                                                                              │
│                    Input: x [batch, seq_len, in_features]                   │
│                                      │                                       │
│                    ┌─────────────────┼─────────────────┐                    │
│                    │                 │                 │                    │
│                    ▼                 ▼                 ▼                    │
│           ┌─────────────┐    ┌─────────────┐                               │
│           │  Original   │    │   Dropout   │                               │
│           │   Linear    │    │  (optional) │                               │
│           │   W₀        │    └──────┬──────┘                               │
│           │ [out, in]   │           │                                       │
│           │  FROZEN     │           ▼                                       │
│           └──────┬──────┘    ┌─────────────┐                               │
│                  │           │   LoRA A    │                               │
│                  │           │  [r, in]    │  ← Kaiming init               │
│                  │           │  TRAINABLE  │                               │
│                  │           └──────┬──────┘                               │
│                  │                  │                                       │
│                  │                  ▼                                       │
│                  │           ┌─────────────┐                               │
│                  │           │   LoRA B    │                               │
│                  │           │  [out, r]   │  ← Zero init                  │
│                  │           │  TRAINABLE  │                               │
│                  │           └──────┬──────┘                               │
│                  │                  │                                       │
│                  │                  ▼                                       │
│                  │           ┌─────────────┐                               │
│                  │           │   Scale     │                               │
│                  │           │  α/r        │                               │
│                  │           └──────┬──────┘                               │
│                  │                  │                                       │
│                  ▼                  ▼                                       │
│           ┌─────────────────────────────────────┐                          │
│           │            Add                       │                          │
│           │     y = W₀x + (α/r)(BAx)            │                          │
│           └──────────────────┬──────────────────┘                          │
│                              │                                              │
│                              ▼                                              │
│                    Output: y [batch, seq_len, out_features]                 │
└─────────────────────────────────────────────────────────────────────────────┘

Where:
  • W₀ = Original frozen weights [out_features × in_features]
  • A = LoRA down-projection [rank × in_features]
  • B = LoRA up-projection [out_features × rank]
  • α = Scaling factor (typically 2 × rank)
  • r = Rank (typically 8, 16, 32, or 64)

The effective weight becomes: W = W₀ + (α/r)BA
Only A and B are trained, reducing trainable parameters significantly.
```

### Target Modules for LLaMA-style Models

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                      Transformer Layer (×N layers)                          │
│  ┌───────────────────────────────────────────────────────────────────────┐  │
│  │                    Multi-Head Attention                                │  │
│  │  ┌────────────┐ ┌────────────┐ ┌────────────┐ ┌────────────┐         │  │
│  │  │  q_proj    │ │  k_proj    │ │  v_proj    │ │  o_proj    │         │  │
│  │  │  ✓ LoRA    │ │  ✓ LoRA    │ │  ✓ LoRA    │ │  ✓ LoRA    │         │  │
│  │  └────────────┘ └────────────┘ └────────────┘ └────────────┘         │  │
│  └───────────────────────────────────────────────────────────────────────┘  │
│  ┌───────────────────────────────────────────────────────────────────────┐  │
│  │                    Feed-Forward Network (MLP)                          │  │
│  │  ┌────────────┐ ┌────────────┐ ┌────────────┐                         │  │
│  │  │ gate_proj  │ │  up_proj   │ │ down_proj  │                         │  │
│  │  │  ✓ LoRA    │ │  ✓ LoRA    │ │  ✓ LoRA    │                         │  │
│  │  └────────────┘ └────────────┘ └────────────┘                         │  │
│  └───────────────────────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────────────────────┘

Default target modules: ["q_proj", "k_proj", "v_proj", "o_proj",
                         "gate_proj", "up_proj", "down_proj"]
```

---

## v0.8.3a: LoRA Layer Implementation

### Objective
Implement the core LoRA layer that wraps linear layers with low-rank adaptation matrices, including weight initialization, forward pass, merge/unmerge functionality, and weight extraction.

### Files to Create

#### 1. Core/Training/LoraConfiguration.cs

```csharp
namespace SeniorIntern.Core.Training;

/// <summary>
/// Configuration for LoRA adaptation
/// </summary>
public sealed record LoraConfiguration
{
    /// <summary>
    /// Rank of the low-rank matrices (r)
    /// Common values: 8, 16, 32, 64
    /// Higher rank = more parameters = more expressive but slower
    /// </summary>
    public int Rank { get; init; } = 16;

    /// <summary>
    /// Scaling factor (α)
    /// The LoRA output is scaled by α/r
    /// Typically set to 2*rank or equal to rank
    /// </summary>
    public float Alpha { get; init; } = 32f;

    /// <summary>
    /// Dropout applied to LoRA layers during training
    /// Helps prevent overfitting
    /// </summary>
    public float Dropout { get; init; } = 0.05f;

    /// <summary>
    /// Target module names to apply LoRA to
    /// For LLaMA: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj
    /// </summary>
    public IReadOnlyList<string> TargetModules { get; init; } = new[]
    {
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    };

    /// <summary>
    /// Whether to use bias in LoRA layers
    /// </summary>
    public LoraLayerBias Bias { get; init; } = LoraLayerBias.None;

    /// <summary>
    /// Whether to apply LoRA to embedding layers
    /// </summary>
    public bool ApplyToEmbeddings { get; init; } = false;

    /// <summary>
    /// Whether to apply LoRA to the language model head
    /// </summary>
    public bool ApplyToLmHead { get; init; } = false;

    /// <summary>
    /// Initialization method for LoRA A matrix
    /// </summary>
    public LoraInitMethod InitMethod { get; init; } = LoraInitMethod.KaimingUniform;

    /// <summary>
    /// Scale factor for initialization
    /// </summary>
    public float InitScale { get; init; } = 1.0f;

    /// <summary>
    /// Computed scaling factor: α/r
    /// </summary>
    public float Scaling => Alpha / Rank;

    /// <summary>
    /// Calculate total trainable parameters for given layer dimensions
    /// </summary>
    public long CalculateTrainableParameters(int inFeatures, int outFeatures)
    {
        // LoRA A: [rank, in_features] + LoRA B: [out_features, rank]
        return (long)Rank * inFeatures + (long)outFeatures * Rank;
    }
}

/// <summary>
/// Bias configuration for LoRA layers
/// </summary>
public enum LoraLayerBias
{
    /// <summary>No bias in LoRA layers</summary>
    None,

    /// <summary>Bias only in LoRA B layer</summary>
    LoraOnly,

    /// <summary>Train all biases (original + LoRA)</summary>
    All
}

/// <summary>
/// Initialization method for LoRA matrices
/// </summary>
public enum LoraInitMethod
{
    /// <summary>Kaiming uniform initialization for A, zeros for B</summary>
    KaimingUniform,

    /// <summary>Gaussian initialization for A, zeros for B</summary>
    Gaussian,

    /// <summary>Zeros for both (training starts from original weights)</summary>
    Zeros
}
```

#### 2. Services/Training/LoraLayer.cs

```csharp
namespace SeniorIntern.Services.Training;

using TorchSharp;
using static TorchSharp.torch;
using static TorchSharp.torch.nn;

/// <summary>
/// Low-Rank Adaptation layer that wraps a linear layer
/// Implements: y = W₀x + (α/r)(BAx) where only A and B are trainable
/// </summary>
public sealed class LoraLayer : Module<Tensor, Tensor>
{
    private readonly Linear _originalLayer;
    private readonly Linear _loraA;
    private readonly Linear _loraB;
    private readonly Dropout? _dropout;
    private readonly float _scaling;
    private readonly string _name;

    private bool _merged;
    private bool _disabled;

    /// <summary>LoRA rank (r)</summary>
    public int Rank { get; }

    /// <summary>Scaling factor (α)</summary>
    public float Alpha { get; }

    /// <summary>Dropout probability</summary>
    public float DropoutProb { get; }

    /// <summary>Input features of the original layer</summary>
    public long InFeatures { get; }

    /// <summary>Output features of the original layer</summary>
    public long OutFeatures { get; }

    /// <summary>Whether LoRA weights are merged into original</summary>
    public bool IsMerged => _merged;

    /// <summary>Whether LoRA is disabled (pass-through mode)</summary>
    public bool IsDisabled => _disabled;

    /// <summary>Number of trainable parameters in this LoRA layer</summary>
    public long TrainableParameters => (Rank * InFeatures) + (OutFeatures * Rank);

    public LoraLayer(
        Linear originalLayer,
        int rank,
        float alpha,
        float dropout = 0.0f,
        LoraInitMethod initMethod = LoraInitMethod.KaimingUniform,
        float initScale = 1.0f,
        string name = "lora")
        : base(name)
    {
        _originalLayer = originalLayer;
        _name = name;
        Rank = rank;
        Alpha = alpha;
        DropoutProb = dropout;
        _scaling = alpha / rank;

        // Get original layer dimensions
        InFeatures = originalLayer.weight!.shape[1];
        OutFeatures = originalLayer.weight!.shape[0];

        // Validate rank
        if (rank > Math.Min(InFeatures, OutFeatures))
        {
            throw new ArgumentException(
                $"LoRA rank ({rank}) cannot exceed layer dimensions ({InFeatures} x {OutFeatures})");
        }

        // Create LoRA matrices
        // A: down-projection [rank, in_features] - reduces input dimension
        // B: up-projection [out_features, rank] - expands back
        _loraA = Linear(InFeatures, rank, hasBias: false);
        _loraB = Linear(rank, OutFeatures, hasBias: false);

        // Initialize weights
        InitializeWeights(initMethod, initScale);

        // Freeze original layer weights
        _originalLayer.weight!.requires_grad = false;
        if (_originalLayer.bias is not null)
        {
            _originalLayer.bias.requires_grad = false;
        }

        // Optional dropout
        if (dropout > 0)
        {
            _dropout = Dropout(dropout);
        }

        RegisterComponents();
    }

    private void InitializeWeights(LoraInitMethod method, float scale)
    {
        switch (method)
        {
            case LoraInitMethod.KaimingUniform:
                // Standard LoRA initialization: Kaiming for A, zeros for B
                init.kaiming_uniform_(_loraA.weight!, a: Math.Sqrt(5));
                if (scale != 1.0f)
                {
                    using (no_grad())
                    {
                        _loraA.weight!.mul_(scale);
                    }
                }
                init.zeros_(_loraB.weight!);
                break;

            case LoraInitMethod.Gaussian:
                // Gaussian initialization for A
                init.normal_(_loraA.weight!, mean: 0, std: 1.0 / Rank);
                if (scale != 1.0f)
                {
                    using (no_grad())
                    {
                        _loraA.weight!.mul_(scale);
                    }
                }
                init.zeros_(_loraB.weight!);
                break;

            case LoraInitMethod.Zeros:
                // Both zero - effectively disabled at start
                init.zeros_(_loraA.weight!);
                init.zeros_(_loraB.weight!);
                break;
        }
    }

    /// <summary>
    /// Forward pass: y = W₀x + (α/r)(B(A(dropout(x))))
    /// </summary>
    public override Tensor forward(Tensor input)
    {
        // If merged or disabled, just use original layer
        if (_merged || _disabled)
        {
            return _originalLayer.forward(input);
        }

        // Original layer output
        var originalOutput = _originalLayer.forward(input);

        // LoRA path
        var loraInput = _dropout?.forward(input) ?? input;
        var loraOutput = _loraB.forward(_loraA.forward(loraInput));

        // Combine: original + scaled LoRA
        return originalOutput + (loraOutput * _scaling);
    }

    /// <summary>
    /// Merge LoRA weights into the original layer for efficient inference
    /// After merging: W_effective = W₀ + (α/r)BA
    /// </summary>
    public void Merge()
    {
        if (_merged)
        {
            return;
        }

        using (no_grad())
        {
            // Compute delta: (α/r) * B @ A
            // A is [rank, in_features], B is [out_features, rank]
            // Result is [out_features, in_features]
            var delta = mm(_loraB.weight!, _loraA.weight!) * _scaling;

            // Add to original weights
            _originalLayer.weight!.add_(delta);
        }

        _merged = true;
    }

    /// <summary>
    /// Unmerge LoRA weights from the original layer (for continued training)
    /// </summary>
    public void Unmerge()
    {
        if (!_merged)
        {
            return;
        }

        using (no_grad())
        {
            // Compute and subtract delta
            var delta = mm(_loraB.weight!, _loraA.weight!) * _scaling;
            _originalLayer.weight!.sub_(delta);
        }

        _merged = false;
    }

    /// <summary>
    /// Disable LoRA (pass-through to original layer)
    /// </summary>
    public void Disable()
    {
        _disabled = true;
    }

    /// <summary>
    /// Enable LoRA
    /// </summary>
    public void Enable()
    {
        _disabled = false;
    }

    /// <summary>
    /// Get LoRA weights for saving
    /// </summary>
    public LoraLayerWeights GetWeights()
    {
        return new LoraLayerWeights
        {
            Name = _name,
            LoraA = _loraA.weight!.clone().cpu(),
            LoraB = _loraB.weight!.clone().cpu(),
            Rank = Rank,
            Alpha = Alpha,
            InFeatures = InFeatures,
            OutFeatures = OutFeatures
        };
    }

    /// <summary>
    /// Load LoRA weights
    /// </summary>
    public void LoadWeights(LoraLayerWeights weights)
    {
        if (weights.Rank != Rank)
        {
            throw new ArgumentException(
                $"Rank mismatch: expected {Rank}, got {weights.Rank}");
        }

        if (weights.InFeatures != InFeatures || weights.OutFeatures != OutFeatures)
        {
            throw new ArgumentException(
                $"Dimension mismatch: expected ({InFeatures}, {OutFeatures}), " +
                $"got ({weights.InFeatures}, {weights.OutFeatures})");
        }

        using (no_grad())
        {
            _loraA.weight!.copy_(weights.LoraA.to(_loraA.weight!.device));
            _loraB.weight!.copy_(weights.LoraB.to(_loraB.weight!.device));
        }
    }

    /// <summary>
    /// Reset LoRA weights to initial state
    /// </summary>
    public void Reset(LoraInitMethod method = LoraInitMethod.KaimingUniform)
    {
        if (_merged)
        {
            Unmerge();
        }

        InitializeWeights(method, 1.0f);
    }

    /// <summary>
    /// Get only the trainable parameters (LoRA A and B weights)
    /// </summary>
    public IEnumerable<Parameter> GetTrainableParameters()
    {
        yield return _loraA.weight!;
        yield return _loraB.weight!;
    }
}

/// <summary>
/// Container for LoRA layer weights (for serialization)
/// </summary>
public sealed class LoraLayerWeights
{
    public string Name { get; init; } = string.Empty;
    public Tensor LoraA { get; init; } = null!;
    public Tensor LoraB { get; init; } = null!;
    public int Rank { get; init; }
    public float Alpha { get; init; }
    public long InFeatures { get; init; }
    public long OutFeatures { get; init; }
}
```

#### 3. Services/Training/LoraLayerFactory.cs

```csharp
namespace SeniorIntern.Services.Training;

/// <summary>
/// Factory for creating LoRA layers with consistent configuration
/// </summary>
public sealed class LoraLayerFactory
{
    private readonly LoraConfiguration _config;
    private readonly ILogger<LoraLayerFactory> _logger;

    private int _layersCreated;
    private long _totalTrainableParams;

    public int LayersCreated => _layersCreated;
    public long TotalTrainableParameters => _totalTrainableParams;

    public LoraLayerFactory(
        LoraConfiguration config,
        ILogger<LoraLayerFactory> logger)
    {
        _config = config;
        _logger = logger;
    }

    /// <summary>
    /// Create a LoRA layer wrapping the given linear layer
    /// </summary>
    public LoraLayer Create(torch.nn.Linear originalLayer, string name)
    {
        var loraLayer = new LoraLayer(
            originalLayer,
            rank: _config.Rank,
            alpha: _config.Alpha,
            dropout: _config.Dropout,
            initMethod: _config.InitMethod,
            initScale: _config.InitScale,
            name: name);

        _layersCreated++;
        _totalTrainableParams += loraLayer.TrainableParameters;

        _logger.LogDebug(
            "Created LoRA layer '{Name}': [{In} → {Rank} → {Out}], params: {Params}",
            name, loraLayer.InFeatures, _config.Rank, loraLayer.OutFeatures,
            loraLayer.TrainableParameters);

        return loraLayer;
    }

    /// <summary>
    /// Check if a module name should have LoRA applied
    /// </summary>
    public bool ShouldApplyLora(string moduleName)
    {
        return _config.TargetModules.Any(target =>
            moduleName.EndsWith(target, StringComparison.OrdinalIgnoreCase) ||
            moduleName.Contains($".{target}.", StringComparison.OrdinalIgnoreCase) ||
            moduleName.Contains($".{target}", StringComparison.OrdinalIgnoreCase));
    }

    /// <summary>
    /// Get summary of created layers
    /// </summary>
    public LoraLayerSummary GetSummary()
    {
        return new LoraLayerSummary
        {
            TotalLayers = _layersCreated,
            TotalTrainableParameters = _totalTrainableParams,
            Rank = _config.Rank,
            Alpha = _config.Alpha,
            TargetModules = _config.TargetModules.ToList()
        };
    }
}

/// <summary>
/// Summary of LoRA layer creation
/// </summary>
public sealed class LoraLayerSummary
{
    public int TotalLayers { get; init; }
    public long TotalTrainableParameters { get; init; }
    public int Rank { get; init; }
    public float Alpha { get; init; }
    public List<string> TargetModules { get; init; } = new();

    public string FormattedParameters => TotalTrainableParameters switch
    {
        < 1_000 => $"{TotalTrainableParameters}",
        < 1_000_000 => $"{TotalTrainableParameters / 1_000.0:F1}K",
        < 1_000_000_000 => $"{TotalTrainableParameters / 1_000_000.0:F1}M",
        _ => $"{TotalTrainableParameters / 1_000_000_000.0:F2}B"
    };
}
```

### v0.8.3a File Summary

| File | Purpose | Lines (Est.) |
|------|---------|--------------|
| `Core/Training/LoraConfiguration.cs` | LoRA configuration and enums | ~110 |
| `Services/Training/LoraLayer.cs` | Core LoRA layer implementation | ~280 |
| `Services/Training/LoraLayerFactory.cs` | Factory for creating LoRA layers | ~100 |

**v0.8.3a Total: 3 files to create**

---

## v0.8.3b: LoRA Model Wrapper

### Objective
Create a model wrapper that applies LoRA adapters to target modules in a base model, manages training/eval modes, and provides access to trainable parameters.

### Files to Create

#### 1. Services/Training/LoraModel.cs

```csharp
namespace SeniorIntern.Services.Training;

using TorchSharp;
using static TorchSharp.torch;
using static TorchSharp.torch.nn;

/// <summary>
/// Wrapper that applies LoRA adapters to a base model
/// </summary>
public sealed class LoraModel : Module<Tensor, Tensor, Tensor>
{
    private readonly Module _baseModel;
    private readonly Dictionary<string, LoraLayer> _loraLayers;
    private readonly LoraConfiguration _config;
    private readonly ILogger<LoraModel> _logger;

    private bool _trainingMode = true;
    private bool _merged = false;

    /// <summary>Number of LoRA layers applied</summary>
    public int LoraLayerCount => _loraLayers.Count;

    /// <summary>Total trainable parameters (LoRA only)</summary>
    public long TrainableParameterCount { get; private set; }

    /// <summary>Total frozen parameters (base model)</summary>
    public long FrozenParameterCount { get; private set; }

    /// <summary>Whether LoRA weights are merged</summary>
    public bool IsMerged => _merged;

    /// <summary>LoRA configuration used</summary>
    public LoraConfiguration Configuration => _config;

    public LoraModel(
        Module baseModel,
        LoraConfiguration config,
        ILogger<LoraModel> logger)
        : base(nameof(LoraModel))
    {
        _baseModel = baseModel;
        _config = config;
        _logger = logger;
        _loraLayers = new Dictionary<string, LoraLayer>();

        ApplyLoraAdapters();
        CountParameters();
        RegisterComponents();

        _logger.LogInformation(
            "Created LoRA model with {LoraLayers} adapters, " +
            "{Trainable} trainable params, {Frozen} frozen params",
            _loraLayers.Count,
            FormatParameterCount(TrainableParameterCount),
            FormatParameterCount(FrozenParameterCount));
    }

    private void ApplyLoraAdapters()
    {
        var factory = new LoraLayerFactory(_config,
            LoggerFactory.Create(b => b.AddConsole()).CreateLogger<LoraLayerFactory>());

        // Find all linear layers matching target modules
        foreach (var (name, module) in _baseModel.named_modules())
        {
            if (module is Linear linear && factory.ShouldApplyLora(name))
            {
                var loraLayer = factory.Create(linear, name);
                _loraLayers[name] = loraLayer;

                // Replace the original module with LoRA-wrapped version
                ReplaceModule(name, loraLayer);
            }
        }

        if (_loraLayers.Count == 0)
        {
            _logger.LogWarning(
                "No LoRA layers created! Check target modules: {Targets}",
                string.Join(", ", _config.TargetModules));
        }
    }

    private void ReplaceModule(string fullName, Module newModule)
    {
        // Split path: "model.layers.0.self_attn.q_proj" -> ["model", "layers", "0", "self_attn", "q_proj"]
        var parts = fullName.Split('.');
        var current = _baseModel;

        // Navigate to parent
        for (int i = 0; i < parts.Length - 1; i++)
        {
            var child = GetChildModule(current, parts[i]);
            if (child == null)
            {
                _logger.LogWarning("Could not find module path: {Path}", fullName);
                return;
            }
            current = child;
        }

        // Replace the final module
        var lastPart = parts[^1];
        SetChildModule(current, lastPart, newModule);
    }

    private static Module? GetChildModule(Module parent, string name)
    {
        // Try as named child
        foreach (var (childName, child) in parent.named_children())
        {
            if (childName == name)
                return child;
        }

        // Try as indexed (for ModuleList)
        if (int.TryParse(name, out var index))
        {
            var children = parent.children().ToList();
            if (index >= 0 && index < children.Count)
                return children[index];
        }

        return null;
    }

    private static void SetChildModule(Module parent, string name, Module newModule)
    {
        // Use reflection to set the module
        // TorchSharp modules store children in internal dictionaries
        var field = parent.GetType().GetField($"_{name}",
            System.Reflection.BindingFlags.NonPublic | System.Reflection.BindingFlags.Instance);

        if (field != null)
        {
            field.SetValue(parent, newModule);
        }
        else
        {
            // Try the register approach
            parent.register_module(name, newModule);
        }
    }

    private void CountParameters()
    {
        TrainableParameterCount = 0;
        FrozenParameterCount = 0;

        foreach (var (_, param) in _baseModel.named_parameters())
        {
            var count = param.numel();
            if (param.requires_grad)
                TrainableParameterCount += count;
            else
                FrozenParameterCount += count;
        }

        // Add LoRA parameters
        foreach (var loraLayer in _loraLayers.Values)
        {
            TrainableParameterCount += loraLayer.TrainableParameters;
        }
    }

    /// <summary>
    /// Forward pass through the LoRA-adapted model
    /// </summary>
    /// <param name="inputIds">Input token IDs [batch, seq_len]</param>
    /// <param name="attentionMask">Attention mask [batch, seq_len]</param>
    /// <returns>Logits [batch, seq_len, vocab_size]</returns>
    public override Tensor forward(Tensor inputIds, Tensor attentionMask)
    {
        // Call base model forward
        // The LoRA layers are already injected, so they're called automatically
        return _baseModel.call(inputIds, attentionMask);
    }

    /// <summary>
    /// Get only trainable LoRA parameters for optimizer
    /// </summary>
    public IEnumerable<Parameter> GetLoraParameters()
    {
        foreach (var loraLayer in _loraLayers.Values)
        {
            foreach (var param in loraLayer.GetTrainableParameters())
            {
                yield return param;
            }
        }
    }

    /// <summary>
    /// Get all LoRA layer weights for saving
    /// </summary>
    public Dictionary<string, LoraLayerWeights> GetAllLoraWeights()
    {
        return _loraLayers.ToDictionary(
            kvp => kvp.Key,
            kvp => kvp.Value.GetWeights());
    }

    /// <summary>
    /// Load LoRA weights from a dictionary
    /// </summary>
    public void LoadLoraWeights(Dictionary<string, LoraLayerWeights> weights)
    {
        foreach (var (name, layerWeights) in weights)
        {
            if (_loraLayers.TryGetValue(name, out var loraLayer))
            {
                loraLayer.LoadWeights(layerWeights);
            }
            else
            {
                _logger.LogWarning("LoRA layer not found for weights: {Name}", name);
            }
        }
    }

    /// <summary>
    /// Merge all LoRA weights into base model (for efficient inference)
    /// </summary>
    public void MergeLoraWeights()
    {
        if (_merged) return;

        foreach (var loraLayer in _loraLayers.Values)
        {
            loraLayer.Merge();
        }

        _merged = true;
        _logger.LogInformation("Merged {Count} LoRA layers into base model", _loraLayers.Count);
    }

    /// <summary>
    /// Unmerge LoRA weights (for continued training)
    /// </summary>
    public void UnmergeLoraWeights()
    {
        if (!_merged) return;

        foreach (var loraLayer in _loraLayers.Values)
        {
            loraLayer.Unmerge();
        }

        _merged = false;
        _logger.LogInformation("Unmerged {Count} LoRA layers from base model", _loraLayers.Count);
    }

    /// <summary>
    /// Set training mode
    /// </summary>
    public new LoraModel train(bool mode = true)
    {
        _trainingMode = mode;
        base.train(mode);

        // Ensure base model stays in eval mode (we only train LoRA)
        _baseModel.eval();

        // But enable dropout in LoRA layers during training
        foreach (var loraLayer in _loraLayers.Values)
        {
            loraLayer.train(mode);
        }

        return this;
    }

    /// <summary>
    /// Set evaluation mode
    /// </summary>
    public new LoraModel eval()
    {
        return train(false);
    }

    /// <summary>
    /// Disable all LoRA layers (use only base model)
    /// </summary>
    public void DisableLora()
    {
        foreach (var loraLayer in _loraLayers.Values)
        {
            loraLayer.Disable();
        }
    }

    /// <summary>
    /// Enable all LoRA layers
    /// </summary>
    public void EnableLora()
    {
        foreach (var loraLayer in _loraLayers.Values)
        {
            loraLayer.Enable();
        }
    }

    /// <summary>
    /// Reset all LoRA weights to initial state
    /// </summary>
    public void ResetLoraWeights()
    {
        foreach (var loraLayer in _loraLayers.Values)
        {
            loraLayer.Reset();
        }
    }

    /// <summary>
    /// Move model to device
    /// </summary>
    public LoraModel ToDevice(Device device)
    {
        _baseModel.to(device);
        foreach (var loraLayer in _loraLayers.Values)
        {
            loraLayer.to(device);
        }
        return this;
    }

    /// <summary>
    /// Get model information
    /// </summary>
    public LoraModelInfo GetInfo()
    {
        return new LoraModelInfo
        {
            LoraLayerCount = _loraLayers.Count,
            TrainableParameters = TrainableParameterCount,
            FrozenParameters = FrozenParameterCount,
            Rank = _config.Rank,
            Alpha = _config.Alpha,
            TargetModules = _config.TargetModules.ToList(),
            LayerNames = _loraLayers.Keys.ToList(),
            IsMerged = _merged
        };
    }

    private static string FormatParameterCount(long count)
    {
        return count switch
        {
            < 1_000 => $"{count}",
            < 1_000_000 => $"{count / 1_000.0:F1}K",
            < 1_000_000_000 => $"{count / 1_000_000.0:F1}M",
            _ => $"{count / 1_000_000_000.0:F2}B"
        };
    }
}

/// <summary>
/// Information about a LoRA model
/// </summary>
public sealed class LoraModelInfo
{
    public int LoraLayerCount { get; init; }
    public long TrainableParameters { get; init; }
    public long FrozenParameters { get; init; }
    public int Rank { get; init; }
    public float Alpha { get; init; }
    public List<string> TargetModules { get; init; } = new();
    public List<string> LayerNames { get; init; } = new();
    public bool IsMerged { get; init; }

    public double TrainableRatio =>
        (TrainableParameters + FrozenParameters) > 0
            ? (double)TrainableParameters / (TrainableParameters + FrozenParameters)
            : 0;

    public string TrainableParametersFormatted => TrainableParameters switch
    {
        < 1_000 => $"{TrainableParameters}",
        < 1_000_000 => $"{TrainableParameters / 1_000.0:F1}K",
        < 1_000_000_000 => $"{TrainableParameters / 1_000_000.0:F1}M",
        _ => $"{TrainableParameters / 1_000_000_000.0:F2}B"
    };
}
```

#### 2. Services/Training/BaseModelLoader.cs

```csharp
namespace SeniorIntern.Services.Training;

using LLama;
using LLama.Common;

/// <summary>
/// Loads base models for LoRA training
/// </summary>
public sealed class BaseModelLoader
{
    private readonly ILogger<BaseModelLoader> _logger;

    public BaseModelLoader(ILogger<BaseModelLoader> logger)
    {
        _logger = logger;
    }

    /// <summary>
    /// Load a base model from a GGUF file
    /// </summary>
    public async Task<LoadedModel> LoadModelAsync(
        string modelPath,
        ModelLoadOptions options,
        CancellationToken ct = default)
    {
        _logger.LogInformation("Loading base model from {Path}", modelPath);
        var stopwatch = Stopwatch.StartNew();

        if (!File.Exists(modelPath))
        {
            throw new FileNotFoundException($"Model file not found: {modelPath}");
        }

        var modelParams = new ModelParams(modelPath)
        {
            ContextSize = options.ContextSize,
            GpuLayerCount = options.GpuLayers,
            UseMemorymap = options.UseMemoryMap,
            UseMemoryLock = options.UseMemoryLock,
            Seed = options.Seed ?? 0
        };

        var model = await LLamaWeights.LoadFromFileAsync(modelParams, ct);

        stopwatch.Stop();

        var info = new LoadedModelInfo
        {
            ModelPath = modelPath,
            VocabSize = model.VocabCount,
            ContextSize = options.ContextSize,
            GpuLayers = options.GpuLayers,
            LoadDuration = stopwatch.Elapsed,
            ModelSizeBytes = new FileInfo(modelPath).Length
        };

        _logger.LogInformation(
            "Model loaded in {Duration:F1}s: vocab={Vocab}, context={Context}",
            stopwatch.Elapsed.TotalSeconds, info.VocabSize, info.ContextSize);

        return new LoadedModel
        {
            Weights = model,
            Info = info,
            ModelParams = modelParams
        };
    }

    /// <summary>
    /// Get model information without fully loading
    /// </summary>
    public async Task<ModelMetadata> GetModelMetadataAsync(
        string modelPath,
        CancellationToken ct = default)
    {
        // Quick load with minimal context
        var modelParams = new ModelParams(modelPath)
        {
            ContextSize = 128,
            GpuLayerCount = 0
        };

        using var model = await LLamaWeights.LoadFromFileAsync(modelParams, ct);

        return new ModelMetadata
        {
            ModelPath = modelPath,
            VocabSize = model.VocabCount,
            ModelSizeBytes = new FileInfo(modelPath).Length,
            FileHash = await ComputeFileHashAsync(modelPath, ct)
        };
    }

    private static async Task<string> ComputeFileHashAsync(string path, CancellationToken ct)
    {
        using var sha256 = System.Security.Cryptography.SHA256.Create();
        await using var stream = File.OpenRead(path);

        // Only hash first 1MB for speed
        var buffer = new byte[1024 * 1024];
        var bytesRead = await stream.ReadAsync(buffer, ct);

        var hash = sha256.ComputeHash(buffer, 0, bytesRead);
        return Convert.ToHexString(hash)[..16];
    }
}

/// <summary>
/// Options for loading a base model
/// </summary>
public sealed class ModelLoadOptions
{
    public int ContextSize { get; init; } = 2048;
    public int GpuLayers { get; init; } = -1; // -1 = auto
    public bool UseMemoryMap { get; init; } = true;
    public bool UseMemoryLock { get; init; } = false;
    public int? Seed { get; init; }
}

/// <summary>
/// Loaded model container
/// </summary>
public sealed class LoadedModel : IDisposable
{
    public LLamaWeights Weights { get; init; } = null!;
    public LoadedModelInfo Info { get; init; } = null!;
    public ModelParams ModelParams { get; init; } = null!;

    public void Dispose()
    {
        Weights?.Dispose();
    }
}

/// <summary>
/// Information about a loaded model
/// </summary>
public sealed class LoadedModelInfo
{
    public string ModelPath { get; init; } = string.Empty;
    public int VocabSize { get; init; }
    public int ContextSize { get; init; }
    public int GpuLayers { get; init; }
    public TimeSpan LoadDuration { get; init; }
    public long ModelSizeBytes { get; init; }

    public string FormattedSize => ModelSizeBytes switch
    {
        < 1024 => $"{ModelSizeBytes} B",
        < 1024 * 1024 => $"{ModelSizeBytes / 1024.0:F1} KB",
        < 1024 * 1024 * 1024 => $"{ModelSizeBytes / (1024.0 * 1024):F1} MB",
        _ => $"{ModelSizeBytes / (1024.0 * 1024 * 1024):F2} GB"
    };
}

/// <summary>
/// Model metadata (for quick inspection)
/// </summary>
public sealed class ModelMetadata
{
    public string ModelPath { get; init; } = string.Empty;
    public int VocabSize { get; init; }
    public long ModelSizeBytes { get; init; }
    public string FileHash { get; init; } = string.Empty;
}
```

### v0.8.3b File Summary

| File | Purpose | Lines (Est.) |
|------|---------|--------------|
| `Services/Training/LoraModel.cs` | LoRA model wrapper | ~350 |
| `Services/Training/BaseModelLoader.cs` | Base model loading | ~150 |

**v0.8.3b Total: 2 files to create**

---

## v0.8.3c: Training Job & Progress Models

### Objective
Define the data models for training jobs, progress reporting, metrics tracking, and training results.

### Files to Create

#### 1. Core/Training/TrainingJob.cs

```csharp
namespace SeniorIntern.Core.Training;

/// <summary>
/// Represents a training job to be executed
/// </summary>
public sealed class TrainingJob
{
    /// <summary>Unique identifier for this job</summary>
    public Guid Id { get; init; } = Guid.NewGuid();

    /// <summary>Human-readable name for this job</summary>
    public string Name { get; init; } = string.Empty;

    /// <summary>Optional description</summary>
    public string? Description { get; init; }

    /// <summary>Path to the base model (GGUF file)</summary>
    public string BaseModelPath { get; init; } = string.Empty;

    /// <summary>Path to the training dataset</summary>
    public string DatasetPath { get; init; } = string.Empty;

    /// <summary>Output directory for checkpoints and final adapter</summary>
    public string OutputDir { get; init; } = string.Empty;

    /// <summary>Training configuration</summary>
    public TrainingConfiguration Configuration { get; init; } = new();

    /// <summary>LoRA-specific configuration</summary>
    public LoraConfiguration LoraConfig { get; init; } = new();

    /// <summary>When this job was created</summary>
    public DateTimeOffset CreatedAt { get; init; } = DateTimeOffset.UtcNow;

    /// <summary>Optional tags for organization</summary>
    public Dictionary<string, string> Tags { get; init; } = new();

    /// <summary>
    /// Validate the job configuration
    /// </summary>
    public TrainingJobValidationResult Validate()
    {
        var errors = new List<string>();
        var warnings = new List<string>();

        // Required fields
        if (string.IsNullOrWhiteSpace(Name))
            errors.Add("Job name is required");

        if (string.IsNullOrWhiteSpace(BaseModelPath))
            errors.Add("Base model path is required");
        else if (!File.Exists(BaseModelPath))
            errors.Add($"Base model not found: {BaseModelPath}");

        if (string.IsNullOrWhiteSpace(DatasetPath))
            errors.Add("Dataset path is required");
        else if (!File.Exists(DatasetPath) && !Directory.Exists(DatasetPath))
            errors.Add($"Dataset not found: {DatasetPath}");

        if (string.IsNullOrWhiteSpace(OutputDir))
            errors.Add("Output directory is required");

        // Configuration validation
        if (Configuration.Epochs < 1)
            errors.Add("Epochs must be at least 1");

        if (Configuration.BatchSize < 1)
            errors.Add("Batch size must be at least 1");

        if (Configuration.LearningRate <= 0)
            errors.Add("Learning rate must be positive");

        if (LoraConfig.Rank < 1 || LoraConfig.Rank > 256)
            errors.Add("LoRA rank must be between 1 and 256");

        // Warnings
        if (Configuration.Epochs > 20)
            warnings.Add("More than 20 epochs may cause overfitting");

        if (Configuration.LearningRate > 1e-3)
            warnings.Add("Learning rate is unusually high for fine-tuning");

        if (LoraConfig.Rank > 64)
            warnings.Add("High LoRA rank (>64) increases memory usage significantly");

        return new TrainingJobValidationResult
        {
            IsValid = errors.Count == 0,
            Errors = errors,
            Warnings = warnings
        };
    }
}

/// <summary>
/// Result of training job validation
/// </summary>
public sealed class TrainingJobValidationResult
{
    public bool IsValid { get; init; }
    public IReadOnlyList<string> Errors { get; init; } = Array.Empty<string>();
    public IReadOnlyList<string> Warnings { get; init; } = Array.Empty<string>();
}
```

#### 2. Core/Training/TrainingProgress.cs

```csharp
namespace SeniorIntern.Core.Training;

/// <summary>
/// Current training progress information
/// </summary>
public sealed class TrainingProgress
{
    /// <summary>Current epoch (1-based)</summary>
    public int CurrentEpoch { get; init; }

    /// <summary>Total epochs to train</summary>
    public int TotalEpochs { get; init; }

    /// <summary>Current step within epoch</summary>
    public int CurrentStep { get; init; }

    /// <summary>Total steps per epoch</summary>
    public int StepsPerEpoch { get; init; }

    /// <summary>Global step across all epochs</summary>
    public int GlobalStep { get; init; }

    /// <summary>Total global steps</summary>
    public int TotalSteps { get; init; }

    /// <summary>Samples processed so far</summary>
    public int SamplesProcessed { get; init; }

    /// <summary>Total samples in dataset</summary>
    public int TotalSamples { get; init; }

    /// <summary>Overall completion percentage</summary>
    public double PercentComplete => TotalSteps > 0
        ? (double)GlobalStep / TotalSteps * 100
        : 0;

    /// <summary>Current epoch completion percentage</summary>
    public double EpochPercentComplete => StepsPerEpoch > 0
        ? (double)CurrentStep / StepsPerEpoch * 100
        : 0;

    /// <summary>Current training metrics</summary>
    public TrainingMetrics CurrentMetrics { get; init; } = new();

    /// <summary>Hardware resource usage</summary>
    public HardwareMetrics HardwareMetrics { get; init; } = new();

    /// <summary>Time elapsed since training started</summary>
    public TimeSpan Elapsed { get; init; }

    /// <summary>Estimated time remaining</summary>
    public TimeSpan? EstimatedRemaining { get; init; }

    /// <summary>Estimated completion time</summary>
    public DateTimeOffset? EstimatedCompletion => EstimatedRemaining.HasValue
        ? DateTimeOffset.Now + EstimatedRemaining.Value
        : null;

    /// <summary>Current operation description</summary>
    public string CurrentOperation { get; init; } = string.Empty;

    /// <summary>Samples per second (throughput)</summary>
    public double SamplesPerSecond => Elapsed.TotalSeconds > 0
        ? SamplesProcessed / Elapsed.TotalSeconds
        : 0;

    /// <summary>Tokens per second (if available)</summary>
    public double? TokensPerSecond { get; init; }
}

/// <summary>
/// Training metrics at a point in time
/// </summary>
public sealed class TrainingMetrics
{
    /// <summary>Step number</summary>
    public int Step { get; init; }

    /// <summary>Epoch number</summary>
    public int Epoch { get; init; }

    /// <summary>Training loss</summary>
    public float Loss { get; init; }

    /// <summary>Validation loss (if available)</summary>
    public float? ValidationLoss { get; init; }

    /// <summary>Current learning rate</summary>
    public float LearningRate { get; init; }

    /// <summary>Gradient norm (for monitoring)</summary>
    public float GradientNorm { get; init; }

    /// <summary>Perplexity (exp(loss))</summary>
    public float Perplexity => MathF.Exp(Math.Min(Loss, 20f)); // Cap to prevent overflow

    /// <summary>Validation perplexity</summary>
    public float? ValidationPerplexity => ValidationLoss.HasValue
        ? MathF.Exp(Math.Min(ValidationLoss.Value, 20f))
        : null;

    /// <summary>When these metrics were recorded</summary>
    public DateTimeOffset Timestamp { get; init; } = DateTimeOffset.UtcNow;

    /// <summary>Additional custom metrics</summary>
    public Dictionary<string, float>? CustomMetrics { get; init; }
}

/// <summary>
/// Hardware resource usage metrics
/// </summary>
public sealed class HardwareMetrics
{
    /// <summary>GPU utilization percentage (0-100)</summary>
    public float GpuUtilization { get; init; }

    /// <summary>GPU memory used in bytes</summary>
    public long GpuMemoryUsed { get; init; }

    /// <summary>GPU total memory in bytes</summary>
    public long GpuMemoryTotal { get; init; }

    /// <summary>GPU memory utilization percentage</summary>
    public float GpuMemoryUtilization => GpuMemoryTotal > 0
        ? (float)GpuMemoryUsed / GpuMemoryTotal * 100
        : 0;

    /// <summary>GPU temperature in Celsius</summary>
    public float GpuTemperature { get; init; }

    /// <summary>CPU utilization percentage (0-100)</summary>
    public float CpuUtilization { get; init; }

    /// <summary>System RAM used in bytes</summary>
    public long RamUsed { get; init; }

    /// <summary>System RAM total in bytes</summary>
    public long RamTotal { get; init; }

    /// <summary>RAM utilization percentage</summary>
    public float RamUtilization => RamTotal > 0
        ? (float)RamUsed / RamTotal * 100
        : 0;

    /// <summary>Formatted GPU memory used</summary>
    public string GpuMemoryUsedFormatted => FormatBytes(GpuMemoryUsed);

    /// <summary>Formatted GPU memory total</summary>
    public string GpuMemoryTotalFormatted => FormatBytes(GpuMemoryTotal);

    private static string FormatBytes(long bytes) => bytes switch
    {
        < 1024 => $"{bytes} B",
        < 1024 * 1024 => $"{bytes / 1024.0:F1} KB",
        < 1024 * 1024 * 1024 => $"{bytes / (1024.0 * 1024):F1} MB",
        _ => $"{bytes / (1024.0 * 1024 * 1024):F2} GB"
    };
}
```

#### 3. Core/Training/TrainingResult.cs

```csharp
namespace SeniorIntern.Core.Training;

/// <summary>
/// Result of a completed training run
/// </summary>
public sealed class TrainingResult
{
    /// <summary>Job ID that produced this result</summary>
    public Guid JobId { get; init; }

    /// <summary>Whether training completed successfully</summary>
    public bool Success { get; init; }

    /// <summary>Path to the saved adapter (if successful)</summary>
    public string? AdapterPath { get; init; }

    /// <summary>Final training metrics</summary>
    public TrainingMetrics FinalMetrics { get; init; } = new();

    /// <summary>Best validation metrics achieved</summary>
    public TrainingMetrics? BestMetrics { get; init; }

    /// <summary>Total training duration</summary>
    public TimeSpan TotalDuration { get; init; }

    /// <summary>Total training steps completed</summary>
    public int TotalSteps { get; init; }

    /// <summary>Total epochs completed</summary>
    public int TotalEpochs { get; init; }

    /// <summary>Total samples processed</summary>
    public long TotalSamplesProcessed { get; init; }

    /// <summary>Error message if failed</summary>
    public string? ErrorMessage { get; init; }

    /// <summary>Stack trace if failed</summary>
    public string? ErrorStackTrace { get; init; }

    /// <summary>List of saved checkpoints</summary>
    public IReadOnlyList<CheckpointInfo> Checkpoints { get; init; } = Array.Empty<CheckpointInfo>();

    /// <summary>Full metrics history</summary>
    public IReadOnlyList<TrainingMetrics> MetricsHistory { get; init; } = Array.Empty<TrainingMetrics>();

    /// <summary>Training configuration used</summary>
    public TrainingConfiguration Configuration { get; init; } = new();

    /// <summary>LoRA configuration used</summary>
    public LoraConfiguration LoraConfiguration { get; init; } = new();

    /// <summary>Training completion status</summary>
    public TrainingCompletionStatus Status { get; init; }

    /// <summary>When training started</summary>
    public DateTimeOffset StartedAt { get; init; }

    /// <summary>When training ended</summary>
    public DateTimeOffset EndedAt { get; init; }

    /// <summary>Average samples per second</summary>
    public double AverageSamplesPerSecond => TotalDuration.TotalSeconds > 0
        ? TotalSamplesProcessed / TotalDuration.TotalSeconds
        : 0;

    /// <summary>
    /// Create a success result
    /// </summary>
    public static TrainingResult CreateSuccess(
        Guid jobId,
        string adapterPath,
        TrainingMetrics finalMetrics,
        TrainingMetrics? bestMetrics,
        TimeSpan duration,
        int steps,
        int epochs,
        IReadOnlyList<TrainingMetrics> history,
        TrainingConfiguration config,
        LoraConfiguration loraConfig)
    {
        return new TrainingResult
        {
            JobId = jobId,
            Success = true,
            AdapterPath = adapterPath,
            FinalMetrics = finalMetrics,
            BestMetrics = bestMetrics,
            TotalDuration = duration,
            TotalSteps = steps,
            TotalEpochs = epochs,
            MetricsHistory = history,
            Configuration = config,
            LoraConfiguration = loraConfig,
            Status = TrainingCompletionStatus.Completed,
            StartedAt = DateTimeOffset.UtcNow - duration,
            EndedAt = DateTimeOffset.UtcNow
        };
    }

    /// <summary>
    /// Create a failure result
    /// </summary>
    public static TrainingResult CreateFailure(
        Guid jobId,
        Exception exception,
        TimeSpan duration,
        IReadOnlyList<TrainingMetrics>? history = null)
    {
        return new TrainingResult
        {
            JobId = jobId,
            Success = false,
            ErrorMessage = exception.Message,
            ErrorStackTrace = exception.StackTrace,
            TotalDuration = duration,
            MetricsHistory = history ?? Array.Empty<TrainingMetrics>(),
            Status = TrainingCompletionStatus.Failed,
            StartedAt = DateTimeOffset.UtcNow - duration,
            EndedAt = DateTimeOffset.UtcNow
        };
    }

    /// <summary>
    /// Create a cancelled result
    /// </summary>
    public static TrainingResult CreateCancelled(
        Guid jobId,
        TimeSpan duration,
        int stepsCompleted,
        IReadOnlyList<TrainingMetrics>? history = null)
    {
        return new TrainingResult
        {
            JobId = jobId,
            Success = false,
            ErrorMessage = "Training was cancelled by user",
            TotalDuration = duration,
            TotalSteps = stepsCompleted,
            MetricsHistory = history ?? Array.Empty<TrainingMetrics>(),
            Status = TrainingCompletionStatus.Cancelled,
            StartedAt = DateTimeOffset.UtcNow - duration,
            EndedAt = DateTimeOffset.UtcNow
        };
    }
}

/// <summary>
/// Information about a saved checkpoint
/// </summary>
public sealed class CheckpointInfo
{
    public string Path { get; init; } = string.Empty;
    public int Step { get; init; }
    public int Epoch { get; init; }
    public float Loss { get; init; }
    public float? ValidationLoss { get; init; }
    public DateTimeOffset CreatedAt { get; init; }
    public long SizeBytes { get; init; }
    public bool IsBest { get; init; }
}

/// <summary>
/// Training completion status
/// </summary>
public enum TrainingCompletionStatus
{
    /// <summary>Training completed all epochs</summary>
    Completed,

    /// <summary>Training was cancelled by user</summary>
    Cancelled,

    /// <summary>Training failed with an error</summary>
    Failed,

    /// <summary>Training stopped early (early stopping)</summary>
    EarlyStopped,

    /// <summary>Training is still in progress</summary>
    InProgress
}
```

#### 4. Core/Training/TrainingState.cs

```csharp
namespace SeniorIntern.Core.Training;

/// <summary>
/// Current state of the training service
/// </summary>
public enum TrainingState
{
    /// <summary>No training in progress</summary>
    Idle,

    /// <summary>Preparing for training (loading model, dataset)</summary>
    Preparing,

    /// <summary>Actively training</summary>
    Training,

    /// <summary>Training is paused</summary>
    Paused,

    /// <summary>Running validation evaluation</summary>
    Evaluating,

    /// <summary>Saving checkpoint or final adapter</summary>
    Saving,

    /// <summary>Training completed successfully</summary>
    Completed,

    /// <summary>Training failed with error</summary>
    Failed,

    /// <summary>Training was cancelled</summary>
    Cancelled
}

/// <summary>
/// Event args for training state changes
/// </summary>
public sealed class TrainingStateChangedEventArgs : EventArgs
{
    public TrainingState PreviousState { get; init; }
    public TrainingState NewState { get; init; }
    public string? Message { get; init; }
    public DateTimeOffset Timestamp { get; init; } = DateTimeOffset.UtcNow;
}

/// <summary>
/// Event args for training progress updates
/// </summary>
public sealed class TrainingProgressEventArgs : EventArgs
{
    public TrainingProgress Progress { get; init; } = null!;
}
```

### v0.8.3c File Summary

| File | Purpose | Lines (Est.) |
|------|---------|--------------|
| `Core/Training/TrainingJob.cs` | Training job definition | ~110 |
| `Core/Training/TrainingProgress.cs` | Progress and metrics models | ~180 |
| `Core/Training/TrainingResult.cs` | Training result models | ~180 |
| `Core/Training/TrainingState.cs` | State enum and events | ~60 |

**v0.8.3c Total: 4 files to create**

---

## v0.8.3d: Loss Computation & Gradient Management

### Objective
Implement cross-entropy loss computation for causal language modeling with label masking, along with gradient clipping and accumulation utilities.

### Files to Create

#### 1. Services/Training/LossComputation.cs

```csharp
namespace SeniorIntern.Services.Training;

using TorchSharp;
using static TorchSharp.torch;

/// <summary>
/// Loss computation for causal language model training
/// </summary>
public sealed class CausalLMLoss
{
    private readonly int _ignoreIndex;
    private readonly bool _reduction;

    /// <summary>
    /// Create a causal LM loss function
    /// </summary>
    /// <param name="ignoreIndex">Label value to ignore (typically -100)</param>
    /// <param name="reduction">Whether to reduce (mean) the loss</param>
    public CausalLMLoss(int ignoreIndex = -100, bool reduction = true)
    {
        _ignoreIndex = ignoreIndex;
        _reduction = reduction;
    }

    /// <summary>
    /// Compute cross-entropy loss for next-token prediction
    /// </summary>
    /// <param name="logits">Model output logits [batch, seq_len, vocab_size]</param>
    /// <param name="labels">Target labels [batch, seq_len]</param>
    /// <returns>Scalar loss value</returns>
    public Tensor Compute(Tensor logits, Tensor labels)
    {
        // Shift for causal LM: predict token i+1 from position i
        // logits: use positions 0 to seq_len-2
        // labels: use positions 1 to seq_len-1
        var shiftLogits = logits[.., ..^1, ..].contiguous();
        var shiftLabels = labels[.., 1..].contiguous();

        // Flatten for cross_entropy
        // shiftLogits: [batch * (seq_len-1), vocab_size]
        // shiftLabels: [batch * (seq_len-1)]
        var flatLogits = shiftLogits.view(-1, shiftLogits.shape[^1]);
        var flatLabels = shiftLabels.view(-1);

        // Cross-entropy with ignore_index for masked positions
        var loss = nn.functional.cross_entropy(
            flatLogits,
            flatLabels,
            ignore_index: _ignoreIndex,
            reduction: _reduction ? nn.Reduction.Mean : nn.Reduction.None);

        return loss;
    }

    /// <summary>
    /// Compute loss with additional metrics
    /// </summary>
    public LossResult ComputeWithMetrics(Tensor logits, Tensor labels)
    {
        var loss = Compute(logits, labels);

        // Count valid tokens (not ignored)
        var shiftLabels = labels[.., 1..].contiguous().view(-1);
        var validMask = shiftLabels.ne(_ignoreIndex);
        var numValidTokens = validMask.sum().item<long>();

        // Compute accuracy on valid tokens
        var shiftLogits = logits[.., ..^1, ..].contiguous();
        var predictions = shiftLogits.view(-1, shiftLogits.shape[^1]).argmax(dim: 1);
        var correctPredictions = predictions.eq(shiftLabels).logical_and(validMask);
        var numCorrect = correctPredictions.sum().item<long>();

        var accuracy = numValidTokens > 0
            ? (float)numCorrect / numValidTokens
            : 0f;

        return new LossResult
        {
            Loss = loss,
            LossValue = loss.item<float>(),
            NumValidTokens = numValidTokens,
            NumCorrectPredictions = numCorrect,
            Accuracy = accuracy,
            Perplexity = MathF.Exp(Math.Min(loss.item<float>(), 20f))
        };
    }
}

/// <summary>
/// Result of loss computation with additional metrics
/// </summary>
public sealed class LossResult
{
    public Tensor Loss { get; init; } = null!;
    public float LossValue { get; init; }
    public long NumValidTokens { get; init; }
    public long NumCorrectPredictions { get; init; }
    public float Accuracy { get; init; }
    public float Perplexity { get; init; }
}
```

#### 2. Services/Training/GradientManager.cs

```csharp
namespace SeniorIntern.Services.Training;

using TorchSharp;
using static TorchSharp.torch;

/// <summary>
/// Manages gradient operations during training
/// </summary>
public sealed class GradientManager
{
    private readonly ILogger<GradientManager> _logger;
    private readonly float _maxGradNorm;
    private readonly int _accumulationSteps;

    private int _currentAccumulationStep;
    private bool _gradientAccumulationEnabled;

    /// <summary>Current accumulation step (0-based)</summary>
    public int CurrentAccumulationStep => _currentAccumulationStep;

    /// <summary>Whether we should perform optimizer step</summary>
    public bool ShouldStep => _currentAccumulationStep >= _accumulationSteps - 1;

    public GradientManager(
        ILogger<GradientManager> logger,
        float maxGradNorm = 1.0f,
        int accumulationSteps = 1)
    {
        _logger = logger;
        _maxGradNorm = maxGradNorm;
        _accumulationSteps = Math.Max(1, accumulationSteps);
        _gradientAccumulationEnabled = accumulationSteps > 1;
    }

    /// <summary>
    /// Scale loss for gradient accumulation
    /// </summary>
    public Tensor ScaleLoss(Tensor loss)
    {
        if (!_gradientAccumulationEnabled)
            return loss;

        return loss / _accumulationSteps;
    }

    /// <summary>
    /// Accumulate gradients and return whether to step
    /// </summary>
    public bool Accumulate()
    {
        _currentAccumulationStep++;

        if (_currentAccumulationStep >= _accumulationSteps)
        {
            _currentAccumulationStep = 0;
            return true; // Should step
        }

        return false; // Continue accumulating
    }

    /// <summary>
    /// Clip gradients by global norm
    /// </summary>
    /// <param name="parameters">Model parameters</param>
    /// <returns>The gradient norm before clipping</returns>
    public float ClipGradients(IEnumerable<Parameter> parameters)
    {
        var paramList = parameters.ToList();

        if (paramList.Count == 0)
            return 0f;

        var gradNorm = nn.utils.clip_grad_norm_(paramList, _maxGradNorm);
        return gradNorm.item<float>();
    }

    /// <summary>
    /// Check for gradient anomalies (NaN, Inf)
    /// </summary>
    public GradientHealthCheck CheckGradientHealth(IEnumerable<Parameter> parameters)
    {
        var hasNan = false;
        var hasInf = false;
        var maxAbsGrad = 0f;
        var totalParams = 0;
        var paramsWithGrad = 0;

        foreach (var param in parameters)
        {
            totalParams++;

            if (param.grad is null)
                continue;

            paramsWithGrad++;

            using var disposeScope = NewDisposeScope();

            // Check for NaN
            if (param.grad.isnan().any().item<bool>())
                hasNan = true;

            // Check for Inf
            if (param.grad.isinf().any().item<bool>())
                hasInf = true;

            // Track max absolute gradient
            var absMax = param.grad.abs().max().item<float>();
            if (absMax > maxAbsGrad && !float.IsNaN(absMax) && !float.IsInfinity(absMax))
                maxAbsGrad = absMax;
        }

        var isHealthy = !hasNan && !hasInf && maxAbsGrad < 1e6f;

        if (!isHealthy)
        {
            _logger.LogWarning(
                "Gradient health issue: NaN={HasNan}, Inf={HasInf}, MaxAbs={Max}",
                hasNan, hasInf, maxAbsGrad);
        }

        return new GradientHealthCheck
        {
            IsHealthy = isHealthy,
            HasNaN = hasNan,
            HasInfinity = hasInf,
            MaxAbsoluteGradient = maxAbsGrad,
            TotalParameters = totalParams,
            ParametersWithGradient = paramsWithGrad
        };
    }

    /// <summary>
    /// Zero gradients for all parameters
    /// </summary>
    public void ZeroGradients(IEnumerable<Parameter> parameters)
    {
        foreach (var param in parameters)
        {
            if (param.grad is not null)
            {
                param.grad.zero_();
            }
        }
    }

    /// <summary>
    /// Reset accumulation counter
    /// </summary>
    public void Reset()
    {
        _currentAccumulationStep = 0;
    }
}

/// <summary>
/// Result of gradient health check
/// </summary>
public sealed class GradientHealthCheck
{
    public bool IsHealthy { get; init; }
    public bool HasNaN { get; init; }
    public bool HasInfinity { get; init; }
    public float MaxAbsoluteGradient { get; init; }
    public int TotalParameters { get; init; }
    public int ParametersWithGradient { get; init; }
}
```

### v0.8.3d File Summary

| File | Purpose | Lines (Est.) |
|------|---------|--------------|
| `Services/Training/LossComputation.cs` | Cross-entropy loss for causal LM | ~120 |
| `Services/Training/GradientManager.cs` | Gradient clipping and accumulation | ~150 |

**v0.8.3d Total: 2 files to create**

---

## v0.8.3e: Optimizer & Scheduler Factory

### Objective
Create optimizer factory supporting AdamW and other optimizers, along with learning rate schedulers (cosine, linear, constant).

### Files to Create

#### 1. Services/Training/OptimizerFactory.cs

```csharp
namespace SeniorIntern.Services.Training;

using TorchSharp;
using static TorchSharp.torch;

/// <summary>
/// Factory for creating optimizers for LoRA training
/// </summary>
public sealed class OptimizerFactory
{
    private readonly ILogger<OptimizerFactory> _logger;

    public OptimizerFactory(ILogger<OptimizerFactory> logger)
    {
        _logger = logger;
    }

    /// <summary>
    /// Create an optimizer for the given parameters
    /// </summary>
    public optim.Optimizer Create(
        IEnumerable<Parameter> parameters,
        OptimizerOptions options)
    {
        var paramList = parameters.ToList();

        _logger.LogInformation(
            "Creating {Type} optimizer: lr={LR}, weight_decay={WD}",
            options.Type, options.LearningRate, options.WeightDecay);

        return options.Type switch
        {
            OptimizerType.AdamW => CreateAdamW(paramList, options),
            OptimizerType.Adam => CreateAdam(paramList, options),
            OptimizerType.SGD => CreateSGD(paramList, options),
            OptimizerType.RMSprop => CreateRMSprop(paramList, options),
            _ => throw new ArgumentException($"Unknown optimizer type: {options.Type}")
        };
    }

    private optim.Optimizer CreateAdamW(List<Parameter> parameters, OptimizerOptions options)
    {
        return optim.AdamW(
            parameters,
            lr: options.LearningRate,
            weight_decay: options.WeightDecay,
            beta1: options.Beta1,
            beta2: options.Beta2,
            eps: options.Epsilon);
    }

    private optim.Optimizer CreateAdam(List<Parameter> parameters, OptimizerOptions options)
    {
        return optim.Adam(
            parameters,
            lr: options.LearningRate,
            weight_decay: options.WeightDecay,
            beta1: options.Beta1,
            beta2: options.Beta2,
            eps: options.Epsilon);
    }

    private optim.Optimizer CreateSGD(List<Parameter> parameters, OptimizerOptions options)
    {
        return optim.SGD(
            parameters,
            lr: options.LearningRate,
            momentum: options.Momentum,
            weight_decay: options.WeightDecay,
            nesterov: options.Nesterov);
    }

    private optim.Optimizer CreateRMSprop(List<Parameter> parameters, OptimizerOptions options)
    {
        return optim.RMSProp(
            parameters,
            lr: options.LearningRate,
            alpha: options.RMSpropAlpha,
            eps: options.Epsilon,
            weight_decay: options.WeightDecay,
            momentum: options.Momentum);
    }
}

/// <summary>
/// Optimizer configuration options
/// </summary>
public sealed class OptimizerOptions
{
    /// <summary>Optimizer algorithm</summary>
    public OptimizerType Type { get; init; } = OptimizerType.AdamW;

    /// <summary>Learning rate</summary>
    public double LearningRate { get; init; } = 2e-4;

    /// <summary>Weight decay (L2 regularization)</summary>
    public double WeightDecay { get; init; } = 0.01;

    /// <summary>Adam/AdamW beta1</summary>
    public double Beta1 { get; init; } = 0.9;

    /// <summary>Adam/AdamW beta2</summary>
    public double Beta2 { get; init; } = 0.999;

    /// <summary>Epsilon for numerical stability</summary>
    public double Epsilon { get; init; } = 1e-8;

    /// <summary>Momentum for SGD/RMSprop</summary>
    public double Momentum { get; init; } = 0.9;

    /// <summary>Use Nesterov momentum for SGD</summary>
    public bool Nesterov { get; init; } = false;

    /// <summary>Alpha for RMSprop</summary>
    public double RMSpropAlpha { get; init; } = 0.99;
}

/// <summary>
/// Supported optimizer types
/// </summary>
public enum OptimizerType
{
    /// <summary>AdamW (Adam with decoupled weight decay)</summary>
    AdamW,

    /// <summary>Adam optimizer</summary>
    Adam,

    /// <summary>Stochastic Gradient Descent</summary>
    SGD,

    /// <summary>RMSprop optimizer</summary>
    RMSprop
}
```

#### 2. Services/Training/SchedulerFactory.cs

```csharp
namespace SeniorIntern.Services.Training;

using TorchSharp;
using static TorchSharp.torch;

/// <summary>
/// Factory for creating learning rate schedulers
/// </summary>
public sealed class SchedulerFactory
{
    private readonly ILogger<SchedulerFactory> _logger;

    public SchedulerFactory(ILogger<SchedulerFactory> logger)
    {
        _logger = logger;
    }

    /// <summary>
    /// Create a learning rate scheduler
    /// </summary>
    public optim.lr_scheduler.LRScheduler Create(
        optim.Optimizer optimizer,
        SchedulerOptions options,
        int totalSteps)
    {
        _logger.LogInformation(
            "Creating {Type} scheduler: warmup={Warmup}, total={Total}",
            options.Type, options.WarmupSteps, totalSteps);

        var scheduler = options.Type switch
        {
            SchedulerType.Cosine => CreateCosineScheduler(optimizer, options, totalSteps),
            SchedulerType.Linear => CreateLinearScheduler(optimizer, options, totalSteps),
            SchedulerType.Constant => CreateConstantScheduler(optimizer),
            SchedulerType.CosineWithRestarts => CreateCosineWithRestartsScheduler(
                optimizer, options, totalSteps),
            SchedulerType.Polynomial => CreatePolynomialScheduler(optimizer, options, totalSteps),
            _ => throw new ArgumentException($"Unknown scheduler type: {options.Type}")
        };

        // Wrap with warmup if needed
        if (options.WarmupSteps > 0)
        {
            scheduler = WrapWithWarmup(optimizer, scheduler, options.WarmupSteps);
        }

        return scheduler;
    }

    private optim.lr_scheduler.LRScheduler CreateCosineScheduler(
        optim.Optimizer optimizer,
        SchedulerOptions options,
        int totalSteps)
    {
        var effectiveSteps = totalSteps - options.WarmupSteps;
        return optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max: effectiveSteps,
            eta_min: options.MinLearningRate);
    }

    private optim.lr_scheduler.LRScheduler CreateLinearScheduler(
        optim.Optimizer optimizer,
        SchedulerOptions options,
        int totalSteps)
    {
        var effectiveSteps = totalSteps - options.WarmupSteps;
        return optim.lr_scheduler.LinearLR(
            optimizer,
            start_factor: 1.0,
            end_factor: options.MinLearningRate / options.InitialLearningRate,
            total_iters: effectiveSteps);
    }

    private optim.lr_scheduler.LRScheduler CreateConstantScheduler(optim.Optimizer optimizer)
    {
        return optim.lr_scheduler.ConstantLR(optimizer, factor: 1.0);
    }

    private optim.lr_scheduler.LRScheduler CreateCosineWithRestartsScheduler(
        optim.Optimizer optimizer,
        SchedulerOptions options,
        int totalSteps)
    {
        var effectiveSteps = totalSteps - options.WarmupSteps;
        var cycleSteps = effectiveSteps / options.NumCycles;

        return optim.lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer,
            T_0: cycleSteps,
            T_mult: options.CycleMultiplier,
            eta_min: options.MinLearningRate);
    }

    private optim.lr_scheduler.LRScheduler CreatePolynomialScheduler(
        optim.Optimizer optimizer,
        SchedulerOptions options,
        int totalSteps)
    {
        var effectiveSteps = totalSteps - options.WarmupSteps;

        return optim.lr_scheduler.PolynomialLR(
            optimizer,
            total_iters: effectiveSteps,
            power: options.PolynomialPower);
    }

    private optim.lr_scheduler.LRScheduler WrapWithWarmup(
        optim.Optimizer optimizer,
        optim.lr_scheduler.LRScheduler mainScheduler,
        int warmupSteps)
    {
        // Linear warmup from 0 to initial LR
        var warmupScheduler = optim.lr_scheduler.LinearLR(
            optimizer,
            start_factor: 0.0,
            end_factor: 1.0,
            total_iters: warmupSteps);

        // Sequential: warmup then main
        return optim.lr_scheduler.SequentialLR(
            optimizer,
            schedulers: new[] { warmupScheduler, mainScheduler },
            milestones: new[] { warmupSteps });
    }
}

/// <summary>
/// Learning rate scheduler options
/// </summary>
public sealed class SchedulerOptions
{
    /// <summary>Scheduler type</summary>
    public SchedulerType Type { get; init; } = SchedulerType.Cosine;

    /// <summary>Number of warmup steps</summary>
    public int WarmupSteps { get; init; } = 100;

    /// <summary>Initial learning rate (before warmup)</summary>
    public double InitialLearningRate { get; init; } = 2e-4;

    /// <summary>Minimum learning rate at end of schedule</summary>
    public double MinLearningRate { get; init; } = 1e-6;

    /// <summary>Number of cycles for cosine with restarts</summary>
    public int NumCycles { get; init; } = 1;

    /// <summary>Cycle length multiplier for restarts</summary>
    public int CycleMultiplier { get; init; } = 2;

    /// <summary>Power for polynomial decay</summary>
    public double PolynomialPower { get; init; } = 1.0;
}

/// <summary>
/// Supported scheduler types
/// </summary>
public enum SchedulerType
{
    /// <summary>Cosine annealing</summary>
    Cosine,

    /// <summary>Linear decay</summary>
    Linear,

    /// <summary>Constant learning rate</summary>
    Constant,

    /// <summary>Cosine with warm restarts</summary>
    CosineWithRestarts,

    /// <summary>Polynomial decay</summary>
    Polynomial
}
```

### v0.8.3e File Summary

| File | Purpose | Lines (Est.) |
|------|---------|--------------|
| `Services/Training/OptimizerFactory.cs` | Optimizer creation | ~130 |
| `Services/Training/SchedulerFactory.cs` | LR scheduler creation | ~160 |

**v0.8.3e Total: 2 files to create**

---

## v0.8.3f: Checkpoint Manager

### Objective
Implement checkpoint saving and loading for training resumption, including state serialization for model, optimizer, scheduler, and training progress.

### Files to Create

#### 1. Services/Training/CheckpointManager.cs

```csharp
namespace SeniorIntern.Services.Training;

using System.Text.Json;
using TorchSharp;
using static TorchSharp.torch;

/// <summary>
/// Manages saving and loading training checkpoints
/// </summary>
public sealed class CheckpointManager
{
    private readonly ILogger<CheckpointManager> _logger;
    private readonly string _checkpointDir;
    private readonly int _maxCheckpoints;

    private readonly List<CheckpointInfo> _savedCheckpoints = new();

    public IReadOnlyList<CheckpointInfo> SavedCheckpoints => _savedCheckpoints;

    public CheckpointManager(
        ILogger<CheckpointManager> logger,
        string checkpointDir,
        int maxCheckpoints = 3)
    {
        _logger = logger;
        _checkpointDir = checkpointDir;
        _maxCheckpoints = maxCheckpoints;

        Directory.CreateDirectory(_checkpointDir);
    }

    /// <summary>
    /// Save a training checkpoint
    /// </summary>
    public async Task<CheckpointInfo> SaveCheckpointAsync(
        LoraModel model,
        optim.Optimizer optimizer,
        optim.lr_scheduler.LRScheduler scheduler,
        TrainingCheckpointState state,
        string? name = null,
        CancellationToken ct = default)
    {
        var checkpointName = name ?? $"checkpoint-{state.GlobalStep}";
        var checkpointPath = Path.Combine(_checkpointDir, checkpointName);
        Directory.CreateDirectory(checkpointPath);

        _logger.LogInformation("Saving checkpoint: {Name}", checkpointName);
        var stopwatch = Stopwatch.StartNew();

        try
        {
            // Save LoRA weights
            var loraWeightsPath = Path.Combine(checkpointPath, "lora_weights.pt");
            await SaveLoraWeightsAsync(model, loraWeightsPath, ct);

            // Save optimizer state
            var optimizerPath = Path.Combine(checkpointPath, "optimizer.pt");
            optimizer.save(optimizerPath);

            // Save scheduler state
            var schedulerPath = Path.Combine(checkpointPath, "scheduler.pt");
            // Note: TorchSharp scheduler save may need custom implementation
            SaveSchedulerState(scheduler, schedulerPath);

            // Save training state
            var statePath = Path.Combine(checkpointPath, "training_state.json");
            await SaveTrainingStateAsync(state, statePath, ct);

            // Save metadata
            var metadataPath = Path.Combine(checkpointPath, "metadata.json");
            var metadata = new CheckpointMetadata
            {
                Name = checkpointName,
                Step = state.GlobalStep,
                Epoch = state.CurrentEpoch,
                Loss = state.CurrentLoss,
                ValidationLoss = state.ValidationLoss,
                CreatedAt = DateTimeOffset.UtcNow,
                LoraConfig = state.LoraConfig,
                TrainingConfig = state.TrainingConfig
            };
            await SaveMetadataAsync(metadata, metadataPath, ct);

            stopwatch.Stop();

            var info = new CheckpointInfo
            {
                Path = checkpointPath,
                Step = state.GlobalStep,
                Epoch = state.CurrentEpoch,
                Loss = state.CurrentLoss,
                ValidationLoss = state.ValidationLoss,
                CreatedAt = DateTimeOffset.UtcNow,
                SizeBytes = CalculateDirectorySize(checkpointPath),
                IsBest = state.IsBest
            };

            _savedCheckpoints.Add(info);

            // Clean up old checkpoints
            await CleanupOldCheckpointsAsync(ct);

            _logger.LogInformation(
                "Checkpoint saved: {Name} in {Duration:F1}s ({Size})",
                checkpointName, stopwatch.Elapsed.TotalSeconds,
                FormatBytes(info.SizeBytes));

            return info;
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Failed to save checkpoint: {Name}", checkpointName);
            throw;
        }
    }

    /// <summary>
    /// Load a training checkpoint
    /// </summary>
    public async Task<LoadedCheckpoint> LoadCheckpointAsync(
        string checkpointPath,
        LoraModel model,
        optim.Optimizer optimizer,
        optim.lr_scheduler.LRScheduler scheduler,
        CancellationToken ct = default)
    {
        _logger.LogInformation("Loading checkpoint from: {Path}", checkpointPath);

        if (!Directory.Exists(checkpointPath))
        {
            throw new DirectoryNotFoundException($"Checkpoint not found: {checkpointPath}");
        }

        // Load LoRA weights
        var loraWeightsPath = Path.Combine(checkpointPath, "lora_weights.pt");
        if (File.Exists(loraWeightsPath))
        {
            await LoadLoraWeightsAsync(model, loraWeightsPath, ct);
        }

        // Load optimizer state
        var optimizerPath = Path.Combine(checkpointPath, "optimizer.pt");
        if (File.Exists(optimizerPath))
        {
            optimizer.load(optimizerPath);
        }

        // Load scheduler state
        var schedulerPath = Path.Combine(checkpointPath, "scheduler.pt");
        if (File.Exists(schedulerPath))
        {
            LoadSchedulerState(scheduler, schedulerPath);
        }

        // Load training state
        var statePath = Path.Combine(checkpointPath, "training_state.json");
        var state = await LoadTrainingStateAsync(statePath, ct);

        // Load metadata
        var metadataPath = Path.Combine(checkpointPath, "metadata.json");
        var metadata = await LoadMetadataAsync(metadataPath, ct);

        _logger.LogInformation(
            "Checkpoint loaded: epoch={Epoch}, step={Step}, loss={Loss:F4}",
            state.CurrentEpoch, state.GlobalStep, state.CurrentLoss);

        return new LoadedCheckpoint
        {
            State = state,
            Metadata = metadata
        };
    }

    /// <summary>
    /// Get the latest checkpoint path
    /// </summary>
    public string? GetLatestCheckpointPath()
    {
        if (!Directory.Exists(_checkpointDir))
            return null;

        var checkpoints = Directory.GetDirectories(_checkpointDir)
            .Select(d => new
            {
                Path = d,
                MetadataPath = Path.Combine(d, "metadata.json")
            })
            .Where(x => File.Exists(x.MetadataPath))
            .OrderByDescending(x => File.GetLastWriteTimeUtc(x.MetadataPath))
            .FirstOrDefault();

        return checkpoints?.Path;
    }

    /// <summary>
    /// Get the best checkpoint path
    /// </summary>
    public string? GetBestCheckpointPath()
    {
        var bestPath = Path.Combine(_checkpointDir, "best");
        return Directory.Exists(bestPath) ? bestPath : null;
    }

    private async Task SaveLoraWeightsAsync(
        LoraModel model,
        string path,
        CancellationToken ct)
    {
        var weights = model.GetAllLoraWeights();
        var tensors = new Dictionary<string, Tensor>();

        foreach (var (name, layerWeights) in weights)
        {
            tensors[$"{name}.lora_A"] = layerWeights.LoraA;
            tensors[$"{name}.lora_B"] = layerWeights.LoraB;
        }

        // Use TorchSharp save
        await Task.Run(() =>
        {
            using var stream = File.Create(path);
            // Save as safetensors-like format
            foreach (var (name, tensor) in tensors)
            {
                tensor.save(Path.Combine(Path.GetDirectoryName(path)!, $"{name.Replace(".", "_")}.tensor"));
            }
        }, ct);
    }

    private async Task LoadLoraWeightsAsync(
        LoraModel model,
        string path,
        CancellationToken ct)
    {
        var dir = Path.GetDirectoryName(path)!;
        var tensorFiles = Directory.GetFiles(dir, "*.tensor");
        var weights = new Dictionary<string, LoraLayerWeights>();

        await Task.Run(() =>
        {
            foreach (var file in tensorFiles)
            {
                var name = Path.GetFileNameWithoutExtension(file).Replace("_", ".");
                // Parse tensor and reconstruct LoraLayerWeights
                var tensor = Tensor.load(file);
                // Implementation depends on naming convention
            }
        }, ct);

        model.LoadLoraWeights(weights);
    }

    private void SaveSchedulerState(optim.lr_scheduler.LRScheduler scheduler, string path)
    {
        // TorchSharp schedulers may not have built-in save
        // Store the step count for restoration
        var state = new SchedulerState
        {
            LastEpoch = scheduler.LastEpoch
        };

        var json = JsonSerializer.Serialize(state);
        File.WriteAllText(path, json);
    }

    private void LoadSchedulerState(optim.lr_scheduler.LRScheduler scheduler, string path)
    {
        if (!File.Exists(path)) return;

        var json = File.ReadAllText(path);
        var state = JsonSerializer.Deserialize<SchedulerState>(json);

        if (state != null)
        {
            // Step scheduler to correct position
            for (int i = 0; i < state.LastEpoch; i++)
            {
                scheduler.step();
            }
        }
    }

    private async Task SaveTrainingStateAsync(
        TrainingCheckpointState state,
        string path,
        CancellationToken ct)
    {
        var json = JsonSerializer.Serialize(state, new JsonSerializerOptions
        {
            WriteIndented = true
        });
        await File.WriteAllTextAsync(path, json, ct);
    }

    private async Task<TrainingCheckpointState> LoadTrainingStateAsync(
        string path,
        CancellationToken ct)
    {
        var json = await File.ReadAllTextAsync(path, ct);
        return JsonSerializer.Deserialize<TrainingCheckpointState>(json)!;
    }

    private async Task SaveMetadataAsync(
        CheckpointMetadata metadata,
        string path,
        CancellationToken ct)
    {
        var json = JsonSerializer.Serialize(metadata, new JsonSerializerOptions
        {
            WriteIndented = true
        });
        await File.WriteAllTextAsync(path, json, ct);
    }

    private async Task<CheckpointMetadata> LoadMetadataAsync(
        string path,
        CancellationToken ct)
    {
        var json = await File.ReadAllTextAsync(path, ct);
        return JsonSerializer.Deserialize<CheckpointMetadata>(json)!;
    }

    private async Task CleanupOldCheckpointsAsync(CancellationToken ct)
    {
        // Keep "best" checkpoint always
        var regularCheckpoints = _savedCheckpoints
            .Where(c => !c.Path.EndsWith("best"))
            .OrderByDescending(c => c.CreatedAt)
            .ToList();

        while (regularCheckpoints.Count > _maxCheckpoints)
        {
            var oldest = regularCheckpoints.Last();
            regularCheckpoints.Remove(oldest);
            _savedCheckpoints.Remove(oldest);

            if (Directory.Exists(oldest.Path))
            {
                await Task.Run(() => Directory.Delete(oldest.Path, recursive: true), ct);
                _logger.LogDebug("Deleted old checkpoint: {Path}", oldest.Path);
            }
        }
    }

    private static long CalculateDirectorySize(string path)
    {
        return Directory.GetFiles(path, "*", SearchOption.AllDirectories)
            .Sum(f => new FileInfo(f).Length);
    }

    private static string FormatBytes(long bytes) => bytes switch
    {
        < 1024 => $"{bytes} B",
        < 1024 * 1024 => $"{bytes / 1024.0:F1} KB",
        < 1024 * 1024 * 1024 => $"{bytes / (1024.0 * 1024):F1} MB",
        _ => $"{bytes / (1024.0 * 1024 * 1024):F2} GB"
    };
}

/// <summary>
/// Training state for checkpoint
/// </summary>
public sealed class TrainingCheckpointState
{
    public int GlobalStep { get; init; }
    public int CurrentEpoch { get; init; }
    public int CurrentStepInEpoch { get; init; }
    public float CurrentLoss { get; init; }
    public float? ValidationLoss { get; init; }
    public float BestValidationLoss { get; init; } = float.MaxValue;
    public bool IsBest { get; init; }
    public int EpochsWithoutImprovement { get; init; }
    public long SamplesProcessed { get; init; }
    public int RandomSeed { get; init; }
    public LoraConfiguration LoraConfig { get; init; } = new();
    public TrainingConfiguration TrainingConfig { get; init; } = new();
    public List<TrainingMetrics> MetricsHistory { get; init; } = new();
}

/// <summary>
/// Checkpoint metadata
/// </summary>
public sealed class CheckpointMetadata
{
    public string Name { get; init; } = string.Empty;
    public int Step { get; init; }
    public int Epoch { get; init; }
    public float Loss { get; init; }
    public float? ValidationLoss { get; init; }
    public DateTimeOffset CreatedAt { get; init; }
    public LoraConfiguration LoraConfig { get; init; } = new();
    public TrainingConfiguration TrainingConfig { get; init; } = new();
}

/// <summary>
/// Scheduler state for serialization
/// </summary>
public sealed class SchedulerState
{
    public int LastEpoch { get; init; }
}

/// <summary>
/// Loaded checkpoint data
/// </summary>
public sealed class LoadedCheckpoint
{
    public TrainingCheckpointState State { get; init; } = null!;
    public CheckpointMetadata Metadata { get; init; } = null!;
}
```

### v0.8.3f File Summary

| File | Purpose | Lines (Est.) |
|------|---------|--------------|
| `Services/Training/CheckpointManager.cs` | Checkpoint save/load | ~380 |

**v0.8.3f Total: 1 file to create**

---

## v0.8.3g: Training Service Interface

### Objective
Define the main training service interface and implement the training orchestration.

### Files to Create

#### 1. Core/Interfaces/ITrainingService.cs

```csharp
namespace SeniorIntern.Core.Interfaces;

/// <summary>
/// Main service interface for LoRA training operations
/// </summary>
public interface ITrainingService
{
    /// <summary>Current training state</summary>
    TrainingState State { get; }

    /// <summary>Current job (if training)</summary>
    TrainingJob? CurrentJob { get; }

    /// <summary>
    /// Start a new training run
    /// </summary>
    Task<TrainingResult> TrainAsync(
        TrainingJob job,
        IProgress<TrainingProgress>? progress = null,
        CancellationToken ct = default);

    /// <summary>
    /// Resume training from a checkpoint
    /// </summary>
    Task<TrainingResult> ResumeTrainingAsync(
        string checkpointPath,
        IProgress<TrainingProgress>? progress = null,
        CancellationToken ct = default);

    /// <summary>
    /// Pause current training (can be resumed)
    /// </summary>
    Task PauseTrainingAsync();

    /// <summary>
    /// Cancel current training
    /// </summary>
    void CancelTraining();

    /// <summary>
    /// Load a trained LoRA adapter
    /// </summary>
    Task<LoraAdapter> LoadAdapterAsync(
        string adapterPath,
        CancellationToken ct = default);

    /// <summary>
    /// List all available adapters
    /// </summary>
    Task<IReadOnlyList<LoraAdapterInfo>> ListAdaptersAsync(
        CancellationToken ct = default);

    /// <summary>
    /// Delete an adapter
    /// </summary>
    Task DeleteAdapterAsync(
        string adapterId,
        CancellationToken ct = default);

    /// <summary>
    /// Validate a training job before starting
    /// </summary>
    Task<TrainingJobValidationResult> ValidateJobAsync(
        TrainingJob job,
        CancellationToken ct = default);

    /// <summary>
    /// Estimate training requirements (time, memory)
    /// </summary>
    Task<TrainingEstimate> EstimateTrainingAsync(
        TrainingJob job,
        CancellationToken ct = default);

    /// <summary>Event when training progress updates</summary>
    event EventHandler<TrainingProgressEventArgs>? ProgressChanged;

    /// <summary>Event when training state changes</summary>
    event EventHandler<TrainingStateChangedEventArgs>? StateChanged;
}

/// <summary>
/// Estimated training requirements
/// </summary>
public sealed class TrainingEstimate
{
    public TimeSpan EstimatedDuration { get; init; }
    public long EstimatedVramBytes { get; init; }
    public long EstimatedDiskBytes { get; init; }
    public int TotalSteps { get; init; }
    public long TrainableParameters { get; init; }
    public bool CanTrainOnDevice { get; init; }
    public string RecommendedDevice { get; init; } = string.Empty;
    public int RecommendedBatchSize { get; init; }
    public IReadOnlyList<string> Warnings { get; init; } = Array.Empty<string>();
}
```

#### 2. Services/Training/LoraTrainingEngine.cs

```csharp
namespace SeniorIntern.Services.Training;

/// <summary>
/// Main LoRA training engine implementation
/// </summary>
public sealed class LoraTrainingEngine : ITrainingService
{
    private readonly ILogger<LoraTrainingEngine> _logger;
    private readonly IHardwareDetectionService _hardwareService;
    private readonly IDatasetService _datasetService;
    private readonly BaseModelLoader _modelLoader;
    private readonly OptimizerFactory _optimizerFactory;
    private readonly SchedulerFactory _schedulerFactory;
    private readonly CheckpointManager _checkpointManager;
    private readonly string _adaptersDir;

    private CancellationTokenSource? _trainingCts;
    private TrainingState _state = TrainingState.Idle;
    private TrainingJob? _currentJob;
    private bool _pauseRequested;

    public TrainingState State => _state;
    public TrainingJob? CurrentJob => _currentJob;

    public event EventHandler<TrainingProgressEventArgs>? ProgressChanged;
    public event EventHandler<TrainingStateChangedEventArgs>? StateChanged;

    public LoraTrainingEngine(
        ILogger<LoraTrainingEngine> logger,
        IHardwareDetectionService hardwareService,
        IDatasetService datasetService,
        BaseModelLoader modelLoader,
        OptimizerFactory optimizerFactory,
        SchedulerFactory schedulerFactory,
        string outputDir)
    {
        _logger = logger;
        _hardwareService = hardwareService;
        _datasetService = datasetService;
        _modelLoader = modelLoader;
        _optimizerFactory = optimizerFactory;
        _schedulerFactory = schedulerFactory;
        _adaptersDir = Path.Combine(outputDir, "adapters");

        Directory.CreateDirectory(_adaptersDir);
        _checkpointManager = new CheckpointManager(
            LoggerFactory.Create(b => b.AddConsole()).CreateLogger<CheckpointManager>(),
            Path.Combine(outputDir, "checkpoints"));
    }

    public async Task<TrainingResult> TrainAsync(
        TrainingJob job,
        IProgress<TrainingProgress>? progress = null,
        CancellationToken ct = default)
    {
        if (_state != TrainingState.Idle)
        {
            throw new InvalidOperationException($"Cannot start training in state: {_state}");
        }

        _currentJob = job;
        _trainingCts = CancellationTokenSource.CreateLinkedTokenSource(ct);
        var stopwatch = Stopwatch.StartNew();
        var metricsHistory = new List<TrainingMetrics>();

        try
        {
            // Validate job
            var validation = job.Validate();
            if (!validation.IsValid)
            {
                throw new ArgumentException(
                    $"Invalid training job: {string.Join(", ", validation.Errors)}");
            }

            SetState(TrainingState.Preparing, "Initializing training...");

            // 1. Detect hardware
            var hardware = await _hardwareService.DetectHardwareAsync(_trainingCts.Token);
            var device = torch.device(job.Configuration.Device);

            _logger.LogInformation(
                "Training device: {Device}, Available VRAM: {Vram}",
                device, hardware.GpuDevices.FirstOrDefault()?.FreeMemory);

            // 2. Load base model
            ReportProgress(progress, "Loading base model...");
            using var loadedModel = await _modelLoader.LoadModelAsync(
                job.BaseModelPath,
                new ModelLoadOptions
                {
                    ContextSize = job.Configuration.MaxSequenceLength,
                    GpuLayers = hardware.CudaAvailable ? -1 : 0
                },
                _trainingCts.Token);

            // 3. Create LoRA model
            ReportProgress(progress, "Initializing LoRA adapters...");
            var loraModel = new LoraModel(
                loadedModel.Weights.CreateContext(loadedModel.ModelParams).Model,
                job.LoraConfig,
                LoggerFactory.Create(b => b.AddConsole()).CreateLogger<LoraModel>());

            loraModel.ToDevice(device);

            _logger.LogInformation(
                "LoRA model created: {Layers} layers, {Params} trainable parameters",
                loraModel.LoraLayerCount,
                loraModel.GetInfo().TrainableParametersFormatted);

            // 4. Load and tokenize dataset
            ReportProgress(progress, "Loading dataset...");
            var dataset = await _datasetService.LoadDatasetAsync(
                job.DatasetPath, _trainingCts.Token);

            var tokenizationOptions = new TokenizationOptions
            {
                MaxLength = job.Configuration.MaxSequenceLength,
                MaskInstructionTokens = true
            };

            var tokenizedDataset = await _datasetService.TokenizeDatasetAsync(
                dataset, tokenizationOptions, null, _trainingCts.Token);

            var (trainLoader, validLoader) = _datasetService.CreateDataLoaders(
                tokenizedDataset,
                new DataLoaderOptions
                {
                    BatchSize = job.Configuration.BatchSize,
                    Shuffle = true,
                    DropLast = true
                });

            // 5. Create optimizer and scheduler
            var optimizerOptions = new OptimizerOptions
            {
                Type = OptimizerType.AdamW,
                LearningRate = job.Configuration.LearningRate,
                WeightDecay = job.Configuration.WeightDecay
            };

            var optimizer = _optimizerFactory.Create(
                loraModel.GetLoraParameters(), optimizerOptions);

            var totalSteps = trainLoader.TotalBatches * job.Configuration.Epochs;
            var schedulerOptions = new SchedulerOptions
            {
                Type = ParseSchedulerType(job.Configuration.LrScheduler),
                WarmupSteps = job.Configuration.WarmupSteps,
                InitialLearningRate = job.Configuration.LearningRate
            };

            var scheduler = _schedulerFactory.Create(
                optimizer, schedulerOptions, totalSteps);

            // 6. Setup training components
            var lossFunction = new CausalLMLoss();
            var gradientManager = new GradientManager(
                LoggerFactory.Create(b => b.AddConsole()).CreateLogger<GradientManager>(),
                maxGradNorm: 1.0f,
                accumulationSteps: job.Configuration.GradientAccumulationSteps);

            // 7. Training loop
            SetState(TrainingState.Training, "Training in progress...");

            var globalStep = 0;
            var bestValLoss = float.MaxValue;
            TrainingMetrics? bestMetrics = null;

            for (int epoch = 0; epoch < job.Configuration.Epochs; epoch++)
            {
                _trainingCts.Token.ThrowIfCancellationRequested();
                CheckPauseRequested();

                var epochMetrics = await TrainEpochAsync(
                    loraModel, trainLoader, optimizer, scheduler,
                    lossFunction, gradientManager, device,
                    job.Configuration, epoch, globalStep,
                    progress, metricsHistory, _trainingCts.Token);

                globalStep += trainLoader.TotalBatches;

                // Validation
                if (validLoader != null && job.Configuration.EvalEveryNSteps > 0)
                {
                    SetState(TrainingState.Evaluating, "Running validation...");

                    var valLoss = await EvaluateAsync(
                        loraModel, validLoader, lossFunction, device, _trainingCts.Token);

                    epochMetrics = epochMetrics with { ValidationLoss = valLoss };

                    if (valLoss < bestValLoss)
                    {
                        bestValLoss = valLoss;
                        bestMetrics = epochMetrics;

                        // Save best checkpoint
                        SetState(TrainingState.Saving, "Saving best checkpoint...");
                        await _checkpointManager.SaveCheckpointAsync(
                            loraModel, optimizer, scheduler,
                            CreateCheckpointState(epoch, globalStep, epochMetrics, job, true),
                            "best", _trainingCts.Token);
                    }

                    SetState(TrainingState.Training, "Training in progress...");
                }

                // Save periodic checkpoint
                if (ShouldSaveCheckpoint(epoch, job.Configuration))
                {
                    SetState(TrainingState.Saving, $"Saving checkpoint (epoch {epoch + 1})...");
                    await _checkpointManager.SaveCheckpointAsync(
                        loraModel, optimizer, scheduler,
                        CreateCheckpointState(epoch, globalStep, epochMetrics, job, false),
                        $"epoch-{epoch + 1}", _trainingCts.Token);
                    SetState(TrainingState.Training, "Training in progress...");
                }

                // Shuffle for next epoch
                trainLoader.Shuffle();
            }

            // 8. Save final adapter
            SetState(TrainingState.Saving, "Saving final adapter...");
            var adapterPath = await SaveFinalAdapterAsync(
                loraModel, job, metricsHistory.LastOrDefault(),
                _trainingCts.Token);

            stopwatch.Stop();
            SetState(TrainingState.Completed, "Training completed successfully!");

            return TrainingResult.CreateSuccess(
                job.Id,
                adapterPath,
                metricsHistory.LastOrDefault() ?? new(),
                bestMetrics,
                stopwatch.Elapsed,
                globalStep,
                job.Configuration.Epochs,
                metricsHistory,
                job.Configuration,
                job.LoraConfig);
        }
        catch (OperationCanceledException)
        {
            SetState(TrainingState.Cancelled, "Training was cancelled");
            return TrainingResult.CreateCancelled(
                job.Id, stopwatch.Elapsed, metricsHistory.Count, metricsHistory);
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Training failed");
            SetState(TrainingState.Failed, ex.Message);
            return TrainingResult.CreateFailure(job.Id, ex, stopwatch.Elapsed, metricsHistory);
        }
        finally
        {
            _trainingCts?.Dispose();
            _trainingCts = null;
            _currentJob = null;
            _pauseRequested = false;
        }
    }

    private async Task<TrainingMetrics> TrainEpochAsync(
        LoraModel model,
        IDataLoader trainLoader,
        optim.Optimizer optimizer,
        optim.lr_scheduler.LRScheduler scheduler,
        CausalLMLoss lossFunction,
        GradientManager gradientManager,
        torch.Device device,
        TrainingConfiguration config,
        int epoch,
        int startingGlobalStep,
        IProgress<TrainingProgress>? progress,
        List<TrainingMetrics> metricsHistory,
        CancellationToken ct)
    {
        model.train();
        var totalLoss = 0f;
        var batchCount = 0;
        var globalStep = startingGlobalStep;

        optimizer.zero_grad();
        gradientManager.Reset();

        foreach (var batch in trainLoader)
        {
            ct.ThrowIfCancellationRequested();
            CheckPauseRequested();

            using var disposeScope = torch.NewDisposeScope();

            // Move batch to device
            var inputIds = batch.InputIds.to(device);
            var attentionMask = batch.AttentionMask.to(device);
            var labels = batch.Labels.to(device);

            // Forward pass
            var logits = model.forward(inputIds, attentionMask);
            var lossResult = lossFunction.ComputeWithMetrics(logits, labels);

            // Scale loss for gradient accumulation
            var scaledLoss = gradientManager.ScaleLoss(lossResult.Loss);
            scaledLoss.backward();

            totalLoss += lossResult.LossValue;
            batchCount++;

            // Gradient accumulation and optimizer step
            if (gradientManager.Accumulate())
            {
                // Clip gradients
                var gradNorm = gradientManager.ClipGradients(model.GetLoraParameters());

                // Check gradient health
                var health = gradientManager.CheckGradientHealth(model.GetLoraParameters());
                if (!health.IsHealthy)
                {
                    _logger.LogWarning("Unhealthy gradients detected at step {Step}", globalStep);
                }

                // Optimizer step
                optimizer.step();
                scheduler.step();
                optimizer.zero_grad();

                globalStep++;

                // Record metrics
                var metrics = new TrainingMetrics
                {
                    Step = globalStep,
                    Epoch = epoch,
                    Loss = totalLoss / batchCount,
                    LearningRate = (float)scheduler.get_last_lr()[0],
                    GradientNorm = gradNorm
                };
                metricsHistory.Add(metrics);

                // Report progress
                var hwMetrics = await GetHardwareMetricsAsync();
                var epochProgress = new TrainingProgress
                {
                    CurrentEpoch = epoch + 1,
                    TotalEpochs = config.Epochs,
                    CurrentStep = batchCount,
                    StepsPerEpoch = trainLoader.TotalBatches,
                    GlobalStep = globalStep,
                    TotalSteps = trainLoader.TotalBatches * config.Epochs,
                    SamplesProcessed = batchCount * config.BatchSize,
                    TotalSamples = trainLoader.TotalExamples,
                    CurrentMetrics = metrics,
                    HardwareMetrics = hwMetrics,
                    CurrentOperation = $"Epoch {epoch + 1}/{config.Epochs}, " +
                                       $"Step {batchCount}/{trainLoader.TotalBatches}"
                };

                progress?.Report(epochProgress);
                OnProgressChanged(epochProgress);
            }
        }

        return new TrainingMetrics
        {
            Step = globalStep,
            Epoch = epoch,
            Loss = totalLoss / batchCount,
            LearningRate = (float)scheduler.get_last_lr()[0]
        };
    }

    private async Task<float> EvaluateAsync(
        LoraModel model,
        IDataLoader validLoader,
        CausalLMLoss lossFunction,
        torch.Device device,
        CancellationToken ct)
    {
        model.eval();
        var totalLoss = 0f;
        var batchCount = 0;

        using (torch.no_grad())
        {
            foreach (var batch in validLoader)
            {
                ct.ThrowIfCancellationRequested();

                using var disposeScope = torch.NewDisposeScope();

                var inputIds = batch.InputIds.to(device);
                var attentionMask = batch.AttentionMask.to(device);
                var labels = batch.Labels.to(device);

                var logits = model.forward(inputIds, attentionMask);
                var loss = lossFunction.Compute(logits, labels);

                totalLoss += loss.item<float>();
                batchCount++;
            }
        }

        return totalLoss / batchCount;
    }

    // Additional helper methods...

    private void SetState(TrainingState state, string? message = null)
    {
        var previous = _state;
        _state = state;
        StateChanged?.Invoke(this, new TrainingStateChangedEventArgs
        {
            PreviousState = previous,
            NewState = state,
            Message = message
        });
    }

    private void OnProgressChanged(TrainingProgress progress)
    {
        ProgressChanged?.Invoke(this, new TrainingProgressEventArgs
        {
            Progress = progress
        });
    }

    private void CheckPauseRequested()
    {
        if (_pauseRequested)
        {
            SetState(TrainingState.Paused, "Training paused");
            while (_pauseRequested && _state == TrainingState.Paused)
            {
                Thread.Sleep(100);
            }
            SetState(TrainingState.Training, "Training resumed");
        }
    }

    public Task PauseTrainingAsync()
    {
        _pauseRequested = true;
        return Task.CompletedTask;
    }

    public void CancelTraining()
    {
        _trainingCts?.Cancel();
    }

    // ... remaining interface implementations
}
```

### v0.8.3g File Summary

| File | Purpose | Lines (Est.) |
|------|---------|--------------|
| `Core/Interfaces/ITrainingService.cs` | Training service interface | ~80 |
| `Services/Training/LoraTrainingEngine.cs` | Main training implementation | ~450 |

**v0.8.3g Total: 2 files to create**

---

## v0.8.3h: Adapter Serialization & Management

### Objective
Implement adapter saving in a portable format and adapter management (list, load, delete).

### Files to Create

#### 1. Core/Training/LoraAdapter.cs

```csharp
namespace SeniorIntern.Core.Training;

/// <summary>
/// Represents a saved LoRA adapter
/// </summary>
public sealed class LoraAdapter
{
    /// <summary>Unique identifier</summary>
    public string Id { get; init; } = Guid.NewGuid().ToString();

    /// <summary>Human-readable name</summary>
    public string Name { get; init; } = string.Empty;

    /// <summary>Description of what this adapter was trained for</summary>
    public string? Description { get; init; }

    /// <summary>Hash of the base model this was trained on</summary>
    public string BaseModelHash { get; init; } = string.Empty;

    /// <summary>LoRA rank used</summary>
    public int Rank { get; init; }

    /// <summary>LoRA alpha scaling factor</summary>
    public float Alpha { get; init; }

    /// <summary>Target modules that have adapters</summary>
    public IReadOnlyList<string> TargetModules { get; init; } = Array.Empty<string>();

    /// <summary>When this adapter was created</summary>
    public DateTimeOffset CreatedAt { get; init; }

    /// <summary>Final training metrics</summary>
    public TrainingMetrics FinalMetrics { get; init; } = new();

    /// <summary>Training configuration used</summary>
    public TrainingConfiguration TrainingConfig { get; init; } = new();

    /// <summary>LoRA configuration used</summary>
    public LoraConfiguration LoraConfig { get; init; } = new();

    /// <summary>Optional tags for organization</summary>
    public Dictionary<string, string> Tags { get; init; } = new();

    /// <summary>Path to the adapter weights file</summary>
    public string WeightsPath { get; init; } = string.Empty;

    /// <summary>Adapter version (for compatibility)</summary>
    public int Version { get; init; } = 1;
}

/// <summary>
/// Summary information about an adapter (for listing)
/// </summary>
public sealed class LoraAdapterInfo
{
    public string Id { get; init; } = string.Empty;
    public string Name { get; init; } = string.Empty;
    public string? Description { get; init; }
    public DateTimeOffset CreatedAt { get; init; }
    public long SizeBytes { get; init; }
    public int Rank { get; init; }
    public float Alpha { get; init; }
    public float FinalLoss { get; init; }
    public float? FinalValidationLoss { get; init; }
    public string Path { get; init; } = string.Empty;

    public string FormattedSize => SizeBytes switch
    {
        < 1024 => $"{SizeBytes} B",
        < 1024 * 1024 => $"{SizeBytes / 1024.0:F1} KB",
        < 1024 * 1024 * 1024 => $"{SizeBytes / (1024.0 * 1024):F1} MB",
        _ => $"{SizeBytes / (1024.0 * 1024 * 1024):F2} GB"
    };
}
```

#### 2. Services/Training/AdapterSerializer.cs

```csharp
namespace SeniorIntern.Services.Training;

using System.IO.Compression;
using System.Text.Json;

/// <summary>
/// Serializes and deserializes LoRA adapters
/// </summary>
public sealed class AdapterSerializer
{
    private readonly ILogger<AdapterSerializer> _logger;

    private const string MetadataFileName = "adapter.json";
    private const string WeightsFileName = "weights";
    private const int CurrentVersion = 1;

    public AdapterSerializer(ILogger<AdapterSerializer> logger)
    {
        _logger = logger;
    }

    /// <summary>
    /// Save an adapter to disk
    /// </summary>
    public async Task<string> SaveAdapterAsync(
        LoraModel model,
        LoraAdapter metadata,
        string outputDir,
        CancellationToken ct = default)
    {
        var adapterDir = Path.Combine(outputDir, metadata.Id);
        Directory.CreateDirectory(adapterDir);

        _logger.LogInformation("Saving adapter to {Path}", adapterDir);
        var stopwatch = Stopwatch.StartNew();

        try
        {
            // Save weights
            var weightsDir = Path.Combine(adapterDir, WeightsFileName);
            Directory.CreateDirectory(weightsDir);

            var allWeights = model.GetAllLoraWeights();
            foreach (var (layerName, weights) in allWeights)
            {
                var safeLayerName = layerName.Replace(".", "_").Replace("/", "_");
                var loraAPath = Path.Combine(weightsDir, $"{safeLayerName}_lora_A.bin");
                var loraBPath = Path.Combine(weightsDir, $"{safeLayerName}_lora_B.bin");

                await SaveTensorAsync(weights.LoraA, loraAPath, ct);
                await SaveTensorAsync(weights.LoraB, loraBPath, ct);
            }

            // Save metadata
            var metadataPath = Path.Combine(adapterDir, MetadataFileName);
            var updatedMetadata = metadata with
            {
                WeightsPath = weightsDir,
                Version = CurrentVersion
            };

            var json = JsonSerializer.Serialize(updatedMetadata, new JsonSerializerOptions
            {
                WriteIndented = true
            });
            await File.WriteAllTextAsync(metadataPath, json, ct);

            stopwatch.Stop();
            var size = CalculateDirectorySize(adapterDir);

            _logger.LogInformation(
                "Adapter saved: {Size} in {Duration:F1}s",
                FormatBytes(size), stopwatch.Elapsed.TotalSeconds);

            return adapterDir;
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Failed to save adapter");
            // Clean up on failure
            if (Directory.Exists(adapterDir))
            {
                Directory.Delete(adapterDir, recursive: true);
            }
            throw;
        }
    }

    /// <summary>
    /// Load an adapter from disk
    /// </summary>
    public async Task<LoadedAdapter> LoadAdapterAsync(
        string adapterPath,
        CancellationToken ct = default)
    {
        _logger.LogInformation("Loading adapter from {Path}", adapterPath);

        if (!Directory.Exists(adapterPath))
        {
            throw new DirectoryNotFoundException($"Adapter not found: {adapterPath}");
        }

        // Load metadata
        var metadataPath = Path.Combine(adapterPath, MetadataFileName);
        if (!File.Exists(metadataPath))
        {
            throw new FileNotFoundException($"Adapter metadata not found: {metadataPath}");
        }

        var json = await File.ReadAllTextAsync(metadataPath, ct);
        var metadata = JsonSerializer.Deserialize<LoraAdapter>(json)!;

        // Load weights
        var weightsDir = Path.Combine(adapterPath, WeightsFileName);
        var weights = new Dictionary<string, LoraLayerWeights>();

        var weightFiles = Directory.GetFiles(weightsDir, "*_lora_A.bin");
        foreach (var loraAPath in weightFiles)
        {
            var baseName = Path.GetFileNameWithoutExtension(loraAPath)
                .Replace("_lora_A", "");
            var loraBPath = Path.Combine(weightsDir, $"{baseName}_lora_B.bin");

            if (!File.Exists(loraBPath))
            {
                _logger.LogWarning("Missing LoRA B weights for {Layer}", baseName);
                continue;
            }

            var loraA = await LoadTensorAsync(loraAPath, ct);
            var loraB = await LoadTensorAsync(loraBPath, ct);

            var layerName = baseName.Replace("_", ".");

            weights[layerName] = new LoraLayerWeights
            {
                Name = layerName,
                LoraA = loraA,
                LoraB = loraB,
                Rank = metadata.Rank,
                Alpha = metadata.Alpha,
                InFeatures = loraA.shape[1],
                OutFeatures = loraB.shape[0]
            };
        }

        _logger.LogInformation(
            "Adapter loaded: {Layers} layers, rank={Rank}",
            weights.Count, metadata.Rank);

        return new LoadedAdapter
        {
            Metadata = metadata,
            Weights = weights
        };
    }

    /// <summary>
    /// List all adapters in a directory
    /// </summary>
    public async Task<IReadOnlyList<LoraAdapterInfo>> ListAdaptersAsync(
        string adaptersDir,
        CancellationToken ct = default)
    {
        var adapters = new List<LoraAdapterInfo>();

        if (!Directory.Exists(adaptersDir))
        {
            return adapters;
        }

        foreach (var dir in Directory.GetDirectories(adaptersDir))
        {
            var metadataPath = Path.Combine(dir, MetadataFileName);
            if (!File.Exists(metadataPath))
                continue;

            try
            {
                var json = await File.ReadAllTextAsync(metadataPath, ct);
                var metadata = JsonSerializer.Deserialize<LoraAdapter>(json)!;

                adapters.Add(new LoraAdapterInfo
                {
                    Id = metadata.Id,
                    Name = metadata.Name,
                    Description = metadata.Description,
                    CreatedAt = metadata.CreatedAt,
                    SizeBytes = CalculateDirectorySize(dir),
                    Rank = metadata.Rank,
                    Alpha = metadata.Alpha,
                    FinalLoss = metadata.FinalMetrics.Loss,
                    FinalValidationLoss = metadata.FinalMetrics.ValidationLoss,
                    Path = dir
                });
            }
            catch (Exception ex)
            {
                _logger.LogWarning(ex, "Failed to read adapter metadata: {Path}", dir);
            }
        }

        return adapters.OrderByDescending(a => a.CreatedAt).ToList();
    }

    /// <summary>
    /// Delete an adapter
    /// </summary>
    public Task DeleteAdapterAsync(string adapterPath, CancellationToken ct = default)
    {
        if (Directory.Exists(adapterPath))
        {
            Directory.Delete(adapterPath, recursive: true);
            _logger.LogInformation("Deleted adapter: {Path}", adapterPath);
        }

        return Task.CompletedTask;
    }

    private async Task SaveTensorAsync(torch.Tensor tensor, string path, CancellationToken ct)
    {
        await Task.Run(() =>
        {
            // Convert to CPU and save as binary
            var cpuTensor = tensor.cpu();
            var data = cpuTensor.data<float>().ToArray();

            using var stream = File.Create(path);
            using var writer = new BinaryWriter(stream);

            // Write shape
            writer.Write(cpuTensor.dim());
            foreach (var dim in cpuTensor.shape)
            {
                writer.Write(dim);
            }

            // Write data
            foreach (var value in data)
            {
                writer.Write(value);
            }
        }, ct);
    }

    private async Task<torch.Tensor> LoadTensorAsync(string path, CancellationToken ct)
    {
        return await Task.Run(() =>
        {
            using var stream = File.OpenRead(path);
            using var reader = new BinaryReader(stream);

            // Read shape
            var ndim = reader.ReadInt32();
            var shape = new long[ndim];
            for (int i = 0; i < ndim; i++)
            {
                shape[i] = reader.ReadInt64();
            }

            // Read data
            var size = shape.Aggregate(1L, (a, b) => a * b);
            var data = new float[size];
            for (int i = 0; i < size; i++)
            {
                data[i] = reader.ReadSingle();
            }

            return torch.tensor(data).reshape(shape);
        }, ct);
    }

    private static long CalculateDirectorySize(string path)
    {
        return Directory.GetFiles(path, "*", SearchOption.AllDirectories)
            .Sum(f => new FileInfo(f).Length);
    }

    private static string FormatBytes(long bytes) => bytes switch
    {
        < 1024 => $"{bytes} B",
        < 1024 * 1024 => $"{bytes / 1024.0:F1} KB",
        < 1024 * 1024 * 1024 => $"{bytes / (1024.0 * 1024):F1} MB",
        _ => $"{bytes / (1024.0 * 1024 * 1024):F2} GB"
    };
}

/// <summary>
/// Loaded adapter with weights
/// </summary>
public sealed class LoadedAdapter
{
    public LoraAdapter Metadata { get; init; } = null!;
    public Dictionary<string, LoraLayerWeights> Weights { get; init; } = new();
}
```

### v0.8.3h File Summary

| File | Purpose | Lines (Est.) |
|------|---------|--------------|
| `Core/Training/LoraAdapter.cs` | Adapter models | ~90 |
| `Services/Training/AdapterSerializer.cs` | Adapter save/load | ~280 |

**v0.8.3h Total: 2 files to create**

---

## v0.8.3i: Early Stopping & Training Utilities

### Objective
Implement early stopping, learning rate finding, and other training utilities.

### Files to Create

#### 1. Services/Training/EarlyStopping.cs

```csharp
namespace SeniorIntern.Services.Training;

/// <summary>
/// Early stopping to prevent overfitting
/// </summary>
public sealed class EarlyStopping
{
    private readonly int _patience;
    private readonly float _minDelta;
    private readonly EarlyStoppingMode _mode;

    private int _counter;
    private float _bestValue;
    private bool _shouldStop;

    public int Counter => _counter;
    public float BestValue => _bestValue;
    public bool ShouldStop => _shouldStop;
    public int EpochsSinceImprovement => _counter;

    public EarlyStopping(
        int patience = 5,
        float minDelta = 0.0001f,
        EarlyStoppingMode mode = EarlyStoppingMode.Min)
    {
        _patience = patience;
        _minDelta = minDelta;
        _mode = mode;
        _bestValue = mode == EarlyStoppingMode.Min ? float.MaxValue : float.MinValue;
    }

    /// <summary>
    /// Check if training should stop
    /// </summary>
    /// <returns>True if improvement occurred, false otherwise</returns>
    public bool Step(float value)
    {
        var improved = _mode == EarlyStoppingMode.Min
            ? value < _bestValue - _minDelta
            : value > _bestValue + _minDelta;

        if (improved)
        {
            _bestValue = value;
            _counter = 0;
            return true;
        }

        _counter++;
        if (_counter >= _patience)
        {
            _shouldStop = true;
        }

        return false;
    }

    /// <summary>
    /// Reset the early stopping state
    /// </summary>
    public void Reset()
    {
        _counter = 0;
        _shouldStop = false;
        _bestValue = _mode == EarlyStoppingMode.Min ? float.MaxValue : float.MinValue;
    }
}

public enum EarlyStoppingMode
{
    /// <summary>Stop when value stops decreasing</summary>
    Min,

    /// <summary>Stop when value stops increasing</summary>
    Max
}
```

#### 2. Services/Training/TrainingMetricsTracker.cs

```csharp
namespace SeniorIntern.Services.Training;

/// <summary>
/// Tracks and aggregates training metrics
/// </summary>
public sealed class TrainingMetricsTracker
{
    private readonly List<TrainingMetrics> _history = new();
    private readonly int _smoothingWindow;

    private float _runningLoss;
    private int _runningCount;

    public IReadOnlyList<TrainingMetrics> History => _history;
    public int TotalSteps => _history.Count;

    public TrainingMetricsTracker(int smoothingWindow = 100)
    {
        _smoothingWindow = smoothingWindow;
    }

    /// <summary>
    /// Record a training step
    /// </summary>
    public void Record(TrainingMetrics metrics)
    {
        _history.Add(metrics);
        _runningLoss += metrics.Loss;
        _runningCount++;
    }

    /// <summary>
    /// Get smoothed loss (moving average)
    /// </summary>
    public float GetSmoothedLoss()
    {
        if (_history.Count == 0) return 0;

        var window = Math.Min(_smoothingWindow, _history.Count);
        return _history.TakeLast(window).Average(m => m.Loss);
    }

    /// <summary>
    /// Get average loss for an epoch
    /// </summary>
    public float GetEpochAverageLoss(int epoch)
    {
        var epochMetrics = _history.Where(m => m.Epoch == epoch).ToList();
        return epochMetrics.Count > 0 ? epochMetrics.Average(m => m.Loss) : 0;
    }

    /// <summary>
    /// Get best metrics by validation loss
    /// </summary>
    public TrainingMetrics? GetBestMetrics()
    {
        return _history
            .Where(m => m.ValidationLoss.HasValue)
            .OrderBy(m => m.ValidationLoss)
            .FirstOrDefault();
    }

    /// <summary>
    /// Get loss trend (positive = increasing, negative = decreasing)
    /// </summary>
    public float GetLossTrend(int window = 50)
    {
        if (_history.Count < window * 2) return 0;

        var recent = _history.TakeLast(window).Average(m => m.Loss);
        var previous = _history.SkipLast(window).TakeLast(window).Average(m => m.Loss);

        return recent - previous;
    }

    /// <summary>
    /// Check for loss spike (sudden increase)
    /// </summary>
    public bool HasLossSpike(float threshold = 2.0f)
    {
        if (_history.Count < 2) return false;

        var currentLoss = _history[^1].Loss;
        var previousLoss = _history[^2].Loss;

        return currentLoss > previousLoss * threshold;
    }

    /// <summary>
    /// Get summary statistics
    /// </summary>
    public MetricsSummary GetSummary()
    {
        if (_history.Count == 0)
        {
            return new MetricsSummary();
        }

        var losses = _history.Select(m => m.Loss).ToList();

        return new MetricsSummary
        {
            TotalSteps = _history.Count,
            FinalLoss = losses.Last(),
            MinLoss = losses.Min(),
            MaxLoss = losses.Max(),
            AverageLoss = losses.Average(),
            BestValidationLoss = _history
                .Where(m => m.ValidationLoss.HasValue)
                .Select(m => m.ValidationLoss!.Value)
                .DefaultIfEmpty(float.MaxValue)
                .Min()
        };
    }

    /// <summary>
    /// Reset tracker
    /// </summary>
    public void Reset()
    {
        _history.Clear();
        _runningLoss = 0;
        _runningCount = 0;
    }

    /// <summary>
    /// Export metrics to JSON
    /// </summary>
    public string ExportToJson()
    {
        return JsonSerializer.Serialize(_history, new JsonSerializerOptions
        {
            WriteIndented = true
        });
    }
}

/// <summary>
/// Summary of training metrics
/// </summary>
public sealed class MetricsSummary
{
    public int TotalSteps { get; init; }
    public float FinalLoss { get; init; }
    public float MinLoss { get; init; }
    public float MaxLoss { get; init; }
    public float AverageLoss { get; init; }
    public float BestValidationLoss { get; init; }
}
```

### v0.8.3i File Summary

| File | Purpose | Lines (Est.) |
|------|---------|--------------|
| `Services/Training/EarlyStopping.cs` | Early stopping implementation | ~80 |
| `Services/Training/TrainingMetricsTracker.cs` | Metrics tracking and aggregation | ~140 |

**v0.8.3i Total: 2 files to create**

---

## v0.8.3j: Unit Testing

### Objective
Create comprehensive unit tests for all LoRA training components.

### Files to Create

| File | Purpose |
|------|---------|
| `Tests/Training/LoraLayerTests.cs` | Tests for LoRA layer |
| `Tests/Training/LoraModelTests.cs` | Tests for LoRA model wrapper |
| `Tests/Training/LossComputationTests.cs` | Tests for loss functions |
| `Tests/Training/GradientManagerTests.cs` | Tests for gradient management |
| `Tests/Training/OptimizerFactoryTests.cs` | Tests for optimizer creation |
| `Tests/Training/SchedulerFactoryTests.cs` | Tests for scheduler creation |
| `Tests/Training/CheckpointManagerTests.cs` | Tests for checkpoint save/load |
| `Tests/Training/AdapterSerializerTests.cs` | Tests for adapter serialization |
| `Tests/Training/EarlyStoppingTests.cs` | Tests for early stopping |
| `Tests/Training/LoraTrainingEngineTests.cs` | Integration tests for training |

**v0.8.3j Total: 10 test files to create**

---

## Success Criteria

### Functional Requirements
- [ ] LoRA layers correctly wrap linear layers with trainable adapters
- [ ] Forward pass produces correct output (original + scaled LoRA)
- [ ] Merge/unmerge operations are mathematically correct
- [ ] Cross-entropy loss correctly handles label masking (-100)
- [ ] Gradient clipping and accumulation work correctly
- [ ] Optimizers (AdamW, Adam, SGD) update only LoRA parameters
- [ ] Learning rate schedulers (cosine, linear) follow expected curves
- [ ] Checkpoints save and restore complete training state
- [ ] Training can resume from checkpoint without loss of progress
- [ ] Adapters save in portable format and reload correctly
- [ ] Early stopping triggers after specified patience epochs

### Performance Requirements
- [ ] LoRA forward pass adds <10% overhead vs base model
- [ ] Memory usage stays within VRAM limits for configured batch size
- [ ] Checkpoint save completes in <10 seconds
- [ ] Adapter save completes in <5 seconds
- [ ] Training achieves >90% GPU utilization when CUDA available

### Quality Requirements
- [ ] All public APIs have XML documentation
- [ ] Unit test coverage exceeds 80%
- [ ] No memory leaks during training loops
- [ ] Proper error handling for OOM conditions
- [ ] Graceful cancellation support

---

## File Summary

| Sub-Version | Files to Create | Files to Modify |
|-------------|-----------------|-----------------|
| v0.8.3a | 3 | 0 |
| v0.8.3b | 2 | 0 |
| v0.8.3c | 4 | 0 |
| v0.8.3d | 2 | 0 |
| v0.8.3e | 2 | 0 |
| v0.8.3f | 1 | 0 |
| v0.8.3g | 2 | 0 |
| v0.8.3h | 2 | 0 |
| v0.8.3i | 2 | 0 |
| v0.8.3j | 10 | 0 |
| **Total** | **30** | **0** |

---

## Dependencies

### NuGet Packages Required
```xml
<PackageReference Include="TorchSharp" Version="0.102.0" />
<PackageReference Include="TorchSharp-cuda-windows" Version="0.102.0" Condition="..." />
<PackageReference Include="TorchSharp-cuda-linux" Version="0.102.0" Condition="..." />
<PackageReference Include="libtorch-mps" Version="2.1.0" Condition="..." />
<PackageReference Include="LLamaSharp" Version="0.11.0" />
```

### Project References
- `SeniorIntern.Core` (interfaces and models)
- `SeniorIntern.Services` (service implementations)

---

## Integration Points

### With v0.8.1 (Training Infrastructure)
- Uses `IHardwareDetectionService` for device selection
- Uses `TrainingConfiguration` for hyperparameters
- Uses TorchSharp initialization from `TorchSharpInitializer`

### With v0.8.2 (Dataset Pipeline)
- Uses `IDatasetService` for loading datasets
- Uses `IDataLoader` for training iteration
- Uses `TrainingBatch` for forward pass

### With v0.8.4 (Hardware Monitoring)
- Reports `HardwareMetrics` during training
- Subscribes to memory warnings

### With v0.8.5 (Training UI)
- Exposes `TrainingProgress` for UI display
- Fires events for state changes
- Supports pause/resume for user control

---

## Next Steps (v0.8.4)

After completing the LoRA Training Engine, the next version (v0.8.4: Hardware Monitoring) will:

1. Implement real-time GPU/CPU metrics collection
2. Create memory tracking and warning system
3. Build streaming metrics for UI graphs
4. Add temperature monitoring and thermal throttling detection
