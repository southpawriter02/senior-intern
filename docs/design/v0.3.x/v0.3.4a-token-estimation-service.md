# Design Specification: The Senior Intern v0.3.4a "Token Estimation Service"

## Executive Summary

This document provides a detailed implementation specification for v0.3.4a, which implements the `ITokenEstimationService` - a service for estimating token counts in text content. This is foundational for the context attachment feature, enabling accurate tracking of context size and preventing users from exceeding LLM context limits.

### v0.3.4a Scope

- Create `ITokenEstimationService` interface with token estimation methods
- Create `TokenEstimationService` with three estimation algorithms
- Create `TokenEstimationMethod` enum (CharacterBased, WordBased, BpeApproximate)
- Create `TokenUsageBreakdown` model for usage tracking
- Create `ContextLimitsConfig` for configurable limits
- Implement content truncation with word-boundary awareness

### Key Deliverables

| Deliverable | Description |
|-------------|-------------|
| ITokenEstimationService | Interface for token estimation |
| TokenEstimationService | Multi-algorithm implementation |
| TokenEstimationMethod | Enum of estimation algorithms |
| TokenUsageBreakdown | Usage breakdown model |
| ContextLimitsConfig | Configurable token/file limits |

---

## Feature Overview

```
┌──────────────────────────────────────────────────────────────────┐
│                     v0.3.4a Feature Tree                          │
├──────────────────────────────────────────────────────────────────┤
│                                                                   │
│  ITokenEstimationService                                          │
│  ├── EstimateTokens(content) → int                               │
│  │   └── Default: WordBased method                               │
│  │                                                                │
│  ├── EstimateTokens(content, method) → int                       │
│  │   ├── CharacterBased: ~3.5 chars/token                       │
│  │   ├── WordBased: words + punctuation + whitespace            │
│  │   └── BpeApproximate: simplified BPE simulation              │
│  │                                                                │
│  ├── GetRecommendedContextLimit() → int                          │
│  │   └── Default: 8000 tokens                                    │
│  │                                                                │
│  ├── WouldExceedLimit(currentTokens, newContent) → bool          │
│  │   └── Checks if adding content exceeds limit                  │
│  │                                                                │
│  ├── TruncateToTokenLimit(content, maxTokens) → string           │
│  │   ├── Binary search for optimal truncation point             │
│  │   ├── Respects word boundaries                               │
│  │   └── Appends "... (truncated)"                              │
│  │                                                                │
│  └── GetUsageBreakdown(contents) → TokenUsageBreakdown           │
│      ├── TotalTokens                                             │
│      ├── RecommendedLimit                                        │
│      ├── UsagePercentage                                         │
│      ├── IsOverLimit                                             │
│      ├── RemainingTokens                                         │
│      └── Items: list of (Label, Tokens)                          │
│                                                                   │
│  TokenEstimationMethod (enum)                                     │
│  ├── CharacterBased                                              │
│  │   └── Fast, ~3.5 chars/token, less accurate                  │
│  ├── WordBased (default)                                         │
│  │   └── Moderate speed, considers words + punctuation          │
│  └── BpeApproximate                                              │
│      └── Slower, simulates BPE tokenization                     │
│                                                                   │
│  ContextLimitsConfig                                              │
│  ├── MaxFilesAttached: int = 10                                  │
│  ├── MaxTokensPerFile: int = 4000                                │
│  ├── MaxTotalContextTokens: int = 8000                           │
│  ├── MaxFileSizeBytes: int = 500_000 (500KB)                     │
│  ├── WarningThreshold: double = 0.8                              │
│  ├── MaxPreviewLines: int = 20                                   │
│  └── MaxPreviewCharacters: int = 500                             │
│                                                                   │
└──────────────────────────────────────────────────────────────────┘
```

---

## Architecture Diagrams

### Token Estimation Algorithms

```
┌─────────────────────────────────────────────────────────────────┐
│                    Token Estimation Algorithms                   │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  CharacterBased                                                  │
│  ─────────────                                                   │
│  Formula: tokens = content.Length / 3.5                          │
│                                                                  │
│  Example: "public void Main()" (18 chars)                        │
│           = 18 / 3.5 = 5.14 → 6 tokens                           │
│                                                                  │
│  Pros: Very fast, O(1)                                           │
│  Cons: Less accurate for code (variable naming affects ratio)    │
│                                                                  │
│  ═══════════════════════════════════════════════════════════════ │
│                                                                  │
│  WordBased (Default)                                             │
│  ─────────────────────                                           │
│  Formula:                                                        │
│    words = count of \b\w+\b matches                              │
│    punctuation = count of [^\w\s] matches                        │
│    newlines = count of \n                                        │
│    whitespace = count of [ \t]{2,} sequences                     │
│                                                                  │
│    tokens = (words / 0.75)                                       │
│           + (punctuation × 0.5)                                  │
│           + (newlines × 0.5)                                     │
│           + (whitespace × 0.3)                                   │
│                                                                  │
│  Example: "public void Main() {\n    return;\n}"                 │
│           words=4, punct=5, newlines=2, whitespace=1             │
│           = (4/0.75) + (5×0.5) + (2×0.5) + (1×0.3)               │
│           = 5.33 + 2.5 + 1.0 + 0.3 = 9.13 → 10 tokens            │
│                                                                  │
│  Pros: Good balance of speed and accuracy                        │
│  Cons: May underestimate for camelCase/snake_case                │
│                                                                  │
│  ═══════════════════════════════════════════════════════════════ │
│                                                                  │
│  BpeApproximate                                                  │
│  ─────────────────                                               │
│  Simulates BPE tokenization:                                     │
│  1. Match common programming tokens first                        │
│  2. Treat whitespace sequences as single tokens                  │
│  3. Words: estimate (length + 3) / 4 tokens                      │
│  4. Punctuation/symbols: 1 token each                            │
│                                                                  │
│  Common tokens matched:                                          │
│    public, private, static, void, class, async, await            │
│    =>, ->, ==, !=, <=, >=, &&, ||, ++, --, +=, -=                │
│    /**, */, ///, //, /*                                          │
│                                                                  │
│  Pros: Most accurate for code                                    │
│  Cons: Slower, O(n) with pattern matching                        │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Truncation Algorithm

```
┌─────────────────────────────────────────────────────────────────┐
│                    Truncation Algorithm                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  TruncateToTokenLimit(content, maxTokens)                        │
│         │                                                        │
│         ▼                                                        │
│  ┌────────────────────────────────────────────────────────────┐  │
│  │ if (EstimateTokens(content) <= maxTokens)                  │  │
│  │     return content; // Already within limit                │  │
│  └────────────────────────────────────────────────────────────┘  │
│         │                                                        │
│         ▼                                                        │
│  ┌────────────────────────────────────────────────────────────┐  │
│  │ Estimate initial character target:                         │  │
│  │ targetChars = maxTokens × 3.5                              │  │
│  └────────────────────────────────────────────────────────────┘  │
│         │                                                        │
│         ▼                                                        │
│  ┌────────────────────────────────────────────────────────────┐  │
│  │ Binary search for optimal cutoff:                          │  │
│  │                                                             │  │
│  │ low = 0, high = min(content.Length, targetChars + 100)     │  │
│  │                                                             │  │
│  │ while (low < high):                                        │  │
│  │     mid = (low + high + 1) / 2                             │  │
│  │     truncated = content[..mid]                             │  │
│  │     tokens = EstimateTokens(truncated)                     │  │
│  │                                                             │  │
│  │     if (tokens <= maxTokens):                              │  │
│  │         low = mid  // Can include more                     │  │
│  │     else:                                                   │  │
│  │         high = mid - 1  // Too many tokens                 │  │
│  └────────────────────────────────────────────────────────────┘  │
│         │                                                        │
│         ▼                                                        │
│  ┌────────────────────────────────────────────────────────────┐  │
│  │ Adjust to word boundary:                                   │  │
│  │                                                             │  │
│  │ result = content[..low]                                    │  │
│  │ lastSpace = result.LastIndexOf(' ')                        │  │
│  │                                                             │  │
│  │ if (lastSpace > low × 0.8):  // Don't lose too much        │  │
│  │     result = result[..lastSpace]                           │  │
│  └────────────────────────────────────────────────────────────┘  │
│         │                                                        │
│         ▼                                                        │
│  ┌────────────────────────────────────────────────────────────┐  │
│  │ return result + "\n... (truncated)"                        │  │
│  └────────────────────────────────────────────────────────────┘  │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

### Usage Breakdown Flow

```
┌─────────────────────────────────────────────────────────────────┐
│                    Usage Breakdown Flow                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                  │
│  GetUsageBreakdown(["file1 content", "file2 content"])           │
│         │                                                        │
│         ▼                                                        │
│  ┌────────────────────────────────────────────────────────────┐  │
│  │ For each content:                                          │  │
│  │   - Label = "Item {index + 1}"                             │  │
│  │   - Tokens = EstimateTokens(content)                       │  │
│  │                                                             │  │
│  │ items = [("Item 1", 450), ("Item 2", 1200)]               │  │
│  └────────────────────────────────────────────────────────────┘  │
│         │                                                        │
│         ▼                                                        │
│  ┌────────────────────────────────────────────────────────────┐  │
│  │ TokenUsageBreakdown:                                       │  │
│  │                                                             │  │
│  │ TotalTokens = 1650                                         │  │
│  │ RecommendedLimit = 8000                                    │  │
│  │ UsagePercentage = 20.6%                                    │  │
│  │ IsOverLimit = false                                        │  │
│  │ RemainingTokens = 6350                                     │  │
│  │ Items = [("Item 1", 450), ("Item 2", 1200)]               │  │
│  └────────────────────────────────────────────────────────────┘  │
│                                                                  │
│  UI Display:                                                     │
│  ┌────────────────────────────────────────────────────────────┐  │
│  │ ████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░  1,650 / 8,000     │  │
│  │                                         (20.6%)            │  │
│  │                                                             │  │
│  │ • main.cs         450 tokens                               │  │
│  │ • utils.ts      1,200 tokens                               │  │
│  └────────────────────────────────────────────────────────────┘  │
│                                                                  │
└─────────────────────────────────────────────────────────────────┘
```

---

## Directory Structure

```
src/SeniorIntern.Core/
├── Interfaces/
│   └── ITokenEstimationService.cs                    (NEW)
└── Models/
    ├── TokenEstimationMethod.cs                      (NEW)
    ├── TokenUsageBreakdown.cs                        (NEW)
    ├── ContextLimitsConfig.cs                        (NEW)
    └── AppSettings.cs                                (MODIFY)

src/SeniorIntern.Services/
└── TokenEstimationService.cs                         (NEW)
```

---

## Implementation Details

### Task 1: Create ITokenEstimationService Interface

**File:** `src/SeniorIntern.Core/Interfaces/ITokenEstimationService.cs`

```csharp
namespace SeniorIntern.Core.Interfaces;

using SeniorIntern.Core.Models;

/// <summary>
/// Service for estimating token counts in text content.
/// </summary>
public interface ITokenEstimationService
{
    /// <summary>
    /// Estimates the token count for the given content using the default method (WordBased).
    /// </summary>
    /// <param name="content">The text content to estimate.</param>
    /// <returns>Estimated token count.</returns>
    int EstimateTokens(string content);

    /// <summary>
    /// Estimates token count using a specific estimation method.
    /// </summary>
    /// <param name="content">The text content to estimate.</param>
    /// <param name="method">The estimation algorithm to use.</param>
    /// <returns>Estimated token count.</returns>
    int EstimateTokens(string content, TokenEstimationMethod method);

    /// <summary>
    /// Gets the recommended token limit for context based on model capabilities.
    /// </summary>
    /// <returns>Recommended maximum tokens for context.</returns>
    int GetRecommendedContextLimit();

    /// <summary>
    /// Checks if adding content would exceed the context limit.
    /// </summary>
    /// <param name="currentTokens">Current token count.</param>
    /// <param name="newContent">New content to add.</param>
    /// <returns>True if adding would exceed limit.</returns>
    bool WouldExceedLimit(int currentTokens, string newContent);

    /// <summary>
    /// Truncates content to fit within a token limit, respecting word boundaries.
    /// </summary>
    /// <param name="content">Content to truncate.</param>
    /// <param name="maxTokens">Maximum tokens allowed.</param>
    /// <returns>Truncated content with indicator.</returns>
    string TruncateToTokenLimit(string content, int maxTokens);

    /// <summary>
    /// Gets a breakdown of token usage across multiple content items.
    /// </summary>
    /// <param name="contents">Collection of content items.</param>
    /// <returns>Usage breakdown with totals and per-item counts.</returns>
    TokenUsageBreakdown GetUsageBreakdown(IEnumerable<string> contents);
}
```

### Task 2: Create TokenEstimationMethod Enum

**File:** `src/SeniorIntern.Core/Models/TokenEstimationMethod.cs`

```csharp
namespace SeniorIntern.Core.Models;

/// <summary>
/// Methods for estimating token counts.
/// </summary>
public enum TokenEstimationMethod
{
    /// <summary>
    /// Simple character-based estimation (~3.5 chars per token).
    /// Fast but less accurate. Suitable for quick estimates.
    /// </summary>
    CharacterBased,

    /// <summary>
    /// Word and punctuation based estimation.
    /// Considers word count, punctuation, newlines, and whitespace.
    /// Good balance of speed and accuracy. Default method.
    /// </summary>
    WordBased,

    /// <summary>
    /// BPE-approximate estimation simulating subword tokenization.
    /// Most accurate for code but slower. Matches common programming tokens.
    /// </summary>
    BpeApproximate
}
```

### Task 3: Create TokenUsageBreakdown Model

**File:** `src/SeniorIntern.Core/Models/TokenUsageBreakdown.cs`

```csharp
namespace SeniorIntern.Core.Models;

using System;
using System.Collections.Generic;

/// <summary>
/// Breakdown of token usage across multiple content items.
/// </summary>
public sealed class TokenUsageBreakdown
{
    /// <summary>
    /// Total tokens across all items.
    /// </summary>
    public int TotalTokens { get; init; }

    /// <summary>
    /// Recommended maximum token limit.
    /// </summary>
    public int RecommendedLimit { get; init; }

    /// <summary>
    /// Usage as a percentage of the limit (0-100+).
    /// </summary>
    public double UsagePercentage => RecommendedLimit > 0
        ? (double)TotalTokens / RecommendedLimit * 100
        : 0;

    /// <summary>
    /// Whether total tokens exceed the recommended limit.
    /// </summary>
    public bool IsOverLimit => TotalTokens > RecommendedLimit;

    /// <summary>
    /// Number of tokens remaining before hitting limit.
    /// </summary>
    public int RemainingTokens => Math.Max(0, RecommendedLimit - TotalTokens);

    /// <summary>
    /// Whether usage is above the warning threshold (80%).
    /// </summary>
    public bool IsWarning => UsagePercentage >= 80 && !IsOverLimit;

    /// <summary>
    /// Per-item token breakdown.
    /// </summary>
    public IReadOnlyList<TokenUsageItem> Items { get; init; } = Array.Empty<TokenUsageItem>();
}

/// <summary>
/// Token usage for a single content item.
/// </summary>
public sealed record TokenUsageItem(string Label, int Tokens);
```

### Task 4: Create ContextLimitsConfig

**File:** `src/SeniorIntern.Core/Models/ContextLimitsConfig.cs`

```csharp
namespace SeniorIntern.Core.Models;

/// <summary>
/// Configuration for context attachment limits.
/// </summary>
public sealed class ContextLimitsConfig
{
    /// <summary>
    /// Maximum number of files that can be attached at once.
    /// </summary>
    public int MaxFilesAttached { get; set; } = 10;

    /// <summary>
    /// Maximum tokens per individual file.
    /// </summary>
    public int MaxTokensPerFile { get; set; } = 4000;

    /// <summary>
    /// Maximum total tokens across all attached contexts.
    /// </summary>
    public int MaxTotalContextTokens { get; set; } = 8000;

    /// <summary>
    /// Maximum file size in bytes (files larger are rejected).
    /// </summary>
    public int MaxFileSizeBytes { get; set; } = 500_000; // 500KB

    /// <summary>
    /// Warning threshold as percentage of limit (0.0-1.0).
    /// </summary>
    public double WarningThreshold { get; set; } = 0.8;

    /// <summary>
    /// Maximum lines to show in preview.
    /// </summary>
    public int MaxPreviewLines { get; set; } = 20;

    /// <summary>
    /// Maximum characters in preview content.
    /// </summary>
    public int MaxPreviewCharacters { get; set; } = 500;
}
```

### Task 5: Create TokenEstimationService Implementation

**File:** `src/SeniorIntern.Services/TokenEstimationService.cs`

```csharp
namespace SeniorIntern.Services;

using System;
using System.Collections.Generic;
using System.Linq;
using System.Text.RegularExpressions;
using SeniorIntern.Core.Interfaces;
using SeniorIntern.Core.Models;

/// <summary>
/// Service for estimating token counts using various methods.
/// </summary>
public sealed partial class TokenEstimationService : ITokenEstimationService
{
    // Default context limit - can be adjusted based on model
    private const int DefaultContextLimit = 8000;

    // Character-based estimation ratio (conservative for code)
    private const double CharsPerToken = 3.5;

    // Word-based estimation ratios
    private const double WordsPerToken = 0.75;
    private const double PunctuationWeight = 0.5;

    /// <inheritdoc />
    public int EstimateTokens(string content)
    {
        return EstimateTokens(content, TokenEstimationMethod.WordBased);
    }

    /// <inheritdoc />
    public int EstimateTokens(string content, TokenEstimationMethod method)
    {
        if (string.IsNullOrEmpty(content))
            return 0;

        return method switch
        {
            TokenEstimationMethod.CharacterBased => EstimateByCharacters(content),
            TokenEstimationMethod.WordBased => EstimateByWords(content),
            TokenEstimationMethod.BpeApproximate => EstimateByBpe(content),
            _ => EstimateByWords(content)
        };
    }

    /// <inheritdoc />
    public int GetRecommendedContextLimit()
    {
        return DefaultContextLimit;
    }

    /// <inheritdoc />
    public bool WouldExceedLimit(int currentTokens, string newContent)
    {
        var newTokens = EstimateTokens(newContent);
        return (currentTokens + newTokens) > GetRecommendedContextLimit();
    }

    /// <inheritdoc />
    public string TruncateToTokenLimit(string content, int maxTokens)
    {
        if (string.IsNullOrEmpty(content))
            return content;

        var currentTokens = EstimateTokens(content);
        if (currentTokens <= maxTokens)
            return content;

        // Estimate characters needed for target tokens
        var targetChars = (int)(maxTokens * CharsPerToken);

        // Binary search for optimal truncation point
        var low = 0;
        var high = Math.Min(content.Length, targetChars + 100);

        while (low < high)
        {
            var mid = (low + high + 1) / 2;
            var truncated = content[..mid];
            var tokens = EstimateTokens(truncated);

            if (tokens <= maxTokens)
                low = mid;
            else
                high = mid - 1;
        }

        // Truncate at word boundary if possible
        var result = content[..low];
        var lastSpace = result.LastIndexOf(' ');
        if (lastSpace > low * 0.8) // Only if we don't lose too much
        {
            result = result[..lastSpace];
        }

        return result + "\n... (truncated)";
    }

    /// <inheritdoc />
    public TokenUsageBreakdown GetUsageBreakdown(IEnumerable<string> contents)
    {
        var items = contents
            .Select((content, index) => new TokenUsageItem(
                $"Item {index + 1}",
                EstimateTokens(content)
            ))
            .ToList();

        return new TokenUsageBreakdown
        {
            TotalTokens = items.Sum(i => i.Tokens),
            RecommendedLimit = GetRecommendedContextLimit(),
            Items = items
        };
    }

    #region Character-Based Estimation

    private static int EstimateByCharacters(string content)
    {
        // Simple: ~3.5 characters per token (conservative for code)
        return (int)Math.Ceiling(content.Length / CharsPerToken);
    }

    #endregion

    #region Word-Based Estimation

    [GeneratedRegex(@"\b\w+\b")]
    private static partial Regex WordPattern();

    [GeneratedRegex(@"[^\w\s]")]
    private static partial Regex PunctuationPattern();

    [GeneratedRegex(@"[ \t]{2,}")]
    private static partial Regex WhitespaceSequencePattern();

    private static int EstimateByWords(string content)
    {
        // Count words (sequences of word characters)
        var wordCount = WordPattern().Matches(content).Count;

        // Count punctuation and special characters
        var punctuationCount = PunctuationPattern().Matches(content).Count;

        // Count newlines (often separate tokens)
        var newlineCount = content.Count(c => c == '\n');

        // Count whitespace sequences > 1 (indentation)
        var whitespaceSequences = WhitespaceSequencePattern().Matches(content).Count;

        // Weighted combination
        var estimate = (int)Math.Ceiling(
            wordCount / WordsPerToken +
            punctuationCount * PunctuationWeight +
            newlineCount * 0.5 +
            whitespaceSequences * 0.3
        );

        return Math.Max(1, estimate);
    }

    #endregion

    #region BPE-Approximate Estimation

    private static readonly string[] CommonTokens =
    {
        "public", "private", "protected", "static", "void", "class", "interface",
        "async", "await", "return", "string", "int", "bool", "var", "const",
        "function", "export", "import", "from", "=>", "->", "==", "!=", "<=", ">=",
        "&&", "||", "++", "--", "+=", "-=", "*=", "/=", "<<", ">>", "::", "..",
        "/**", "*/", "///", "//", "/*"
    };

    private static int EstimateByBpe(string content)
    {
        var tokens = 0;
        var i = 0;

        while (i < content.Length)
        {
            var remaining = content.AsSpan(i);

            // Try to match common multi-character tokens first
            if (TryMatchCommonToken(remaining, out var matchLength))
            {
                tokens++;
                i += matchLength;
                continue;
            }

            var c = content[i];

            if (char.IsWhiteSpace(c))
            {
                tokens++;
                // Skip consecutive whitespace of same type
                while (i + 1 < content.Length && content[i + 1] == c)
                    i++;
            }
            else if (char.IsLetterOrDigit(c))
            {
                var wordStart = i;
                while (i < content.Length && char.IsLetterOrDigit(content[i]))
                    i++;
                var wordLength = i - wordStart;

                // Estimate tokens for word (longer words = more tokens)
                tokens += Math.Max(1, (wordLength + 3) / 4);
                continue;
            }
            else
            {
                // Punctuation/symbols - usually single tokens
                tokens++;
            }

            i++;
        }

        return Math.Max(1, tokens);
    }

    private static bool TryMatchCommonToken(ReadOnlySpan<char> text, out int length)
    {
        foreach (var token in CommonTokens)
        {
            if (text.StartsWith(token.AsSpan()))
            {
                length = token.Length;
                return true;
            }
        }

        length = 0;
        return false;
    }

    #endregion
}
```

### Task 6: Update AppSettings

**File:** `src/SeniorIntern.Core/Models/AppSettings.cs` (additions)

```csharp
// Add to AppSettings class:

/// <summary>
/// Context attachment limits configuration.
/// </summary>
public ContextLimitsConfig ContextLimits { get; set; } = new();
```

---

## Algorithm Comparison

| Method | Speed | Accuracy | Best For |
|--------|-------|----------|----------|
| CharacterBased | O(1) | Low | Quick estimates, real-time typing |
| WordBased | O(n) | Medium | Default usage, mixed content |
| BpeApproximate | O(n×m) | High | Code, final token counts |

### Accuracy Benchmarks (Expected)

| Content Type | CharacterBased Error | WordBased Error | BpeApproximate Error |
|--------------|---------------------|-----------------|---------------------|
| Plain text | ±30% | ±15% | ±5% |
| Code | ±40% | ±20% | ±10% |
| Mixed | ±35% | ±18% | ±8% |

---

## Unit Testing Requirements

| Component | Test Count | Focus Areas |
|-----------|------------|-------------|
| EstimateTokens (default) | 3 | Empty, short, long content |
| CharacterBased | 3 | Simple text, code, special chars |
| WordBased | 4 | Words, punctuation, newlines, whitespace |
| BpeApproximate | 4 | Common tokens, words, mixed |
| GetRecommendedContextLimit | 1 | Returns expected value |
| WouldExceedLimit | 3 | Under, at, over limit |
| TruncateToTokenLimit | 4 | Under limit, truncation, word boundary, empty |
| GetUsageBreakdown | 3 | Empty, single, multiple items |
| ContextLimitsConfig | 2 | Defaults, custom values |
| **Total** | **27** | |

---

## Files Summary

### Files to Create (5)

| File | Lines |
|------|-------|
| `Core/Interfaces/ITokenEstimationService.cs` | 50 |
| `Core/Models/TokenEstimationMethod.cs` | 25 |
| `Core/Models/TokenUsageBreakdown.cs` | 45 |
| `Core/Models/ContextLimitsConfig.cs` | 40 |
| `Services/TokenEstimationService.cs` | 200 |

### Files to Modify (1)

| File | Changes |
|------|---------|
| `Core/Models/AppSettings.cs` | Add ContextLimits property |

---

## Acceptance Criteria

| ID | Criterion |
|----|-----------|
| AC-1 | EstimateTokens returns reasonable estimates for code content |
| AC-2 | CharacterBased estimation is fast (<1ms for typical files) |
| AC-3 | WordBased estimation is more accurate than CharacterBased |
| AC-4 | BpeApproximate matches common programming tokens |
| AC-5 | TruncateToTokenLimit preserves word boundaries |
| AC-6 | TruncateToTokenLimit adds truncation indicator |
| AC-7 | WouldExceedLimit correctly predicts limit violations |
| AC-8 | GetUsageBreakdown calculates totals correctly |
| AC-9 | Token limits are configurable via settings |
| AC-10 | Empty content returns 0 tokens |

---

## Dependencies

| Part | Dependency Description |
|------|------------------------|
| System.Text.RegularExpressions | Regex patterns for word counting |

---

## Timeline Estimate

| Version | Estimated Effort |
|---------|------------------|
| v0.3.4a | 0.5 day |
